{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCGeVy9NwjRa"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "## Problem Setup\n",
        "This example demonstrates a multi-agent deep deterministic policy gradient (MADDPG) approach to a competitive environment where chasers aim to catch evaders. Each group of agents (chasers and evaders) has its own policy and value networks, trained either independently or in a mixed cooperative-competitive setting. It serves as our control when we try and integrate prospect theory into the policy gradient, seeing if we can get different results than the reward graphs below. Code is based from [https://pytorch.org/rl/0.6/tutorials/multiagent_competitive_ddpg.html]\n",
        "\n",
        "## Clear Problem Statement\n",
        "Train two chaser agents to minimize the evader’s cumulative reward while simultaneously training the evader agent to maximize its own cumulative reward. The environment runs for a fixed number of steps, and training can be halted for certain agents at a chosen iteration.\n",
        "\n",
        "## Mathematical Formulation\n",
        "- **Agent Policies**: $\\pi_i(\\mathbf{o_i}; \\theta_i)$ map observations $\\mathbf{o_i}$ to continuous actions.\n",
        "- **Value Function**: $Q_i(\\mathbf{o}, \\mathbf{a}; \\phi_i)$ estimates future return given all agents’ actions $\\mathbf{a}$ and observations $\\mathbf{o}$.\n",
        "- **Loss Functions**: DDPG losses incorporate actor and critic objectives, ensuring that each agent maximizes expected returns while considering centralized training and decentralized execution.\n",
        "- **Updates**: Soft updates are performed on target networks with \\(\\tau\\) for both the policy and value functions.\n",
        "\n",
        "## Data Requirements\n",
        "- Episodes of agent interactions, collected with exploration strategies (e.g., Gaussian noise).\n",
        "- Replay buffers per group for sampled training batches containing states, actions, rewards, and next states.\n",
        "\n",
        "## Success Metrics\n",
        "- Mean episode reward for each group (chasers and evaders), typically measured and plotted over training iterations.\n",
        "- Convergence or stabilization of the reward signal, indicating improved policy performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchrl==0.6.0 pettingzoo gymnasium torch tqdm"
      ],
      "metadata": {
        "id": "RPIbRmLC_fqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dce79de-af09-4529-dc7c-80b9693ac3ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchrl==0.6.0\n",
            "  Using cached torchrl-0.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (39 kB)\n",
            "Requirement already satisfied: pettingzoo in /usr/local/lib/python3.11/dist-packages (1.24.3)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchrl==0.6.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from torchrl==0.6.0) (24.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from torchrl==0.6.0) (3.1.1)\n",
            "Collecting tensordict>=0.6.0 (from torchrl==0.6.0)\n",
            "  Using cached tensordict-0.7.2-cp311-cp311-manylinux1_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from tensordict>=0.6.0->torchrl==0.6.0) (3.10.16)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Using cached torchrl-0.6.0-cp311-cp311-manylinux1_x86_64.whl (1.0 MB)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached tensordict-0.7.2-cp311-cp311-manylinux1_x86_64.whl (400 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tensordict, torchrl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tensordict-0.7.2 torchrl-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install vmas\n",
        "!pip3 install pettingzoo[mpe]==1.24.3\n",
        "!pip3 install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeCdZvUCWBba",
        "outputId": "225108bc-7681-495c-c0c5-3e2bcb17be8a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vmas\n",
            "  Using cached vmas-1.5.0.tar.gz (217 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vmas) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from vmas) (2.6.0+cu124)\n",
            "Collecting pyglet<=1.5.27 (from vmas)\n",
            "  Using cached pyglet-1.5.27-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from vmas) (0.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from vmas) (1.17.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->vmas) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym->vmas) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->vmas) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->vmas) (3.0.2)\n",
            "Using cached pyglet-1.5.27-py3-none-any.whl (1.1 MB)\n",
            "Building wheels for collected packages: vmas\n",
            "  Building wheel for vmas (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vmas: filename=vmas-1.5.0-py3-none-any.whl size=257528 sha256=4946d248ffe6a59d708d6eda67342e8779248f0e8aff3f2b25bc5b00ba5e36ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/ab/3b/f1eb0befe556b53a3f78b4780554c29dca0cb781fb664f6a81\n",
            "Successfully built vmas\n",
            "Installing collected packages: pyglet, vmas\n",
            "Successfully installed pyglet-1.5.27 vmas-1.5.0\n",
            "Requirement already satisfied: pettingzoo==1.24.3 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[mpe]==1.24.3) (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (1.1.1)\n",
            "Requirement already satisfied: pygame==2.3.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[mpe]==1.24.3) (2.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (0.0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RttHqpEoy-uW"
      },
      "source": [
        "### **Approach: Importing Required Libraries**\n",
        "This section imports essential modules for implementing MADDPG:  \n",
        "- **PyTorch** for deep learning operations.  \n",
        "- **`torchrl` modules** for multi-agent reinforcement learning, including environments, policies, collectors, and replay buffers.  \n",
        "- **`tensordict`** for structured tensor operations.  \n",
        "- **Matplotlib** for visualization.  \n",
        "- **`tqdm`** for progress tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Yhec5pa7VzyM"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import tempfile\n",
        "\n",
        "import torch\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensordict import TensorDictBase\n",
        "\n",
        "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
        "from torch import multiprocessing\n",
        "\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
        "\n",
        "from torchrl.envs import (\n",
        "    check_env_specs,\n",
        "    ExplorationType,\n",
        "    PettingZooEnv,\n",
        "    RewardSum,\n",
        "    set_exploration_type,\n",
        "    TransformedEnv,\n",
        "    VmasEnv,\n",
        ")\n",
        "\n",
        "from torchrl.modules import (\n",
        "    AdditiveGaussianModule,\n",
        "    MultiAgentMLP,\n",
        "    ProbabilisticActor,\n",
        "    TanhDelta,\n",
        ")\n",
        "\n",
        "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
        "\n",
        "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    is_sphinx = __sphinx_build__\n",
        "except NameError:\n",
        "    is_sphinx = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import tempfile\n",
        "\n",
        "import torch\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensordict import TensorDictBase, is_tensor_collection\n",
        "\n",
        "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
        "from torch import multiprocessing\n",
        "\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
        "\n",
        "from torchrl.envs import (\n",
        "    check_env_specs,\n",
        "    ExplorationType,\n",
        "    PettingZooEnv,\n",
        "    RewardSum,\n",
        "    set_exploration_type,\n",
        "    TransformedEnv,\n",
        "    VmasEnv,\n",
        ")\n",
        "\n",
        "from torchrl.modules import (\n",
        "    AdditiveGaussianModule,\n",
        "    MultiAgentMLP,\n",
        "    ProbabilisticActor,\n",
        "    TanhDelta,\n",
        ")\n",
        "\n",
        "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
        "\n",
        "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    is_sphinx = __sphinx_build__\n",
        "except NameError:\n",
        "    is_sphinx = False\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.compiler import is_compiling\n",
        "except ImportError:\n",
        "    from torch._dynamo import is_compiling"
      ],
      "metadata": {
        "id": "Se009O-PKMC_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools, numpy as np, torch\n",
        "from torch import nn\n",
        "from gymnasium.spaces import Box\n",
        "from gymnasium.utils import seeding\n",
        "\n",
        "# PettingZoo API\n",
        "from pettingzoo import ParallelEnv\n",
        "from pettingzoo.utils import parallel_to_aec, wrappers\n",
        "\n",
        "# TorchRL wrappers & collector\n",
        "from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
        "from torchrl.envs.utils            import set_exploration_type, ExplorationType\n",
        "from torchrl.collectors            import SyncDataCollector\n",
        "\n",
        "# DDPG ingredients\n",
        "from torchrl.objectives.ddpg import DDPGLoss\n",
        "from torchrl.modules           import Actor, ValueOperator"
      ],
      "metadata": {
        "id": "6nOVBzF9V_6T"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "p7ikv2rIXvvT"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def env(render_mode=None):\n",
        "    \"\"\"AEC‑style view of our ParallelEnv.\"\"\"\n",
        "    aec = raw_env(render_mode=render_mode)\n",
        "    if render_mode == \"ansi\":\n",
        "        aec = wrappers.CaptureStdoutWrapper(aec)\n",
        "    aec = wrappers.AssertOutOfBoundsWrapper(aec)\n",
        "    aec = wrappers.OrderEnforcingWrapper(aec)\n",
        "    return aec\n",
        "\n",
        "def raw_env(render_mode=None):\n",
        "    \"\"\"Convert ParallelEnv → AEC via parallel_to_aec.\"\"\"\n",
        "    par = parallel_env(render_mode=render_mode)\n",
        "    return parallel_to_aec(par)\n",
        "\n",
        "class parallel_env(ParallelEnv):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"name\": \"first_price_auction_v0\"}\n",
        "\n",
        "    def __init__(self, num_agents=3, max_bid=1.0, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.max_bid     = max_bid\n",
        "        self.render_mode = render_mode\n",
        "        self.possible_agents = [f\"agent_{i}\" for i in range(num_agents)]\n",
        "        self.agents           = []\n",
        "        # obs = your private valuation ∈[0,1]\n",
        "        self.observation_spaces = {\n",
        "            a: Box(0.0, 1.0, (1,), dtype=np.float32)\n",
        "            for a in self.possible_agents\n",
        "        }\n",
        "        # action = bid ∈[0,max_bid]\n",
        "        self.action_spaces = {\n",
        "            a: Box(0.0, float(max_bid), (1,), dtype=np.float32)\n",
        "            for a in self.possible_agents\n",
        "        }\n",
        "        self.valuations = {}\n",
        "\n",
        "    @functools.lru_cache(None)\n",
        "    def observation_space(self, agent):\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    @functools.lru_cache(None)\n",
        "    def action_space(self, agent):\n",
        "        return self.action_spaces[agent]\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        if seed is not None:\n",
        "            self.np_random, _ = seeding.np_random(seed)\n",
        "        # all agents active\n",
        "        self.agents = self.possible_agents[:]\n",
        "        # sample private valuations U[0,1]\n",
        "        self.valuations = {a: float(self.np_random.random()) for a in self.agents}\n",
        "        obs   = {a: np.array([self.valuations[a]], dtype=np.float32) for a in self.agents}\n",
        "        infos = {a: {} for a in self.agents}\n",
        "        return obs, infos\n",
        "\n",
        "    def step(self, actions):\n",
        "        if not actions:  # no agents → done\n",
        "            self.agents = []\n",
        "            return {}, {}, {}, {}, {}\n",
        "\n",
        "        bids = {a: float(actions[a][0]) for a in actions}\n",
        "        # highest bid wins (ties by agent order)\n",
        "        winner = max(self.possible_agents, key=lambda a: bids[a])\n",
        "        # payoffs\n",
        "        rewards = {\n",
        "            a: (self.valuations[a] - bids[a] if a == winner else 0.0)\n",
        "            for a in self.possible_agents\n",
        "        }\n",
        "        terminated = {a: True  for a in self.possible_agents}\n",
        "        truncated  = {a: False for a in self.possible_agents}\n",
        "        next_obs    = {a: np.zeros((1,), dtype=np.float32) for a in self.possible_agents}\n",
        "        infos       = {a: {} for a in self.possible_agents}\n",
        "\n",
        "        # end episode\n",
        "        self.agents = []\n",
        "        return next_obs, rewards, terminated, truncated, infos\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == \"human\":\n",
        "            print(\"Valuations:\", self.valuations)\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "NlXgfpAYWNu6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpSRX7gfzFeN"
      },
      "source": [
        "### **Approach: Environment Setup & Hyperparameters**\n",
        "- **Seed & Device**: Sets the random seed for reproducibility and selects the appropriate device (GPU if available, otherwise CPU).  \n",
        "- **Sampling**: Defines frames collected per batch (`1,000`), total iterations (`50`), and total frames (`50,000`).  \n",
        "- **Training Control**: Stops evader training at `iteration_when_stop_training_evaders = 25`.  \n",
        "- **Replay Buffer**: Stores up to `1M` frames for experience replay.  \n",
        "- **Training Parameters**:  \n",
        "  - **Optimization**: `100` updates per iteration, batch size of `128`.  \n",
        "  - **Learning Rate**: `3e-4`, gradient clipping at `1.0`.  \n",
        "- **DDPG-Specific**: Uses discount factor (`γ = 0.99`) and soft update parameter (`τ = 0.005`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "2eDQqQ5YV3Sw"
      },
      "outputs": [],
      "source": [
        "# Seed\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Devices\n",
        "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
        "# Sampling\n",
        "frames_per_batch = 1_000  # Number of team frames collected per sampling iteration\n",
        "n_iters = 200  # Number of sampling and training iterations\n",
        "total_frames = frames_per_batch * n_iters\n",
        "\n",
        "\n",
        "# Replay buffer\n",
        "memory_size = 1_000_000  # The replay buffer of each group can store this many frames\n",
        "\n",
        "# Training\n",
        "n_optimiser_steps = 100  # Number of optimization steps per training iteration\n",
        "train_batch_size = 128  # Number of frames trained in each optimiser step\n",
        "lr = 3e-4  # Learning rate\n",
        "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
        "\n",
        "# DDPG\n",
        "gamma = 0.99  # Discount factor\n",
        "polyak_tau = 0.005  # Tau for the soft-update of the target network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qldjssDfy8l5"
      },
      "source": [
        "### **Approach: Environment Configuration**\n",
        "- **Max Steps**: Each episode runs for `100` steps.  \n",
        "- **Agents & Obstacles**: `2` chasers, `1` evader, and `2` obstacles.  \n",
        "- **VMAS for Performance**:  \n",
        "  - If `use_vmas = True`, uses `VmasEnv` for efficient vectorized multi-agent simulation.  \n",
        "  - Otherwise, defaults to `PettingZooEnv` (parallel mode) for `simple_tag_v3`.  \n",
        "- **Vectorization**: `num_vmas_envs = frames_per_batch / max_steps` ensures efficient frame collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3dx0KswwV97f"
      },
      "outputs": [],
      "source": [
        "max_steps = 100  # Environment steps before done\n",
        "\n",
        "n_agents = 2\n",
        "n_landmarks = 1\n",
        "\n",
        "use_vmas = False  # Set this to True for a great performance speedup\n",
        "\n",
        "if not use_vmas:\n",
        "    par = parallel_env(num_agents=3, max_bid=1.0, render_mode=None)\n",
        "\n",
        "    # 2) wrap it directly with TorchRL's PettingZooWrapper\n",
        "    base_env = PettingZooWrapper(\n",
        "        env               = par,\n",
        "        return_state      = False,\n",
        "        group_map         = None,\n",
        "        use_mask          = False,\n",
        "        categorical_actions = False,\n",
        "        seed              = 42,\n",
        "        device            = device,\n",
        "        done_on_any       = True,\n",
        "    )\n",
        "else:\n",
        "    num_vmas_envs = (\n",
        "        frames_per_batch // max_steps\n",
        "    )\n",
        "    base_env = VmasEnv(\n",
        "        scenario=\"simple_spread\",\n",
        "        num_envs=num_vmas_envs,\n",
        "        continuous_actions=True,\n",
        "        max_steps=max_steps,\n",
        "        local_ratio=0.5,\n",
        "        device=device,\n",
        "        seed=seed,\n",
        "        n_agents = n_agents\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9qc-A5yWGZJ",
        "outputId": "0f243c38-c026-49ea-970d-a2f276939063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "group_map: {'agent': ['agent_0', 'agent_1', 'agent_2']}\n"
          ]
        }
      ],
      "source": [
        "print(f\"group_map: {base_env.group_map}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYBvfAjqWH3j",
        "outputId": "2bde791d-838a-43eb-e7bc-c844801d1c62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action_spec: Composite(\n",
            "    agent: Composite(\n",
            "        action: BoundedContinuous(\n",
            "            shape=torch.Size([3, 1]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([3])),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([]))\n",
            "reward_spec: Composite(\n",
            "    agent: Composite(\n",
            "        reward: UnboundedContinuous(\n",
            "            shape=torch.Size([3, 1]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([3])),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([]))\n",
            "done_spec: Composite(\n",
            "    done: Categorical(\n",
            "        shape=torch.Size([1]),\n",
            "        space=CategoricalBox(n=2),\n",
            "        device=cuda:0,\n",
            "        dtype=torch.bool,\n",
            "        domain=discrete),\n",
            "    terminated: Categorical(\n",
            "        shape=torch.Size([1]),\n",
            "        space=CategoricalBox(n=2),\n",
            "        device=cuda:0,\n",
            "        dtype=torch.bool,\n",
            "        domain=discrete),\n",
            "    truncated: Categorical(\n",
            "        shape=torch.Size([1]),\n",
            "        space=CategoricalBox(n=2),\n",
            "        device=cuda:0,\n",
            "        dtype=torch.bool,\n",
            "        domain=discrete),\n",
            "    agent: Composite(\n",
            "        done: Categorical(\n",
            "            shape=torch.Size([3, 1]),\n",
            "            space=CategoricalBox(n=2),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.bool,\n",
            "            domain=discrete),\n",
            "        terminated: Categorical(\n",
            "            shape=torch.Size([3, 1]),\n",
            "            space=CategoricalBox(n=2),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.bool,\n",
            "            domain=discrete),\n",
            "        truncated: Categorical(\n",
            "            shape=torch.Size([3, 1]),\n",
            "            space=CategoricalBox(n=2),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.bool,\n",
            "            domain=discrete),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([3])),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([]))\n",
            "observation_spec: Composite(\n",
            "    agent: Composite(\n",
            "        observation: BoundedContinuous(\n",
            "            shape=torch.Size([3, 1]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([3])),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([]))\n"
          ]
        }
      ],
      "source": [
        "print(\"action_spec:\", base_env.full_action_spec)\n",
        "print(\"reward_spec:\", base_env.full_reward_spec)\n",
        "print(\"done_spec:\", base_env.full_done_spec)\n",
        "print(\"observation_spec:\", base_env.observation_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGyVd6LTWJgu",
        "outputId": "236c6992-e635-40f5-e58e-11617fd16a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action_keys: [('agent', 'action')]\n",
            "reward_keys: [('agent', 'reward')]\n",
            "done_keys: ['done', 'terminated', 'truncated', ('agent', 'done'), ('agent', 'terminated'), ('agent', 'truncated')]\n"
          ]
        }
      ],
      "source": [
        "print(\"action_keys:\", base_env.action_keys)\n",
        "print(\"reward_keys:\", base_env.reward_keys)\n",
        "print(\"done_keys:\", base_env.done_keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOdgug90z7vx"
      },
      "source": [
        "### **Approach: Environment Transformation**\n",
        "- **Wraps `base_env` with `TransformedEnv`** to apply reward processing.  \n",
        "- **`RewardSum` Aggregation**:  \n",
        "  - Uses `reward_keys` from `base_env` to sum rewards over time.  \n",
        "  - Resets rewards using `_reset` keys for each agent group.  \n",
        "- **Purpose**: Ensures proper reward tracking across multi-agent interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "q8e7iPn_WKvo"
      },
      "outputs": [],
      "source": [
        "env = TransformedEnv(\n",
        "    base_env,\n",
        "    RewardSum(\n",
        "        in_keys=base_env.reward_keys,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3VPMV-UWLqf",
        "outputId": "955d4162-8548-4d04-c561-ce08b2845c58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-19 00:30:29,504 [torchrl][INFO] check_env_specs succeeded!\n"
          ]
        }
      ],
      "source": [
        "check_env_specs(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BKZ9yjkWNlJ",
        "outputId": "55955a55-2857-4cf4-a6e9-16d4ac3750e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rollout of 5 steps: TensorDict(\n",
            "    fields={\n",
            "        agent: TensorDict(\n",
            "            fields={\n",
            "                action: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                done: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                episode_reward: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                observation: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                terminated: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                truncated: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "            batch_size=torch.Size([1, 3]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                agent: TensorDict(\n",
            "                    fields={\n",
            "                        done: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                        episode_reward: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        observation: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        reward: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        terminated: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                        truncated: Tensor(shape=torch.Size([1, 3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "                    batch_size=torch.Size([1, 3]),\n",
            "                    device=cuda:0,\n",
            "                    is_shared=True),\n",
            "                done: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                terminated: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                truncated: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "            batch_size=torch.Size([1]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        truncated: Tensor(shape=torch.Size([1, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([1]),\n",
            "    device=cuda:0,\n",
            "    is_shared=True)\n",
            "Shape of the rollout TensorDict: torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "n_rollout_steps = 5\n",
        "rollout = env.rollout(n_rollout_steps)\n",
        "print(f\"rollout of {n_rollout_steps} steps:\", rollout)\n",
        "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apB4yTml0Nf-"
      },
      "source": [
        "### **Approach: Policy Network Setup**\n",
        "- **Iterates over agent groups** to create independent policies.  \n",
        "- **Defines `MultiAgentMLP`** for decentralized policies:\n",
        "  - **Observations & Actions**: Uses `env.observation_spec` and `env.full_action_spec`.\n",
        "  - **Decentralized Execution**: Each agent acts based on its local observation.\n",
        "  - **Parameter Sharing**: Controlled by `share_parameters_policy` (set to `True` for efficiency).\n",
        "  - **Architecture**: 2-layer MLP (`256` neurons per layer, `Tanh` activation).\n",
        "- **Wraps in `TensorDictModule`**:\n",
        "  - Reads observations from `TensorDict` and writes action parameters.  \n",
        "  - Allows structured tensor operations for multi-agent training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.group_map.items()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InTO41JycrNu",
        "outputId": "02c7156a-9b05-41a2-b1a6-fd18214ac38d"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([('agent', ['agent_0', 'agent_1', 'agent_2'])])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "8bmzos-xWPzE"
      },
      "outputs": [],
      "source": [
        "policy_modules = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    share_parameters_policy = False  # Can change this based on the group\n",
        "\n",
        "    policy_net = MultiAgentMLP(\n",
        "        n_agent_inputs=env.observation_spec[group, \"observation\"].shape[\n",
        "            -1\n",
        "        ],  # n_obs_per_agent\n",
        "        n_agent_outputs=env.full_action_spec[group, \"action\"].shape[\n",
        "            -1\n",
        "        ],  # n_actions_per_agents\n",
        "        n_agents=len(agents),  # Number of agents in the group\n",
        "        centralised=False,  # the policies are decentralised (i.e., each agent will act from its local observation)\n",
        "        share_params=share_parameters_policy,\n",
        "        device=device,\n",
        "        depth=2,\n",
        "        num_cells=256,\n",
        "        activation_class=torch.nn.Tanh,\n",
        "    )\n",
        "\n",
        "    # Wrap the neural network in a :class:`~tensordict.nn.TensorDictModule`.\n",
        "    # This is simply a module that will read the ``in_keys`` from a tensordict, feed them to the\n",
        "    # neural networks, and write the\n",
        "    # outputs in-place at the ``out_keys``.\n",
        "\n",
        "    policy_module = TensorDictModule(\n",
        "        policy_net,\n",
        "        in_keys=[(group, \"observation\")],\n",
        "        out_keys=[(group, \"param\")],\n",
        "    )\n",
        "    policy_module = policy_module.to(device)\n",
        "    policy_modules[group] = policy_module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvgRl6E81dFI"
      },
      "source": [
        "### **Approach: Probabilistic Policy Definition**\n",
        "- **Wraps policy networks (`policy_modules`) in `ProbabilisticActor`** to handle stochastic action sampling.  \n",
        "- **Uses `TanhDelta` Distribution**:\n",
        "  - Ensures continuous action outputs stay within predefined bounds (`low`, `high`).  \n",
        "  - Helps stabilize training by keeping actions constrained.  \n",
        "- **Input & Output Keys**:\n",
        "  - Reads action parameters from `policy_modules` (`(group, \"param\")`).  \n",
        "  - Outputs final actions (`(group, \"action\")`).  \n",
        "- **Log Probabilities Disabled (`return_log_prob=False`)**:  \n",
        "  - Not needed for deterministic policy updates in DDPG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "vbHpMLXRWROv"
      },
      "outputs": [],
      "source": [
        "policies = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    policy = ProbabilisticActor(\n",
        "        module=policy_modules[group],\n",
        "        spec=env.full_action_spec[group, \"action\"],\n",
        "        in_keys=[(group, \"param\")],\n",
        "        out_keys=[(group, \"action\")],\n",
        "        distribution_class=TanhDelta,\n",
        "        distribution_kwargs={\n",
        "            \"low\": env.full_action_spec[group, \"action\"].space.low,\n",
        "            \"high\": env.full_action_spec[group, \"action\"].space.high,\n",
        "        },\n",
        "        return_log_prob=False,\n",
        "    )\n",
        "    policy = policy.to(device)\n",
        "    policies[group] = policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ttFAcMQ1nev"
      },
      "source": [
        "### **Approach: Exploration Policy with Gaussian Noise**\n",
        "- **Adds exploration noise to deterministic policies** using `AdditiveGaussianModule`.  \n",
        "- **Purpose**: Encourages better exploration by injecting Gaussian noise into actions.  \n",
        "- **Annealing Strategy**:\n",
        "  - **Starts with `sigma_init = 0.9`** (high noise for exploration).  \n",
        "  - **Decays to `sigma_end = 0.1`** over `total_frames / 2` steps, reducing noise gradually.  \n",
        "- **Wrapped in `TensorDictSequential`**:\n",
        "  - First applies the base policy (`policies[group]`).  \n",
        "  - Then adds Gaussian noise to the output action (`(group, \"action\")`).  \n",
        "- **Ensures Smooth Transition**: High exploration at the start, stabilizing towards exploitation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "IpDjZUtQWSeF"
      },
      "outputs": [],
      "source": [
        "exploration_policies = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    exploration_policy = TensorDictSequential(\n",
        "        policies[group],\n",
        "        AdditiveGaussianModule(\n",
        "            spec=policies[group].spec,\n",
        "            annealing_num_steps=total_frames\n",
        "            // 2,  # Number of frames after which sigma is sigma_end\n",
        "            action_key=(group, \"action\"),\n",
        "            sigma_init=0.9,  # Initial value of the sigma\n",
        "            sigma_end=0.1,  # Final value of the sigma\n",
        "        ),\n",
        "    )\n",
        "    exploration_policy = exploration_policy.to(device)\n",
        "    exploration_policies[group] = exploration_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H8lIUgF1vzz"
      },
      "source": [
        "### **Approach: Critic Network for Value Estimation**\n",
        "- **Defines critic networks for each agent group** to estimate state-action values (\\(Q\\)-values).  \n",
        "- **Centralized vs. Decentralized Critic**:\n",
        "  - **`MADDPG = True`**: Uses a centralized critic (multi-agent).  \n",
        "  - **`IDDPG = False`**: Uses an independent critic per agent.  \n",
        "- **Feature Concatenation (`cat_module`)**:\n",
        "  - Combines agent's observation and action into a single tensor (`(group, \"obs_action\")`).  \n",
        "- **Critic Network (`critic_module`)**:\n",
        "  - Takes concatenated state-action inputs and predicts a **single Q-value per agent**.  \n",
        "  - Uses a **2-layer MLP (256 neurons per layer, `Tanh` activation)**.  \n",
        "  - Supports parameter sharing (`share_parameters_critic = True`).  \n",
        "- **Final Critic Pipeline (`TensorDictSequential`)**:\n",
        "  - First applies **feature concatenation (`cat_module`)**.  \n",
        "  - Then passes through **`MultiAgentMLP` for value estimation** (`(group, \"state_action_value\")`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "LqVFCeRNWSfY"
      },
      "outputs": [],
      "source": [
        "critics = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    share_parameters_critic = True  # Can change for each group\n",
        "    MADDPG = True  # IDDPG if False, can change for each group\n",
        "\n",
        "    # This module applies the lambda function: reading the action and observation entries for the group\n",
        "    # and concatenating them in a new ``(group, \"obs_action\")`` entry\n",
        "    cat_module = TensorDictModule(\n",
        "        lambda obs, action: torch.cat([obs, action], dim=-1),\n",
        "        in_keys=[(group, \"observation\"), (group, \"action\")],\n",
        "        out_keys=[(group, \"obs_action\")],\n",
        "    )\n",
        "\n",
        "    critic_module = TensorDictModule(\n",
        "        module=MultiAgentMLP(\n",
        "            n_agent_inputs=env.observation_spec[group, \"observation\"].shape[-1]\n",
        "            + env.full_action_spec[group, \"action\"].shape[-1],\n",
        "            n_agent_outputs=1,  # 1 value per agent\n",
        "            n_agents=len(agents),\n",
        "            centralised=MADDPG,\n",
        "            share_params=share_parameters_critic,\n",
        "            device=device,\n",
        "            depth=2,\n",
        "            num_cells=256,\n",
        "            activation_class=torch.nn.Tanh,\n",
        "        ),\n",
        "        in_keys=[(group, \"obs_action\")],  # Read ``(group, \"obs_action\")``\n",
        "        out_keys=[\n",
        "            (group, \"state_action_value\")\n",
        "        ],  # Write ``(group, \"state_action_value\")``\n",
        "    )\n",
        "    critic_module = critic_module.to(device)\n",
        "\n",
        "    critics[group] = TensorDictSequential(\n",
        "        cat_module, critic_module\n",
        "    )  # Run them in sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W7Qq-fBWZRq",
        "outputId": "c466c322-afc5-4f3d-cb86-024ab9417d08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running value and policy for group 'agent': TensorDict(\n",
            "    fields={\n",
            "        agent: TensorDict(\n",
            "            fields={\n",
            "                action: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                episode_reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                obs_action: Tensor(shape=torch.Size([3, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                observation: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                param: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                state_action_value: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                terminated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                truncated: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "            batch_size=torch.Size([3]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        truncated: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=cuda:0,\n",
            "    is_shared=True)\n"
          ]
        }
      ],
      "source": [
        "reset_td = env.reset()\n",
        "for group, _agents in env.group_map.items():\n",
        "    print(\n",
        "        f\"Running value and policy for group '{group}':\",\n",
        "        critics[group](policies[group](reset_td)),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTfMG26J2L4J"
      },
      "source": [
        "### **Approach: Data Collection for Training**\n",
        "- **Combines all group exploration policies** into a single sequential module (`TensorDictSequential`), ensuring actions include exploration noise.  \n",
        "- **`SyncDataCollector` for Data Sampling**:\n",
        "  - Collects experience from the environment using **exploration policies**.  \n",
        "  - Runs on **`device` (GPU or CPU)** for efficiency.  \n",
        "  - **Frames per batch**: `1,000`, ensuring large enough updates per iteration.  \n",
        "  - **Total frames**: `50,000` (over `50` iterations).  \n",
        "- **Purpose**: Efficiently gathers on-policy experiences for training with replay buffers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "67asRrVjWa5O"
      },
      "outputs": [],
      "source": [
        "# Put exploration policies from each group in a sequence\n",
        "agents_exploration_policy = TensorDictSequential(*exploration_policies.values())\n",
        "\n",
        "collector = SyncDataCollector(\n",
        "    env,\n",
        "    agents_exploration_policy,\n",
        "    device=device,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "NghMzg_3Wcky"
      },
      "outputs": [],
      "source": [
        "#Standard in off policy algos for efficient data collections\n",
        "replay_buffers = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    replay_buffer = ReplayBuffer(\n",
        "        storage=LazyMemmapStorage(memory_size, device=\"cpu\"),\n",
        "        sampler=RandomSampler(),\n",
        "        batch_size=train_batch_size,\n",
        "    )\n",
        "    replay_buffer.append_transform(lambda batch: batch.to(\"cuda:0\"))\n",
        "    replay_buffers[group] = replay_buffer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for each agent in the cooperative group\n",
        "agent_params = {\n",
        "    \"agent_0\": {\n",
        "        \"alpha\": 0.7,\n",
        "        \"lam\": 0.8,\n",
        "        \"w_plus_prime_const\": 0.8,\n",
        "        \"w_minus_prime_const\": 0.2,\n",
        "    },\n",
        "    \"agent_1\": {\n",
        "        \"alpha\": 0.65,\n",
        "        \"lam\": 2.8,\n",
        "        \"w_plus_prime_const\": 0.25,\n",
        "        \"w_minus_prime_const\": 0.75,\n",
        "    },\n",
        "    \"agent_2\": {\n",
        "        \"alpha\": 0.65,\n",
        "        \"lam\": 2.8,\n",
        "        \"w_plus_prime_const\": 0.25,\n",
        "        \"w_minus_prime_const\": 0.75,\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "VyQcpsv2HGt8"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AdaptiveBehavioralParameters(nn.Module):\n",
        "    def __init__(self, init_alpha, init_lam, init_w_plus_gamma, init_w_minus_gamma):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.tensor(init_alpha, dtype=torch.float32))\n",
        "        self.lam = nn.Parameter(torch.tensor(init_lam, dtype=torch.float32))\n",
        "        # Instead of fixed w constants, now learn the gamma parameters for weighting functions:\n",
        "        self.w_plus_prime_gamma = nn.Parameter(torch.tensor(init_w_plus_gamma, dtype=torch.float32))\n",
        "        self.w_minus_prime_gamma = nn.Parameter(torch.tensor(init_w_minus_gamma, dtype=torch.float32))\n",
        "\n",
        "    def get_params(self):\n",
        "        return {\n",
        "            \"alpha\": self.alpha,\n",
        "            \"lam\": self.lam,\n",
        "            \"w_plus_prime_gamma\": self.w_plus_prime_gamma,\n",
        "            \"w_minus_prime_gamma\": self.w_minus_prime_gamma\n",
        "        }\n"
      ],
      "metadata": {
        "id": "GJHNzE-HMss_"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaptive_params = nn.ModuleDict({\n",
        "    \"agent_0\": AdaptiveBehavioralParameters(init_alpha=1.2, init_lam=1.5, init_w_plus_gamma=0.5, init_w_minus_gamma=0.69),\n",
        "    \"agent_1\": AdaptiveBehavioralParameters(init_alpha=1.2, init_lam=1.2, init_w_plus_gamma=0.5, init_w_minus_gamma=0.69),\n",
        "    \"agent_2\": AdaptiveBehavioralParameters(init_alpha=1.2, init_lam=1.2, init_w_plus_gamma=0.5, init_w_minus_gamma=0.69)\n",
        "})"
      ],
      "metadata": {
        "id": "ZZLUJo-OMvmZ"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_plus_prime_const = 0.2\n",
        "w_minus_prime_const = 0.8\n",
        "\n",
        "def w_plus_prime_dynamic(p, params, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Compute a dynamic weighting for gains.\n",
        "    p: a tensor of probabilities (values between 0 and 1)\n",
        "    params: dictionary containing a learnable parameter 'w_plus_prime_gamma'\n",
        "    \"\"\"\n",
        "    # Clamp p to avoid log(0)\n",
        "    p = torch.clamp(p, min=epsilon, max=1.0)\n",
        "    gamma = params.get(\"w_plus_prime_gamma\", torch.tensor(0.61, dtype=p.dtype, device=p.device))\n",
        "    # Prelec weighting derivative (an example formulation):\n",
        "    return torch.exp(-(-torch.log(p)) ** gamma)\n",
        "\n",
        "def w_minus_prime_dynamic(p, params, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Compute a dynamic weighting for losses.\n",
        "    p: a tensor of probabilities (values between 0 and 1)\n",
        "    params: dictionary containing a learnable parameter 'w_minus_prime_gamma'\n",
        "    \"\"\"\n",
        "    p = torch.clamp(p, min=epsilon, max=1.0)\n",
        "    gamma = params.get(\"w_minus_prime_gamma\", torch.tensor(0.69, dtype=p.dtype, device=p.device))\n",
        "    return torch.exp(-(-torch.log(p)) ** gamma)\n",
        "\n",
        "def compute_phi_linear(R):\n",
        "    \"\"\"\n",
        "    Compute linearized CPT sensitivity:\n",
        "    φ(R) ≈ w'_+(p*) * u^+(R) for R>=0, and -w'_-(p*) * u^-(R) for R<0.\n",
        "    \"\"\"\n",
        "    R = R.view(-1)\n",
        "    v = torch.where(R >= 0, u_plus(R), -u_minus(R))\n",
        "    phi = torch.where(R >= 0, w_plus_prime_const * v, -w_minus_prime_const * v)\n",
        "    return phi.mean()\n",
        "\n",
        "def compute_phi_linear_dynamic(R, adaptive_params, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Compute linearized CPT sensitivity using dynamic weighting:\n",
        "      φ(R) ≈ w'_+(p*) * u^+(R)  for R >= 0,\n",
        "           ≈ -w'_-(p*) * u^-(R) for R < 0.\n",
        "    Here p* is obtained by normalizing R into (0,1) (using a sigmoid, for example).\n",
        "    adaptive_params is a ModuleDict where each module’s get_params() returns a dict that\n",
        "    includes learnable parameters for the weighting functions (e.g., w_plus_prime_gamma).\n",
        "    \"\"\"\n",
        "    R = R.view(-1)\n",
        "    # For example, normalize R to (0,1) via a sigmoid.\n",
        "    p_star = torch.sigmoid(R)\n",
        "\n",
        "    # Compute the basic utility:\n",
        "    # (Assuming you have functions u_plus(R) and u_minus(R) already defined.)\n",
        "    # If not, you can use your existing stable versions:\n",
        "    v = torch.where(R >= 0, u_plus(R), -u_minus(R))\n",
        "\n",
        "    phi_values = []\n",
        "    for agent_id, param_module in adaptive_params.items():\n",
        "        params = param_module.get_params()\n",
        "        # For gains, use dynamic weighting from our new function:\n",
        "        weight_gain = w_plus_prime_dynamic(p_star, params, epsilon)\n",
        "        # For losses:\n",
        "        weight_loss = w_minus_prime_dynamic(p_star, params, epsilon)\n",
        "        phi = torch.where(R >= 0, weight_gain * v, -weight_loss * v)\n",
        "        phi_values.append(phi)\n",
        "    phi_stack = torch.stack(phi_values)\n",
        "    return phi_stack.mean(dim=0)\n",
        "\n",
        "\n",
        "def stable_u_plus_agent(x, params, epsilon=1e-6, min_val=1e-3):\n",
        "    # Ensure that x + epsilon is not too small; then compute in log space.\n",
        "    y = torch.clamp(x + epsilon, min=min_val)\n",
        "    return torch.exp(params[\"alpha\"] * torch.log(y))\n",
        "\n",
        "def u_minus_agent(x, params, epsilon=1e-6, min_val=1e-3):\n",
        "    # For the negative branch, ensure -x + epsilon is not too small.\n",
        "    y = torch.clamp(-x + epsilon, min=min_val)\n",
        "    return params[\"lam\"] * torch.pow(y, params[\"alpha\"])\n",
        "\n",
        "def compute_phi_cross_dynamic(R, adaptive_params, epsilon=1e-6, min_val=1e-3):\n",
        "    \"\"\"\n",
        "    Compute the cross-agent CPT sensitivity factor dynamically.\n",
        "    For each reward in R, first normalize it to (0,1) via a sigmoid (p_star).\n",
        "    Then compute the utility using u_plus for gains and u_minus_agent for losses.\n",
        "    Finally, apply dynamic weighting using the learnable weighting functions and average across agents.\n",
        "\n",
        "    Args:\n",
        "        R (Tensor): A tensor of rewards.\n",
        "        adaptive_params (ModuleDict): A dictionary (ModuleDict) of adaptive parameter modules.\n",
        "        epsilon (float): A small constant to prevent log(0).\n",
        "        min_val (float): A minimum value to clamp inputs.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: The averaged sensitivity factor φ.\n",
        "    \"\"\"\n",
        "    R = R.view(-1)\n",
        "    # Normalize rewards to [0,1] for the weighting function:\n",
        "    p_star = torch.sigmoid(R)\n",
        "\n",
        "    phi_values = []\n",
        "    for agent_id, param_module in adaptive_params.items():\n",
        "        params = param_module.get_params()\n",
        "        # For gains: clamp R+epsilon, then compute stable u_plus:\n",
        "        y = torch.clamp(R + epsilon, min=min_val)\n",
        "        u_plus_val = torch.exp(params[\"alpha\"] * torch.log(y))\n",
        "        # For losses, use u_minus_agent (as defined elsewhere)\n",
        "        v = torch.where(R >= 0, u_plus_val, -u_minus_agent(R, params, epsilon, min_val))\n",
        "        # Now compute dynamic weights from p_star using our new functions:\n",
        "        weight_gain = w_plus_prime_dynamic(p_star, params, epsilon)\n",
        "        weight_loss = w_minus_prime_dynamic(p_star, params, epsilon)\n",
        "        phi = torch.where(R >= 0, weight_gain * v, -weight_loss * v)\n",
        "        phi_values.append(phi)\n",
        "\n",
        "    phi_stack = torch.stack(phi_values)\n",
        "    return phi_stack.mean(dim=0)\n",
        "\n",
        "def C_transform_cross_dynamic(x, adaptive_params, epsilon=1e-6, min_val=1e-3):\n",
        "    \"\"\"\n",
        "    Compute the CPT-transformed target value by averaging each agent’s transformation,\n",
        "    using dynamic weighting rather than fixed constants.\n",
        "    \"\"\"\n",
        "    transformed_vals = []\n",
        "    # Optionally, define a normalization for x to obtain a probability. For instance:\n",
        "    p_star = torch.sigmoid(x)\n",
        "    for agent_id, param_module in adaptive_params.items():\n",
        "        params = param_module.get_params()\n",
        "        y = torch.clamp(x + epsilon, min=min_val)\n",
        "        # Compute u_plus for gains:\n",
        "        u_plus_val = torch.exp(params[\"alpha\"] * torch.log(y))\n",
        "        # For losses, use u_minus_agent as before:\n",
        "        transformed = torch.where(\n",
        "            x >= 0,\n",
        "            w_plus_prime_dynamic(p_star, params, epsilon) * u_plus_val,\n",
        "            -w_minus_prime_dynamic(p_star, params, epsilon) * u_minus_agent(x, params, epsilon, min_val)\n",
        "        )\n",
        "        # Clamp the final output for safety:\n",
        "        transformed = torch.clamp(transformed, min=-1e6, max=1e6)\n",
        "        transformed_vals.append(transformed)\n",
        "    return torch.stack(transformed_vals).mean(dim=0)\n",
        "\n",
        "\n",
        "def C_transform(x):\n",
        "    \"\"\"\n",
        "    Simple CPT transformation on one-step return:\n",
        "    C(x) ≈ w'_+(p*) * u^+(x) if x >= 0, else -w'_-(p*) * u^-(x).\n",
        "    \"\"\"\n",
        "    return torch.where(x >= 0, w_plus_prime_const * u_plus(x), -w_minus_prime_const * u_minus(x))"
      ],
      "metadata": {
        "id": "h9DCI7ydFUq_"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from tensordict import TensorDict, TensorDictBase, TensorDictParams\n",
        "from tensordict.nn import dispatch, TensorDictModule\n",
        "\n",
        "from tensordict.utils import NestedKey, unravel_key\n",
        "from torchrl.modules.tensordict_module.actors import ActorCriticWrapper\n",
        "from torchrl.objectives.common import LossModule\n",
        "from torchrl.objectives.utils import (\n",
        "    _cache_values,\n",
        "    _GAMMA_LMBDA_DEPREC_ERROR,\n",
        "    _reduce,\n",
        "    default_value_kwargs,\n",
        "    distance_loss,\n",
        "    ValueEstimators,\n",
        ")\n",
        "from torchrl.objectives.value import TD0Estimator, TD1Estimator, TDLambdaEstimator\n",
        "\n",
        "\n",
        "class CPTDDPGLoss(LossModule):\n",
        "    \"\"\"The DDPG Loss class.\n",
        "\n",
        "    Args:\n",
        "        actor_network (TensorDictModule): a policy operator.\n",
        "        value_network (TensorDictModule): a Q value operator.\n",
        "        loss_function (str): loss function for the value discrepancy. Can be one of \"l1\", \"l2\" or \"smooth_l1\".\n",
        "        delay_actor (bool, optional): whether to separate the target actor networks from the actor networks used for\n",
        "            data collection. Default is ``False``.\n",
        "        delay_value (bool, optional): whether to separate the target value networks from the value networks used for\n",
        "            data collection. Default is ``True``.\n",
        "        separate_losses (bool, optional): if ``True``, shared parameters between\n",
        "            policy and critic will only be trained on the policy loss.\n",
        "            Defaults to ``False``, i.e., gradients are propagated to shared\n",
        "            parameters for both policy and critic losses.\n",
        "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
        "            ``\"none\"`` | ``\"mean\"`` | ``\"sum\"``. ``\"none\"``: no reduction will be applied,\n",
        "            ``\"mean\"``: the sum of the output will be divided by the number of\n",
        "            elements in the output, ``\"sum\"``: the output will be summed. Default: ``\"mean\"``.\n",
        "\n",
        "    Examples:\n",
        "        >>> import torch\n",
        "        >>> from torch import nn\n",
        "        >>> from torchrl.data import Bounded\n",
        "        >>> from torchrl.modules.tensordict_module.actors import Actor, ValueOperator\n",
        "        >>> from torchrl.objectives.ddpg import DDPGLoss\n",
        "        >>> from tensordict import TensorDict\n",
        "        >>> n_act, n_obs = 4, 3\n",
        "        >>> spec = Bounded(-torch.ones(n_act), torch.ones(n_act), (n_act,))\n",
        "        >>> actor = Actor(spec=spec, module=nn.Linear(n_obs, n_act))\n",
        "        >>> class ValueClass(nn.Module):\n",
        "        ...     def __init__(self):\n",
        "        ...         super().__init__()\n",
        "        ...         self.linear = nn.Linear(n_obs + n_act, 1)\n",
        "        ...     def forward(self, obs, act):\n",
        "        ...         return self.linear(torch.cat([obs, act], -1))\n",
        "        >>> module = ValueClass()\n",
        "        >>> value = ValueOperator(\n",
        "        ...     module=module,\n",
        "        ...     in_keys=[\"observation\", \"action\"])\n",
        "        >>> loss = DDPGLoss(actor, value)\n",
        "        >>> batch = [2, ]\n",
        "        >>> data = TensorDict({\n",
        "        ...        \"observation\": torch.randn(*batch, n_obs),\n",
        "        ...        \"action\": spec.rand(batch),\n",
        "        ...        (\"next\", \"done\"): torch.zeros(*batch, 1, dtype=torch.bool),\n",
        "        ...        (\"next\", \"terminated\"): torch.zeros(*batch, 1, dtype=torch.bool),\n",
        "        ...        (\"next\", \"reward\"): torch.randn(*batch, 1),\n",
        "        ...        (\"next\", \"observation\"): torch.randn(*batch, n_obs),\n",
        "        ...    }, batch)\n",
        "        >>> loss(data)\n",
        "        TensorDict(\n",
        "            fields={\n",
        "                loss_actor: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                loss_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                pred_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                pred_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                target_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                target_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
        "            batch_size=torch.Size([]),\n",
        "            device=None,\n",
        "            is_shared=False)\n",
        "\n",
        "    This class is compatible with non-tensordict based modules too and can be\n",
        "    used without recurring to any tensordict-related primitive. In this case,\n",
        "    the expected keyword arguments are:\n",
        "    ``[\"next_reward\", \"next_done\", \"next_terminated\"]`` + in_keys of the actor_network and value_network.\n",
        "    The return value is a tuple of tensors in the following order:\n",
        "    ``[\"loss_actor\", \"loss_value\", \"pred_value\", \"target_value\", \"pred_value_max\", \"target_value_max\"]``\n",
        "\n",
        "    Examples:\n",
        "        >>> import torch\n",
        "        >>> from torch import nn\n",
        "        >>> from torchrl.data import Bounded\n",
        "        >>> from torchrl.modules.tensordict_module.actors import Actor, ValueOperator\n",
        "        >>> from torchrl.objectives.ddpg import DDPGLoss\n",
        "        >>> _ = torch.manual_seed(42)\n",
        "        >>> n_act, n_obs = 4, 3\n",
        "        >>> spec = Bounded(-torch.ones(n_act), torch.ones(n_act), (n_act,))\n",
        "        >>> actor = Actor(spec=spec, module=nn.Linear(n_obs, n_act))\n",
        "        >>> class ValueClass(nn.Module):\n",
        "        ...     def __init__(self):\n",
        "        ...         super().__init__()\n",
        "        ...         self.linear = nn.Linear(n_obs + n_act, 1)\n",
        "        ...     def forward(self, obs, act):\n",
        "        ...         return self.linear(torch.cat([obs, act], -1))\n",
        "        >>> module = ValueClass()\n",
        "        >>> value = ValueOperator(\n",
        "        ...     module=module,\n",
        "        ...     in_keys=[\"observation\", \"action\"])\n",
        "        >>> loss = DDPGLoss(actor, value)\n",
        "        >>> loss_actor, loss_value, pred_value, target_value, pred_value_max, target_value_max = loss(\n",
        "        ...     observation=torch.randn(n_obs),\n",
        "        ...     action=spec.rand(),\n",
        "        ...     next_done=torch.zeros(1, dtype=torch.bool),\n",
        "        ...     next_terminated=torch.zeros(1, dtype=torch.bool),\n",
        "        ...     next_observation=torch.randn(n_obs),\n",
        "        ...     next_reward=torch.randn(1))\n",
        "        >>> loss_actor.backward()\n",
        "\n",
        "    The output keys can also be filtered using the :meth:`DDPGLoss.select_out_keys`\n",
        "    method.\n",
        "\n",
        "    Examples:\n",
        "        >>> loss.select_out_keys('loss_actor', 'loss_value')\n",
        "        >>> loss_actor, loss_value = loss(\n",
        "        ...     observation=torch.randn(n_obs),\n",
        "        ...     action=spec.rand(),\n",
        "        ...     next_done=torch.zeros(1, dtype=torch.bool),\n",
        "        ...     next_terminated=torch.zeros(1, dtype=torch.bool),\n",
        "        ...     next_observation=torch.randn(n_obs),\n",
        "        ...     next_reward=torch.randn(1))\n",
        "        >>> loss_actor.backward()\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    @dataclass\n",
        "    class _AcceptedKeys:\n",
        "        \"\"\"Maintains default values for all configurable tensordict keys.\n",
        "\n",
        "        This class defines which tensordict keys can be set using '.set_keys(key_name=key_value)' and their\n",
        "        default values.\n",
        "\n",
        "        Attributes:\n",
        "            state_action_value (NestedKey): The input tensordict key where the\n",
        "                state action value is expected. Will be used for the underlying\n",
        "                value estimator as value key. Defaults to ``\"state_action_value\"``.\n",
        "            priority (NestedKey): The input tensordict key where the target\n",
        "                priority is written to. Defaults to ``\"td_error\"``.\n",
        "            reward (NestedKey): The input tensordict key where the reward is expected.\n",
        "                Will be used for the underlying value estimator. Defaults to ``\"reward\"``.\n",
        "            done (NestedKey): The key in the input TensorDict that indicates\n",
        "                whether a trajectory is done. Will be used for the underlying value estimator.\n",
        "                Defaults to ``\"done\"``.\n",
        "            terminated (NestedKey): The key in the input TensorDict that indicates\n",
        "                whether a trajectory is terminated. Will be used for the underlying value estimator.\n",
        "                Defaults to ``\"terminated\"``.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        state_action_value: NestedKey = \"state_action_value\"\n",
        "        priority: NestedKey = \"td_error\"\n",
        "        reward: NestedKey = \"reward\"\n",
        "        done: NestedKey = \"done\"\n",
        "        terminated: NestedKey = \"terminated\"\n",
        "\n",
        "    tensor_keys: _AcceptedKeys\n",
        "    default_keys = _AcceptedKeys\n",
        "    default_value_estimator: ValueEstimators = ValueEstimators.TD0\n",
        "    out_keys = [\n",
        "        \"loss_actor\",\n",
        "        \"loss_value\",\n",
        "        \"pred_value\",\n",
        "        \"target_value\",\n",
        "        \"pred_value_max\",\n",
        "        \"target_value_max\",\n",
        "    ]\n",
        "\n",
        "    actor_network: TensorDictModule\n",
        "    value_network: actor_network\n",
        "    actor_network_params: TensorDictParams\n",
        "    value_network_params: TensorDictParams\n",
        "    target_actor_network_params: TensorDictParams\n",
        "    target_value_network_params: TensorDictParams\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        actor_network: TensorDictModule,\n",
        "        value_network: TensorDictModule,\n",
        "        *,\n",
        "        loss_function: str = \"l2\",\n",
        "        delay_actor: bool = False,\n",
        "        delay_value: bool = True,\n",
        "        gamma: float = None,\n",
        "        separate_losses: bool = False,\n",
        "        reduction: str = None,\n",
        "    ) -> None:\n",
        "        self._in_keys = None\n",
        "        if reduction is None:\n",
        "            reduction = \"mean\"\n",
        "        super().__init__()\n",
        "        self.delay_actor = delay_actor\n",
        "        self.delay_value = delay_value\n",
        "\n",
        "        actor_critic = ActorCriticWrapper(actor_network, value_network)\n",
        "        params = TensorDict.from_module(actor_critic)\n",
        "        params_meta = params.apply(\n",
        "            self._make_meta_params, device=torch.device(\"meta\"), filter_empty=False\n",
        "        )\n",
        "        with params_meta.to_module(actor_critic):\n",
        "            self.__dict__[\"actor_critic\"] = deepcopy(actor_critic)\n",
        "\n",
        "        self.convert_to_functional(\n",
        "            actor_network,\n",
        "            \"actor_network\",\n",
        "            create_target_params=self.delay_actor,\n",
        "        )\n",
        "        if separate_losses:\n",
        "            # we want to make sure there are no duplicates in the params: the\n",
        "            # params of critic must be refs to actor if they're shared\n",
        "            policy_params = list(actor_network.parameters())\n",
        "        else:\n",
        "            policy_params = None\n",
        "        self.convert_to_functional(\n",
        "            value_network,\n",
        "            \"value_network\",\n",
        "            create_target_params=self.delay_value,\n",
        "            compare_against=policy_params,\n",
        "        )\n",
        "        self.actor_critic.module[0] = self.actor_network\n",
        "        self.actor_critic.module[1] = self.value_network\n",
        "\n",
        "        self.actor_in_keys = actor_network.in_keys\n",
        "        self.value_exclusive_keys = set(self.value_network.in_keys) - (\n",
        "            set(self.actor_in_keys) | set(self.actor_network.out_keys)\n",
        "        )\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.reduction = reduction\n",
        "        if gamma is not None:\n",
        "            raise TypeError(_GAMMA_LMBDA_DEPREC_ERROR)\n",
        "\n",
        "    def _forward_value_estimator_keys(self, **kwargs) -> None:\n",
        "        if self._value_estimator is not None:\n",
        "            self._value_estimator.set_keys(\n",
        "                value=self._tensor_keys.state_action_value,\n",
        "                reward=self._tensor_keys.reward,\n",
        "                done=self._tensor_keys.done,\n",
        "                terminated=self._tensor_keys.terminated,\n",
        "            )\n",
        "        self._set_in_keys()\n",
        "\n",
        "    def _set_in_keys(self):\n",
        "        in_keys = {\n",
        "            unravel_key((\"next\", self.tensor_keys.reward)),\n",
        "            unravel_key((\"next\", self.tensor_keys.done)),\n",
        "            unravel_key((\"next\", self.tensor_keys.terminated)),\n",
        "            *self.actor_in_keys,\n",
        "            *[unravel_key((\"next\", key)) for key in self.actor_in_keys],\n",
        "            *self.value_network.in_keys,\n",
        "            *[unravel_key((\"next\", key)) for key in self.value_network.in_keys],\n",
        "        }\n",
        "        self._in_keys = sorted(in_keys, key=str)\n",
        "\n",
        "    @property\n",
        "    def in_keys(self):\n",
        "        if self._in_keys is None:\n",
        "            self._set_in_keys()\n",
        "        return self._in_keys\n",
        "\n",
        "    @in_keys.setter\n",
        "    def in_keys(self, values):\n",
        "        self._in_keys = values\n",
        "\n",
        "\n",
        "    def _clear_weakrefs(self, *tds):\n",
        "        if is_compiling():\n",
        "            # Waiting for weakrefs reconstruct to be supported by compile\n",
        "            for td in tds:\n",
        "                if isinstance(td, str):\n",
        "                    td = getattr(self, td, None)\n",
        "                if not is_tensor_collection(td):\n",
        "                    continue\n",
        "                td.clear_refs_for_compile_()\n",
        "\n",
        "    @dispatch\n",
        "    def forward(self, tensordict: TensorDictBase) -> TensorDict:\n",
        "        \"\"\"Computes the DDPG losses given a tensordict sampled from the replay buffer.\n",
        "\n",
        "        This function will also write a \"td_error\" key that can be used by prioritized replay buffers to assign\n",
        "            a priority to items in the tensordict.\n",
        "\n",
        "        Args:\n",
        "            tensordict (TensorDictBase): a tensordict with keys [\"done\", \"terminated\", \"reward\"] and the in_keys of the actor\n",
        "                and value networks.\n",
        "\n",
        "        Returns:\n",
        "            a tuple of 2 tensors containing the DDPG loss.\n",
        "\n",
        "        \"\"\"\n",
        "        loss_value, metadata = self.loss_value(tensordict)\n",
        "        loss_actor, metadata_actor = self.loss_actor(tensordict)\n",
        "        metadata.update(metadata_actor)\n",
        "        td_out = TensorDict(\n",
        "            source={\"loss_actor\": loss_actor, \"loss_value\": loss_value, **metadata},\n",
        "            batch_size=[],\n",
        "        )\n",
        "        self._clear_weakrefs(\n",
        "            tensordict,\n",
        "            td_out,\n",
        "            \"value_network_params\",\n",
        "            \"target_value_network_params\",\n",
        "            \"target_actor_network_params\",\n",
        "            \"actor_network_params\",\n",
        "        )\n",
        "        return td_out\n",
        "\n",
        "    def loss_actor(self, tensordict: TensorDictBase) -> Tuple[torch.Tensor, dict]:\n",
        "        td_copy = tensordict.select(\n",
        "            *self.actor_in_keys, *self.value_exclusive_keys, strict=False\n",
        "        ).detach()\n",
        "\n",
        "        with self.actor_network_params.to_module(self.actor_network):\n",
        "            td_copy = self.actor_network(td_copy)\n",
        "\n",
        "        with self._cached_detached_value_params.to_module(self.value_network):\n",
        "            td_copy = self.value_network(td_copy)\n",
        "\n",
        "        actions = td_copy.get((self.actor_network.in_keys[0][0], \"action\"))\n",
        "        Q_values = td_copy.get(self.tensor_keys.state_action_value).squeeze(-1)\n",
        "        grad_Q = torch.autograd.grad(Q_values.sum(), actions, retain_graph=True)[0]\n",
        "\n",
        "        # Retrieve the cooperative reward (assuming common group 'agents')\n",
        "        returns = tensordict.get((\"agent\", \"episode_reward\")).view(-1)\n",
        "        # Now use the dynamic version that computes φ using your learnable weighting functions:\n",
        "        phi_factor = compute_phi_cross_dynamic(returns, adaptive_params)\n",
        "\n",
        "        policy_gradient = actions * grad_Q\n",
        "        loss_actor = -phi_factor.mean() * policy_gradient.mean()\n",
        "\n",
        "        return _reduce(loss_actor, self.reduction), {}\n",
        "\n",
        "\n",
        "    def loss_value(self, tensordict: TensorDictBase) -> Tuple[torch.Tensor, dict]:\n",
        "        td_copy = tensordict.select(*self.value_network.in_keys, strict=False).detach()\n",
        "        with self.value_network_params.to_module(self.value_network):\n",
        "            self.value_network(td_copy)\n",
        "        pred_val = td_copy.get(self.tensor_keys.state_action_value).squeeze(-1)\n",
        "\n",
        "        target_value = self.value_estimator.value_estimate(\n",
        "            tensordict, target_params=self._cached_target_params\n",
        "        ).squeeze(-1)\n",
        "        # Replace the old transformation with your new dynamic version:\n",
        "        target_value_CPT = C_transform_cross_dynamic(target_value, adaptive_params)\n",
        "\n",
        "        loss_value = distance_loss(pred_val, target_value_CPT, loss_function=self.loss_function)\n",
        "        tensordict.set(\"target_value_CPT\", target_value_CPT, inplace=True)\n",
        "\n",
        "        td_error = (pred_val - target_value_CPT).pow(2).detach()\n",
        "        tensordict.set(self.tensor_keys.priority, td_error, inplace=True)\n",
        "\n",
        "        metadata = {\n",
        "            \"td_error\": td_error,\n",
        "            \"pred_value\": pred_val,\n",
        "            \"target_value_CPT\": target_value_CPT,\n",
        "        }\n",
        "        return _reduce(loss_value, self.reduction), metadata\n",
        "\n",
        "\n",
        "    def make_value_estimator(self, value_type: ValueEstimators = None, **hyperparams):\n",
        "        if value_type is None:\n",
        "            value_type = self.default_value_estimator\n",
        "        self.value_type = value_type\n",
        "        hp = dict(default_value_kwargs(value_type))\n",
        "        if hasattr(self, \"gamma\"):\n",
        "            hp[\"gamma\"] = self.gamma\n",
        "        hp.update(hyperparams)\n",
        "        if value_type == ValueEstimators.TD1:\n",
        "            self._value_estimator = TD1Estimator(value_network=self.actor_critic, **hp)\n",
        "        elif value_type == ValueEstimators.TD0:\n",
        "            self._value_estimator = TD0Estimator(value_network=self.actor_critic, **hp)\n",
        "        elif value_type == ValueEstimators.GAE:\n",
        "            raise NotImplementedError(\n",
        "                f\"Value type {value_type} it not implemented for loss {type(self)}.\"\n",
        "            )\n",
        "        elif value_type == ValueEstimators.TDLambda:\n",
        "            self._value_estimator = TDLambdaEstimator(\n",
        "                value_network=self.actor_critic, **hp\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unknown value type {value_type}\")\n",
        "\n",
        "        tensor_keys = {\n",
        "            \"value\": self.tensor_keys.state_action_value,\n",
        "            \"reward\": self.tensor_keys.reward,\n",
        "            \"done\": self.tensor_keys.done,\n",
        "            \"terminated\": self.tensor_keys.terminated,\n",
        "        }\n",
        "        self._value_estimator.set_keys(**tensor_keys)\n",
        "\n",
        "    @property\n",
        "    @_cache_values\n",
        "    def _cached_target_params(self):\n",
        "        target_params = TensorDict(\n",
        "            {\n",
        "                \"module\": {\n",
        "                    \"0\": self.target_actor_network_params,\n",
        "                    \"1\": self.target_value_network_params,\n",
        "                }\n",
        "            },\n",
        "            batch_size=self.target_actor_network_params.batch_size,\n",
        "            device=self.target_actor_network_params.device,\n",
        "        )\n",
        "        return target_params\n",
        "\n",
        "    @property\n",
        "    @_cache_values\n",
        "    def _cached_detached_value_params(self):\n",
        "        return self.value_network_params.detach()"
      ],
      "metadata": {
        "id": "FRx2-pu1FZhH"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szedGF5b2g2V"
      },
      "source": [
        "### **Approach: Loss Calculation & Optimization**\n",
        "#### **Defining the Loss Function (`DDPGLoss`)**\n",
        "- **Uses separate actor and critic losses**:\n",
        "  - **`actor_network = policies[group]`**: Optimizes agent actions.\n",
        "  - **`value_network = critics[group]`**: Estimates state-action values.\n",
        "- **Target Network (`delay_value = True`)**:\n",
        "  - Uses a **target critic** for more stable learning.\n",
        "  - **Loss function**: Mean Squared Error (`\"l2\"`).\n",
        "- **Key Assignments**:\n",
        "  - **State-action value**: `(group, \"state_action_value\")`.\n",
        "  - **Reward Signal**: `(group, \"reward\")`.\n",
        "  - **Termination Handling**: `(group, \"done\")` and `(group, \"terminated\")`.\n",
        "- **TD(0) Estimator**: Uses **Temporal Difference (TD) learning** with discount factor `γ = 0.99`.\n",
        "\n",
        "#### **Target Network Updates**\n",
        "- **Soft update mechanism (`SoftUpdate`)**:\n",
        "  - **Gradually updates target networks** using `τ = 0.005`.\n",
        "  - Prevents drastic changes, improving stability.\n",
        "\n",
        "#### **Optimizers**\n",
        "- **Separate Adam optimizers for actor and critic networks**:\n",
        "  - **`loss_actor`**: Updates policy parameters.\n",
        "  - **`loss_value`**: Updates value network parameters.\n",
        "- **Learning rate (`lr = 3e-4`)** ensures smooth gradient updates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "bJ4u6U6EWfBT"
      },
      "outputs": [],
      "source": [
        "losses = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    loss_module = CPTDDPGLoss(\n",
        "        actor_network=policies[group],  # Use the non-explorative policies\n",
        "        value_network=critics[group],\n",
        "        delay_value=True,  # Whether to use a target network for the value\n",
        "        loss_function=\"l2\",\n",
        "    )\n",
        "    loss_module.set_keys(\n",
        "        state_action_value=(group, \"state_action_value\"),\n",
        "        reward=(group, \"reward\"),\n",
        "        done=(group, \"done\"),\n",
        "        terminated=(group, \"terminated\"),\n",
        "    )\n",
        "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
        "\n",
        "    losses[group] = loss_module\n",
        "\n",
        "target_updaters = {\n",
        "    group: SoftUpdate(loss, tau=polyak_tau) for group, loss in losses.items()\n",
        "}\n",
        "\n",
        "optimisers = {\n",
        "    group: {\n",
        "        \"loss_actor\": torch.optim.Adam(\n",
        "            loss.actor_network_params.flatten_keys().values(), lr=lr\n",
        "        ),\n",
        "        \"loss_value\": torch.optim.Adam(\n",
        "            loss.value_network_params.flatten_keys().values(), lr=lr\n",
        "        ),\n",
        "    }\n",
        "    for group, loss in losses.items()\n",
        "}\n",
        "\n",
        "optimizer_behavioral = torch.optim.Adam(adaptive_params.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "EjBvnOLGWgnp"
      },
      "outputs": [],
      "source": [
        "def process_batch(batch: TensorDictBase) -> TensorDictBase:\n",
        "    \"\"\"\n",
        "    If the `(group, \"terminated\")` and `(group, \"done\")` keys are not present, create them by expanding\n",
        "    `\"terminated\"` and `\"done\"`.\n",
        "    This is needed to present them with the same shape as the reward to the loss.\n",
        "    \"\"\"\n",
        "    for group in env.group_map.keys():\n",
        "        keys = list(batch.keys(True, True))\n",
        "        group_shape = batch.get_item_shape(group)\n",
        "        nested_done_key = (\"next\", group, \"done\")\n",
        "        nested_terminated_key = (\"next\", group, \"terminated\")\n",
        "        if nested_done_key not in keys:\n",
        "            batch.set(\n",
        "                nested_done_key,\n",
        "                batch.get((\"next\", \"done\")).unsqueeze(-1).expand((*group_shape, 1)),\n",
        "            )\n",
        "        if nested_terminated_key not in keys:\n",
        "            batch.set(\n",
        "                nested_terminated_key,\n",
        "                batch.get((\"next\", \"terminated\"))\n",
        "                .unsqueeze(-1)\n",
        "                .expand((*group_shape, 1)),\n",
        "            )\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHXNfxaJ2sQA"
      },
      "source": [
        "### **Approach: Training Loop & Optimization**\n",
        "#### **Progress Bar & Logging Setup**\n",
        "- **Uses `tqdm`** to track training iterations with episode rewards.  \n",
        "- **Initializes `episode_reward_mean_map`** to store reward trends per agent group.  \n",
        "- **Creates `train_group_map`** as a copy of `env.group_map`, allowing dynamic updates.\n",
        "\n",
        "#### **Main Training Loop**\n",
        "- **Iterates through `collector`** to process training batches.\n",
        "- **Preprocesses Data (`process_batch`)**:\n",
        "  - Expands done/terminated keys for proper loss computation.\n",
        "  - **Excludes data from other groups** to isolate training signals.\n",
        "  - **Reshapes batch** to align with replay buffer dimensions.\n",
        "- **Stores Data in Replay Buffer (`replay_buffers[group].extend(group_batch)`)**.\n",
        "\n",
        "#### **Optimization Steps**\n",
        "- **Samples batches (`n_optimiser_steps = 100`)** from replay buffer.\n",
        "- **Computes & Backpropagates Loss**:\n",
        "  - Extracts actor (`loss_actor`) and critic (`loss_value`) loss.\n",
        "  - **Clips gradients (`max_grad_norm = 1.0`)** to prevent instability.\n",
        "  - **Optimizes parameters with Adam**, resetting gradients after each step.\n",
        "- **Soft Updates (`target_updaters[group].step()`)**:\n",
        "  - Gradually syncs target networks using `τ = 0.005`.\n",
        "\n",
        "#### **Adaptive Exploration**\n",
        "- **Anneals exploration noise (`sigma`)** based on the number of frames processed.\n",
        "\n",
        "#### **Training Halting Condition**\n",
        "- **Stops training evaders after `iteration_when_stop_training_evaders = 25`**.\n",
        "\n",
        "#### **Logging & Progress Tracking**\n",
        "- **Computes mean episode reward** for each group.\n",
        "- **Updates `tqdm` progress bar** with latest reward values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_adaptive_loss(replay_sample, value_estimator, adaptive_params, target_params, epsilon=1e-6, min_val=1e-3):\n",
        "    # Compute the standard target Q-value:\n",
        "    target_value = value_estimator.value_estimate(replay_sample, target_params=target_params).squeeze(-1)\n",
        "    # Compute the CPT-transformed target using adaptive parameters:\n",
        "    target_value_CPT = C_transform_cross_dynamic(target_value, adaptive_params, epsilon, min_val)\n",
        "    base_adaptive_loss = torch.mean((target_value_CPT - target_value) ** 2)\n",
        "    return base_adaptive_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "n_e_UTSVSwLW"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rwLjnzsLWjzS",
        "outputId": "cef9e4fb-949e-46d7-a52b-237c361e9113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13389629125595093:   4%|▎         | 7/200 [03:34<1:38:38, 30.67s/it]\n",
            "\n",
            "episode_reward_mean_agent = 0.13589432835578918:   0%|          | 1/200 [00:20<1:08:48, 20.75s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0 - agent_0 alpha: 1.200000, grad mean: 2.930393\n",
            "Iter 0 - agent_0 lam: 1.500000, grad mean: 1.660273\n",
            "Iter 0 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 3.674483\n",
            "Iter 0 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.322690\n",
            "Iter 0 - agent_1 alpha: 1.200000, grad mean: 2.743518\n",
            "Iter 0 - agent_1 lam: 1.200000, grad mean: 1.660273\n",
            "Iter 0 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 3.674483\n",
            "Iter 0 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.258152\n",
            "Iter 0 - agent_2 alpha: 1.200000, grad mean: 2.743518\n",
            "Iter 0 - agent_2 lam: 1.200000, grad mean: 1.660273\n",
            "Iter 0 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 3.674483\n",
            "Iter 0 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.258152\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13417001068592072:   1%|          | 2/200 [00:41<1:08:27, 20.74s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 1 - agent_0 alpha: 1.200000, grad mean: 3.266326\n",
            "Iter 1 - agent_0 lam: 1.500000, grad mean: 1.717242\n",
            "Iter 1 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 4.220649\n",
            "Iter 1 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.332104\n",
            "Iter 1 - agent_1 alpha: 1.200000, grad mean: 3.071735\n",
            "Iter 1 - agent_1 lam: 1.200000, grad mean: 1.717242\n",
            "Iter 1 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 4.220649\n",
            "Iter 1 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.265683\n",
            "Iter 1 - agent_2 alpha: 1.200000, grad mean: 3.071735\n",
            "Iter 1 - agent_2 lam: 1.200000, grad mean: 1.717242\n",
            "Iter 1 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 4.220649\n",
            "Iter 1 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.265683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13828521966934204:   2%|▏         | 3/200 [01:02<1:08:19, 20.81s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 2 - agent_0 alpha: 1.200000, grad mean: 3.602347\n",
            "Iter 2 - agent_0 lam: 1.500000, grad mean: 1.774439\n",
            "Iter 2 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 4.770781\n",
            "Iter 2 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.342680\n",
            "Iter 2 - agent_1 alpha: 1.200000, grad mean: 3.400511\n",
            "Iter 2 - agent_1 lam: 1.200000, grad mean: 1.774439\n",
            "Iter 2 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 4.770781\n",
            "Iter 2 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.274144\n",
            "Iter 2 - agent_2 alpha: 1.200000, grad mean: 3.400511\n",
            "Iter 2 - agent_2 lam: 1.200000, grad mean: 1.774439\n",
            "Iter 2 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 4.770781\n",
            "Iter 2 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.274144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13877764344215393:   2%|▏         | 4/200 [01:23<1:08:12, 20.88s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 3 - agent_0 alpha: 1.200000, grad mean: 3.935788\n",
            "Iter 3 - agent_0 lam: 1.500000, grad mean: 1.822309\n",
            "Iter 3 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 5.336387\n",
            "Iter 3 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.350645\n",
            "Iter 3 - agent_1 alpha: 1.200000, grad mean: 3.727357\n",
            "Iter 3 - agent_1 lam: 1.200000, grad mean: 1.822309\n",
            "Iter 3 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 5.336387\n",
            "Iter 3 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.280516\n",
            "Iter 3 - agent_2 alpha: 1.200000, grad mean: 3.727357\n",
            "Iter 3 - agent_2 lam: 1.200000, grad mean: 1.822309\n",
            "Iter 3 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 5.336387\n",
            "Iter 3 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.280516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13505150377750397:   2%|▎         | 5/200 [01:44<1:07:42, 20.83s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 4 - agent_0 alpha: 1.200000, grad mean: 4.271237\n",
            "Iter 4 - agent_0 lam: 1.500000, grad mean: 1.868312\n",
            "Iter 4 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 5.907833\n",
            "Iter 4 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.358169\n",
            "Iter 4 - agent_1 alpha: 1.200000, grad mean: 4.056433\n",
            "Iter 4 - agent_1 lam: 1.200000, grad mean: 1.868312\n",
            "Iter 4 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 5.907833\n",
            "Iter 4 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.286535\n",
            "Iter 4 - agent_2 alpha: 1.200000, grad mean: 4.056433\n",
            "Iter 4 - agent_2 lam: 1.200000, grad mean: 1.868312\n",
            "Iter 4 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 5.907833\n",
            "Iter 4 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.286535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13710802793502808:   3%|▎         | 6/200 [02:04<1:07:20, 20.83s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 5 - agent_0 alpha: 1.200000, grad mean: 4.601694\n",
            "Iter 5 - agent_0 lam: 1.500000, grad mean: 1.911206\n",
            "Iter 5 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 6.465546\n",
            "Iter 5 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.365762\n",
            "Iter 5 - agent_1 alpha: 1.200000, grad mean: 4.381077\n",
            "Iter 5 - agent_1 lam: 1.200000, grad mean: 1.911206\n",
            "Iter 5 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 6.465546\n",
            "Iter 5 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.292609\n",
            "Iter 5 - agent_2 alpha: 1.200000, grad mean: 4.381077\n",
            "Iter 5 - agent_2 lam: 1.200000, grad mean: 1.911206\n",
            "Iter 5 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 6.465546\n",
            "Iter 5 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.292609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.1374434381723404:   4%|▎         | 7/200 [02:25<1:06:54, 20.80s/it] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 6 - agent_0 alpha: 1.200000, grad mean: 4.934233\n",
            "Iter 6 - agent_0 lam: 1.500000, grad mean: 1.948621\n",
            "Iter 6 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 7.028308\n",
            "Iter 6 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.371223\n",
            "Iter 6 - agent_1 alpha: 1.200000, grad mean: 4.707945\n",
            "Iter 6 - agent_1 lam: 1.200000, grad mean: 1.948621\n",
            "Iter 6 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 7.028308\n",
            "Iter 6 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.296978\n",
            "Iter 6 - agent_2 alpha: 1.200000, grad mean: 4.707945\n",
            "Iter 6 - agent_2 lam: 1.200000, grad mean: 1.948621\n",
            "Iter 6 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 7.028308\n",
            "Iter 6 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.296978\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.1365082561969757:   4%|▍         | 8/200 [02:46<1:06:53, 20.90s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 7 - agent_0 alpha: 1.200000, grad mean: 5.268345\n",
            "Iter 7 - agent_0 lam: 1.500000, grad mean: 1.986237\n",
            "Iter 7 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 7.598986\n",
            "Iter 7 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.377423\n",
            "Iter 7 - agent_1 alpha: 1.200000, grad mean: 5.036709\n",
            "Iter 7 - agent_1 lam: 1.200000, grad mean: 1.986237\n",
            "Iter 7 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 7.598986\n",
            "Iter 7 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.301938\n",
            "Iter 7 - agent_2 alpha: 1.200000, grad mean: 5.036709\n",
            "Iter 7 - agent_2 lam: 1.200000, grad mean: 1.986237\n",
            "Iter 7 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 7.598986\n",
            "Iter 7 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.301938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.14169543981552124:   4%|▍         | 9/200 [03:07<1:06:14, 20.81s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 8 - agent_0 alpha: 1.200000, grad mean: 5.601916\n",
            "Iter 8 - agent_0 lam: 1.500000, grad mean: 2.020250\n",
            "Iter 8 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 8.171803\n",
            "Iter 8 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.383192\n",
            "Iter 8 - agent_1 alpha: 1.200000, grad mean: 5.365370\n",
            "Iter 8 - agent_1 lam: 1.200000, grad mean: 2.020250\n",
            "Iter 8 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 8.171803\n",
            "Iter 8 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.306553\n",
            "Iter 8 - agent_2 alpha: 1.200000, grad mean: 5.365370\n",
            "Iter 8 - agent_2 lam: 1.200000, grad mean: 2.020250\n",
            "Iter 8 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 8.171803\n",
            "Iter 8 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.306553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.140406534075737:   5%|▌         | 10/200 [03:28<1:05:50, 20.79s/it] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 9 - agent_0 alpha: 1.200000, grad mean: 5.935163\n",
            "Iter 9 - agent_0 lam: 1.500000, grad mean: 2.050816\n",
            "Iter 9 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 8.747347\n",
            "Iter 9 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.387420\n",
            "Iter 9 - agent_1 alpha: 1.200000, grad mean: 5.693736\n",
            "Iter 9 - agent_1 lam: 1.200000, grad mean: 2.050816\n",
            "Iter 9 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 8.747347\n",
            "Iter 9 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.309936\n",
            "Iter 9 - agent_2 alpha: 1.200000, grad mean: 5.693736\n",
            "Iter 9 - agent_2 lam: 1.200000, grad mean: 2.050816\n",
            "Iter 9 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 8.747347\n",
            "Iter 9 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.309936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13855914771556854:   6%|▌         | 11/200 [03:48<1:05:28, 20.78s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 10 - agent_0 alpha: 1.200000, grad mean: 6.267298\n",
            "Iter 10 - agent_0 lam: 1.500000, grad mean: 2.082183\n",
            "Iter 10 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 9.322682\n",
            "Iter 10 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.392466\n",
            "Iter 10 - agent_1 alpha: 1.200000, grad mean: 6.021203\n",
            "Iter 10 - agent_1 lam: 1.200000, grad mean: 2.082183\n",
            "Iter 10 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 9.322682\n",
            "Iter 10 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.313972\n",
            "Iter 10 - agent_2 alpha: 1.200000, grad mean: 6.021203\n",
            "Iter 10 - agent_2 lam: 1.200000, grad mean: 2.082183\n",
            "Iter 10 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 9.322682\n",
            "Iter 10 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.313972\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.14111290872097015:   6%|▌         | 12/200 [04:09<1:04:58, 20.74s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 11 - agent_0 alpha: 1.200000, grad mean: 6.597880\n",
            "Iter 11 - agent_0 lam: 1.500000, grad mean: 2.108637\n",
            "Iter 11 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 9.910788\n",
            "Iter 11 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.396330\n",
            "Iter 11 - agent_1 alpha: 1.200000, grad mean: 6.347651\n",
            "Iter 11 - agent_1 lam: 1.200000, grad mean: 2.108637\n",
            "Iter 11 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 9.910788\n",
            "Iter 11 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.317064\n",
            "Iter 11 - agent_2 alpha: 1.200000, grad mean: 6.347651\n",
            "Iter 11 - agent_2 lam: 1.200000, grad mean: 2.108637\n",
            "Iter 11 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 9.910788\n",
            "Iter 11 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.317064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13459251821041107:   6%|▋         | 13/200 [04:30<1:04:59, 20.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 12 - agent_0 alpha: 1.200000, grad mean: 6.932056\n",
            "Iter 12 - agent_0 lam: 1.500000, grad mean: 2.140040\n",
            "Iter 12 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 10.495029\n",
            "Iter 12 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.401710\n",
            "Iter 12 - agent_1 alpha: 1.200000, grad mean: 6.677318\n",
            "Iter 12 - agent_1 lam: 1.200000, grad mean: 2.140040\n",
            "Iter 12 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 10.495029\n",
            "Iter 12 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.321368\n",
            "Iter 12 - agent_2 alpha: 1.200000, grad mean: 6.677318\n",
            "Iter 12 - agent_2 lam: 1.200000, grad mean: 2.140040\n",
            "Iter 12 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 10.495029\n",
            "Iter 12 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.321368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13805460929870605:   7%|▋         | 14/200 [04:51<1:04:34, 20.83s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 13 - agent_0 alpha: 1.200000, grad mean: 7.268262\n",
            "Iter 13 - agent_0 lam: 1.500000, grad mean: 2.166467\n",
            "Iter 13 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 11.086571\n",
            "Iter 13 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.406237\n",
            "Iter 13 - agent_1 alpha: 1.200000, grad mean: 7.009601\n",
            "Iter 13 - agent_1 lam: 1.200000, grad mean: 2.166467\n",
            "Iter 13 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 11.086571\n",
            "Iter 13 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.324989\n",
            "Iter 13 - agent_2 alpha: 1.200000, grad mean: 7.009601\n",
            "Iter 13 - agent_2 lam: 1.200000, grad mean: 2.166467\n",
            "Iter 13 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 11.086571\n",
            "Iter 13 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.324989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.1386231631040573:   8%|▊         | 15/200 [05:12<1:04:08, 20.80s/it] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 14 - agent_0 alpha: 1.200000, grad mean: 7.603292\n",
            "Iter 14 - agent_0 lam: 1.500000, grad mean: 2.193408\n",
            "Iter 14 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 11.675467\n",
            "Iter 14 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.410027\n",
            "Iter 14 - agent_1 alpha: 1.200000, grad mean: 7.340319\n",
            "Iter 14 - agent_1 lam: 1.200000, grad mean: 2.193408\n",
            "Iter 14 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 11.675467\n",
            "Iter 14 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.328021\n",
            "Iter 14 - agent_2 alpha: 1.200000, grad mean: 7.340319\n",
            "Iter 14 - agent_2 lam: 1.200000, grad mean: 2.193408\n",
            "Iter 14 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 11.675467\n",
            "Iter 14 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.328021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.14030569791793823:   8%|▊         | 16/200 [05:32<1:03:43, 20.78s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 15 - agent_0 alpha: 1.200000, grad mean: 7.935717\n",
            "Iter 15 - agent_0 lam: 1.500000, grad mean: 2.219351\n",
            "Iter 15 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 12.243419\n",
            "Iter 15 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.413633\n",
            "Iter 15 - agent_1 alpha: 1.200000, grad mean: 7.668577\n",
            "Iter 15 - agent_1 lam: 1.200000, grad mean: 2.219351\n",
            "Iter 15 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 12.243419\n",
            "Iter 15 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.330906\n",
            "Iter 15 - agent_2 alpha: 1.200000, grad mean: 7.668577\n",
            "Iter 15 - agent_2 lam: 1.200000, grad mean: 2.219351\n",
            "Iter 15 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 12.243419\n",
            "Iter 15 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.330906\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.1353548765182495:   8%|▊         | 17/200 [05:53<1:03:27, 20.81s/it] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 16 - agent_0 alpha: 1.200000, grad mean: 8.268738\n",
            "Iter 16 - agent_0 lam: 1.500000, grad mean: 2.245110\n",
            "Iter 16 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 12.815495\n",
            "Iter 16 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.417577\n",
            "Iter 16 - agent_1 alpha: 1.200000, grad mean: 7.997627\n",
            "Iter 16 - agent_1 lam: 1.200000, grad mean: 2.245110\n",
            "Iter 16 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 12.815495\n",
            "Iter 16 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.334061\n",
            "Iter 16 - agent_2 alpha: 1.200000, grad mean: 7.997627\n",
            "Iter 16 - agent_2 lam: 1.200000, grad mean: 2.245110\n",
            "Iter 16 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 12.815495\n",
            "Iter 16 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.334061\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.1383940577507019:   9%|▉         | 18/200 [06:14<1:02:54, 20.74s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 17 - agent_0 alpha: 1.200000, grad mean: 8.599236\n",
            "Iter 17 - agent_0 lam: 1.500000, grad mean: 2.266936\n",
            "Iter 17 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 13.388602\n",
            "Iter 17 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.420327\n",
            "Iter 17 - agent_1 alpha: 1.200000, grad mean: 8.324405\n",
            "Iter 17 - agent_1 lam: 1.200000, grad mean: 2.266936\n",
            "Iter 17 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 13.388602\n",
            "Iter 17 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.336261\n",
            "Iter 17 - agent_2 alpha: 1.200000, grad mean: 8.324405\n",
            "Iter 17 - agent_2 lam: 1.200000, grad mean: 2.266936\n",
            "Iter 17 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 13.388602\n",
            "Iter 17 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.336261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13772131502628326:  10%|▉         | 19/200 [06:35<1:02:29, 20.71s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 18 - agent_0 alpha: 1.200000, grad mean: 8.933179\n",
            "Iter 18 - agent_0 lam: 1.500000, grad mean: 2.290487\n",
            "Iter 18 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 13.966878\n",
            "Iter 18 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.423589\n",
            "Iter 18 - agent_1 alpha: 1.200000, grad mean: 8.654492\n",
            "Iter 18 - agent_1 lam: 1.200000, grad mean: 2.290487\n",
            "Iter 18 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 13.966878\n",
            "Iter 18 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.338870\n",
            "Iter 18 - agent_2 alpha: 1.200000, grad mean: 8.654492\n",
            "Iter 18 - agent_2 lam: 1.200000, grad mean: 2.290487\n",
            "Iter 18 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 13.966878\n",
            "Iter 18 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.338870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.1302420198917389:  10%|█         | 20/200 [06:55<1:02:04, 20.69s/it] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 19 - agent_0 alpha: 1.200000, grad mean: 9.262519\n",
            "Iter 19 - agent_0 lam: 1.500000, grad mean: 2.314017\n",
            "Iter 19 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 14.545436\n",
            "Iter 19 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.427063\n",
            "Iter 19 - agent_1 alpha: 1.200000, grad mean: 8.980071\n",
            "Iter 19 - agent_1 lam: 1.200000, grad mean: 2.314017\n",
            "Iter 19 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 14.545436\n",
            "Iter 19 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.341649\n",
            "Iter 19 - agent_2 alpha: 1.200000, grad mean: 8.980071\n",
            "Iter 19 - agent_2 lam: 1.200000, grad mean: 2.314017\n",
            "Iter 19 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 14.545436\n",
            "Iter 19 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.341649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13681809604167938:  10%|█         | 21/200 [07:16<1:01:53, 20.74s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 20 - agent_0 alpha: 1.200000, grad mean: 9.592777\n",
            "Iter 20 - agent_0 lam: 1.500000, grad mean: 2.335630\n",
            "Iter 20 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 15.123823\n",
            "Iter 20 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.430175\n",
            "Iter 20 - agent_1 alpha: 1.200000, grad mean: 9.306856\n",
            "Iter 20 - agent_1 lam: 1.200000, grad mean: 2.335630\n",
            "Iter 20 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 15.123823\n",
            "Iter 20 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.344140\n",
            "Iter 20 - agent_2 alpha: 1.200000, grad mean: 9.306856\n",
            "Iter 20 - agent_2 lam: 1.200000, grad mean: 2.335630\n",
            "Iter 20 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 15.123823\n",
            "Iter 20 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.344140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13795658946037292:  11%|█         | 22/200 [07:37<1:01:30, 20.74s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 21 - agent_0 alpha: 1.200000, grad mean: 9.920970\n",
            "Iter 21 - agent_0 lam: 1.500000, grad mean: 2.354654\n",
            "Iter 21 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 15.690796\n",
            "Iter 21 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.432622\n",
            "Iter 21 - agent_1 alpha: 1.200000, grad mean: 9.631747\n",
            "Iter 21 - agent_1 lam: 1.200000, grad mean: 2.354654\n",
            "Iter 21 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 15.690796\n",
            "Iter 21 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.346097\n",
            "Iter 21 - agent_2 alpha: 1.200000, grad mean: 9.631747\n",
            "Iter 21 - agent_2 lam: 1.200000, grad mean: 2.354654\n",
            "Iter 21 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 15.690796\n",
            "Iter 21 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.346097\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13500437140464783:  12%|█▏        | 23/200 [07:58<1:01:18, 20.78s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 22 - agent_0 alpha: 1.200000, grad mean: 10.252801\n",
            "Iter 22 - agent_0 lam: 1.500000, grad mean: 2.375675\n",
            "Iter 22 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 16.266592\n",
            "Iter 22 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.435073\n",
            "Iter 22 - agent_1 alpha: 1.200000, grad mean: 9.959868\n",
            "Iter 22 - agent_1 lam: 1.200000, grad mean: 2.375675\n",
            "Iter 22 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 16.266592\n",
            "Iter 22 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.348057\n",
            "Iter 22 - agent_2 alpha: 1.200000, grad mean: 9.959868\n",
            "Iter 22 - agent_2 lam: 1.200000, grad mean: 2.375675\n",
            "Iter 22 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 16.266592\n",
            "Iter 22 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.348057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.14003776013851166:  12%|█▏        | 24/200 [08:18<1:00:50, 20.74s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 23 - agent_0 alpha: 1.200000, grad mean: 10.584785\n",
            "Iter 23 - agent_0 lam: 1.500000, grad mean: 2.394324\n",
            "Iter 23 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 16.840519\n",
            "Iter 23 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.437351\n",
            "Iter 23 - agent_1 alpha: 1.200000, grad mean: 10.288575\n",
            "Iter 23 - agent_1 lam: 1.200000, grad mean: 2.394324\n",
            "Iter 23 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 16.840519\n",
            "Iter 23 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.349880\n",
            "Iter 23 - agent_2 alpha: 1.200000, grad mean: 10.288575\n",
            "Iter 23 - agent_2 lam: 1.200000, grad mean: 2.394324\n",
            "Iter 23 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 16.840519\n",
            "Iter 23 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.349880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.1383814662694931:  12%|█▎        | 25/200 [08:39<1:00:32, 20.75s/it] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 24 - agent_0 alpha: 1.200000, grad mean: 10.916213\n",
            "Iter 24 - agent_0 lam: 1.500000, grad mean: 2.412929\n",
            "Iter 24 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 17.431030\n",
            "Iter 24 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.439931\n",
            "Iter 24 - agent_1 alpha: 1.200000, grad mean: 10.616799\n",
            "Iter 24 - agent_1 lam: 1.200000, grad mean: 2.412929\n",
            "Iter 24 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 17.431030\n",
            "Iter 24 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.351944\n",
            "Iter 24 - agent_2 alpha: 1.200000, grad mean: 10.616799\n",
            "Iter 24 - agent_2 lam: 1.200000, grad mean: 2.412929\n",
            "Iter 24 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 17.431030\n",
            "Iter 24 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.351944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.1394975334405899:  13%|█▎        | 26/200 [09:00<1:00:17, 20.79s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 25 - agent_0 alpha: 1.200000, grad mean: 11.244759\n",
            "Iter 25 - agent_0 lam: 1.500000, grad mean: 2.430208\n",
            "Iter 25 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 18.004625\n",
            "Iter 25 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.442020\n",
            "Iter 25 - agent_1 alpha: 1.200000, grad mean: 10.942194\n",
            "Iter 25 - agent_1 lam: 1.200000, grad mean: 2.430208\n",
            "Iter 25 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 18.004625\n",
            "Iter 25 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.353615\n",
            "Iter 25 - agent_2 alpha: 1.200000, grad mean: 10.942194\n",
            "Iter 25 - agent_2 lam: 1.200000, grad mean: 2.430208\n",
            "Iter 25 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 18.004625\n",
            "Iter 25 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.353615\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13898040354251862:  14%|█▎        | 27/200 [09:21<59:49, 20.75s/it] \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 26 - agent_0 alpha: 1.200000, grad mean: 11.574458\n",
            "Iter 26 - agent_0 lam: 1.500000, grad mean: 2.447019\n",
            "Iter 26 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 18.587656\n",
            "Iter 26 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.444171\n",
            "Iter 26 - agent_1 alpha: 1.200000, grad mean: 11.268889\n",
            "Iter 26 - agent_1 lam: 1.200000, grad mean: 2.447019\n",
            "Iter 26 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 18.587656\n",
            "Iter 26 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.355336\n",
            "Iter 26 - agent_2 alpha: 1.200000, grad mean: 11.268889\n",
            "Iter 26 - agent_2 lam: 1.200000, grad mean: 2.447019\n",
            "Iter 26 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 18.587656\n",
            "Iter 26 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.355336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13494780659675598:  14%|█▍        | 28/200 [09:41<59:25, 20.73s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 27 - agent_0 alpha: 1.200000, grad mean: 11.903017\n",
            "Iter 27 - agent_0 lam: 1.500000, grad mean: 2.462561\n",
            "Iter 27 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 19.164442\n",
            "Iter 27 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.445632\n",
            "Iter 27 - agent_1 alpha: 1.200000, grad mean: 11.594463\n",
            "Iter 27 - agent_1 lam: 1.200000, grad mean: 2.462561\n",
            "Iter 27 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 19.164442\n",
            "Iter 27 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.356505\n",
            "Iter 27 - agent_2 alpha: 1.200000, grad mean: 11.594463\n",
            "Iter 27 - agent_2 lam: 1.200000, grad mean: 2.462561\n",
            "Iter 27 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 19.164442\n",
            "Iter 27 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.356505\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13922740519046783:  14%|█▍        | 29/200 [10:02<59:01, 20.71s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 28 - agent_0 alpha: 1.200000, grad mean: 12.228605\n",
            "Iter 28 - agent_0 lam: 1.500000, grad mean: 2.477068\n",
            "Iter 28 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 19.742495\n",
            "Iter 28 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.447329\n",
            "Iter 28 - agent_1 alpha: 1.200000, grad mean: 11.917430\n",
            "Iter 28 - agent_1 lam: 1.200000, grad mean: 2.477068\n",
            "Iter 28 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 19.742495\n",
            "Iter 28 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.357862\n",
            "Iter 28 - agent_2 alpha: 1.200000, grad mean: 11.917430\n",
            "Iter 28 - agent_2 lam: 1.200000, grad mean: 2.477068\n",
            "Iter 28 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 19.742495\n",
            "Iter 28 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.357862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "episode_reward_mean_agent = 0.13774849474430084:  15%|█▌        | 30/200 [10:23<58:53, 20.78s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 29 - agent_0 alpha: 1.200000, grad mean: 12.555275\n",
            "Iter 29 - agent_0 lam: 1.500000, grad mean: 2.492884\n",
            "Iter 29 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 20.324966\n",
            "Iter 29 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 0.449215\n",
            "Iter 29 - agent_1 alpha: 1.200000, grad mean: 12.241251\n",
            "Iter 29 - agent_1 lam: 1.200000, grad mean: 2.492884\n",
            "Iter 29 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 20.324966\n",
            "Iter 29 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 0.359371\n",
            "Iter 29 - agent_2 alpha: 1.200000, grad mean: 12.241251\n",
            "Iter 29 - agent_2 lam: 1.200000, grad mean: 2.492884\n",
            "Iter 29 - agent_2 w_plus_prime_gamma: 0.500000, grad mean: 20.324966\n",
            "Iter 29 - agent_2 w_minus_prime_gamma: 0.690000, grad mean: 0.359371\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'agents'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-f5e08cd35a50>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfreeze_adaptive_until\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0madaptive_update_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Sample a batch from the cooperative group \"agents\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msubdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agents\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Compute the standard target Q-value:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'agents'"
          ]
        }
      ],
      "source": [
        "pbar = tqdm(\n",
        "    total=n_iters,\n",
        "    desc=\", \".join(\n",
        "        [f\"episode_reward_mean_{group} = 0\" for group in env.group_map.keys()]\n",
        "    ),\n",
        ")\n",
        "episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
        "train_group_map = copy.deepcopy(env.group_map)\n",
        "\n",
        "\n",
        "adaptive_update_frequency = 10  # update adaptive parameters every 10 iterations\n",
        "freeze_adaptive_until = 20  # don't update adaptive parameters until after 20 iterations\n",
        "scale_factor = 1e-3  # scaling for adaptive loss\n",
        "reg_lambda = 1e-3  # regularization coefficient\n",
        "\n",
        "# (Assume initial_params is defined right after adaptive_params creation)\n",
        "# For example:\n",
        "initial_params = {\n",
        "    agent_id: {name: param.clone().detach() for name, param in module.get_params().items()}\n",
        "    for agent_id, module in adaptive_params.items()\n",
        "}\n",
        "\n",
        "previous_base_loss = None\n",
        "\n",
        "for iteration, batch in enumerate(collector):\n",
        "    current_frames = batch.numel()\n",
        "    batch = process_batch(batch)\n",
        "\n",
        "    for group in train_group_map.keys():\n",
        "        group_batch = batch.exclude(\n",
        "            *[\n",
        "                key\n",
        "                for _group in env.group_map.keys()\n",
        "                if _group != group\n",
        "                for key in [_group, (\"next\", _group)]\n",
        "            ]\n",
        "        )\n",
        "        group_batch = group_batch.reshape(-1)\n",
        "        replay_buffers[group].extend(group_batch)\n",
        "\n",
        "        for _ in range(n_optimiser_steps):\n",
        "            subdata = replay_buffers[group].sample()\n",
        "            loss_vals = losses[group](subdata)\n",
        "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
        "                loss_value = loss_vals[loss_name]\n",
        "                optimiser = optimisers[group][loss_name]\n",
        "                loss_value.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(optimiser.param_groups[0][\"params\"], max_grad_norm)\n",
        "                optimiser.step()\n",
        "                optimiser.zero_grad()\n",
        "            target_updaters[group].step()\n",
        "            exploration_policies[group][-1].step(current_frames)\n",
        "\n",
        "    # Decoupled update for adaptive parameters every adaptive_update_frequency iterations,\n",
        "    # but only after freeze_adaptive_until iterations.\n",
        "    if iteration > freeze_adaptive_until and iteration % adaptive_update_frequency == 0:\n",
        "        # Sample a batch from the cooperative group \"agents\"\n",
        "        subdata = replay_buffers[\"agents\"].sample()\n",
        "\n",
        "        # Compute the standard target Q-value:\n",
        "        target_value = losses[\"agents\"]._value_estimator.value_estimate(\n",
        "            subdata, target_params=losses[\"agents\"]._cached_target_params\n",
        "        ).squeeze(-1)\n",
        "\n",
        "        # Compute the CPT-transformed target:\n",
        "        target_value_CPT = C_transform_cross_dynamic(target_value, adaptive_params)\n",
        "\n",
        "        # Compute the base adaptive loss as mean squared error:\n",
        "        base_loss = torch.mean((target_value_CPT - target_value) ** 2)\n",
        "\n",
        "        # Compute a dynamic scaling factor based on the change in base loss\n",
        "        if previous_base_loss is None:\n",
        "            dynamic_factor = 1.0\n",
        "        else:\n",
        "            # Increase the factor if the loss has changed substantially\n",
        "            dynamic_factor = 1.0 + torch.abs(base_loss - previous_base_loss)\n",
        "            # If you prefer to have a plain Python float, you can call .item() here\n",
        "            # dynamic_factor = 1.0 + torch.abs(base_loss - previous_base_loss).item()\n",
        "\n",
        "        # Update the previous_base_loss for the next adaptive update (detach it to avoid gradient tracking)\n",
        "        previous_base_loss = base_loss.detach()\n",
        "\n",
        "        # Compute the L2 regularization loss for keeping adaptive parameters near their initial values:\n",
        "        reg_loss = 0.0\n",
        "        for agent_id, module in adaptive_params.items():\n",
        "            params = module.get_params()\n",
        "            for name, param in params.items():\n",
        "                reg_loss += torch.mean((param - initial_params[agent_id][name]) ** 2)\n",
        "\n",
        "        # Combine the base adaptive loss (scaled dynamically) with the regularization term\n",
        "        adaptive_loss = dynamic_factor * scale_factor * base_loss + reg_lambda * reg_loss\n",
        "\n",
        "        # Print out the loss and the dynamic factor (dynamic_factor is a float or a tensor—if it's a float, no .item() is needed)\n",
        "        print(f\"Iteration {iteration}: Adaptive loss = {adaptive_loss.item()}, dynamic factor = {dynamic_factor}\")\n",
        "\n",
        "        # Backward pass and update adaptive parameters:\n",
        "        adaptive_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(adaptive_params.parameters(), max_norm=1.0)\n",
        "        optimizer_behavioral.step()\n",
        "        optimizer_behavioral.zero_grad()\n",
        "\n",
        "    # Optionally, monitor adaptive parameters here\n",
        "    for agent_id, module in adaptive_params.items():\n",
        "        params = module.get_params()\n",
        "        for name, param in params.items():\n",
        "            if param.grad is not None:\n",
        "                print(f\"Iter {iteration} - {agent_id} {name}: {param.item():.6f}, grad mean: {param.grad.abs().mean().item():.6f}\")\n",
        "\n",
        "    # Logging of episode rewards, etc.\n",
        "    for group in env.group_map.keys():\n",
        "        episode_reward_mean = (\n",
        "            batch.get((\"next\", group, \"episode_reward\"))[\n",
        "                batch.get((\"next\", group, \"done\"))\n",
        "            ]\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        episode_reward_mean_map[group].append(episode_reward_mean)\n",
        "\n",
        "    pbar.set_description(\n",
        "        \", \".join(\n",
        "            [\n",
        "                f\"episode_reward_mean_{group} = {episode_reward_mean_map[group][-1]}\"\n",
        "                for group in env.group_map.keys()\n",
        "            ]\n",
        "        ),\n",
        "        refresh=False,\n",
        "    )\n",
        "    pbar.update()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdxfvvGb2uRb"
      },
      "source": [
        "This is our \"test\" to make sure our agents are trainng, we see after the agent stops training the adversaries rewards are increasing and then while it is trainng their rewards both go to 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_voZiw3WnTw"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 1)\n",
        "for i, group in enumerate(env.group_map.keys()):\n",
        "    axs.plot(episode_reward_mean_map[group], label=f\"Episode reward mean {group}\")\n",
        "    axs.set_ylabel(\"Reward\")\n",
        "    axs.legend()\n",
        "axs.set_xlabel(\"Training iterations\")\n",
        "plt.show()\n",
        "print(env.group_map.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBIAaI_SXoSd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "f8bcdaf0-5a79-4b01-d63c-cfa19f66e46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating rendering env\n",
            "Rendering rollout...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Error occurred while running `from pyglet.gl import *`, HINT: make sure you have OpenGL installed. On Ubuntu, you can run 'apt-get install python3-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work:'xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vmas/simulator/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     from pyglet.gl import (\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mGL_BLEND\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/gl/gl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mctypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlink_GL\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_link_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mc_ptrdiff_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib_glx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlink_GL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_GLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_GLX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/gl/lib_glx.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mgl_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mglu_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/lib.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, *names, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Library \"%s\" not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Library \"GLU\" not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-a1fda816bdad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_exploration_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExplorationType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDETERMINISTIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rendering rollout...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0menv_with_render\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magents_exploration_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving the video...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0menv_with_render\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/common.py\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, break_when_all_done, return_contiguous, tensordict, set_truncated, out, trust_policy)\u001b[0m\n\u001b[1;32m   2598\u001b[0m             policy = _make_compatible_policy(\n\u001b[1;32m   2599\u001b[0m                 \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2601\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m                 \u001b[0mfast_wrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/common.py\u001b[0m in \u001b[0;36mobservation_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \"\"\"\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0mobservation_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"full_observation_spec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobservation_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mobservation_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComposite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/transforms/transforms.py\u001b[0m in \u001b[0;36moutput_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0moutput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlock_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0moutput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_output_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0moutput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/transforms/transforms.py\u001b[0m in \u001b[0;36mtransform_output_spec\u001b[0;34m(self, output_spec)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform_output_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensorSpec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0moutput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_output_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/transforms/transforms.py\u001b[0m in \u001b[0;36mtransform_output_spec\u001b[0;34m(self, output_spec)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0moutput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         output_spec[\"full_observation_spec\"] = self.transform_observation_spec(\n\u001b[0m\u001b[1;32m    382\u001b[0m             \u001b[0moutput_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"full_observation_spec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/record/recorder.py\u001b[0m in \u001b[0;36mtransform_observation_spec\u001b[0;34m(self, observation_spec)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mtd_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNonTensorData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/record/recorder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_tensordict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vmas/simulator/environment/environment.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, env_index, agent_index_focus, visualize_when_rgb, plot_position_function, plot_position_function_precision, plot_position_function_range, plot_position_function_cmap_range, plot_position_function_cmap_alpha, plot_position_function_cmap_name)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headless\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheadless\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_rendering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer_zoom\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vmas/simulator/environment/environment.py\u001b[0m in \u001b[0;36m_init_rendering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_rendering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mvmas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         self.viewer = rendering.Viewer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vmas/simulator/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;34m\"Error occurred while running `from pyglet.gl import *`, HINT: make sure you have OpenGL installed. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m\"On Ubuntu, you can run 'apt-get install python3-opengl'. If you're running on a server, you may need a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Error occurred while running `from pyglet.gl import *`, HINT: make sure you have OpenGL installed. On Ubuntu, you can run 'apt-get install python3-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work:'xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "if use_vmas and not is_sphinx:\n",
        "    # Replace tmpdir with any desired path where the video should be saved\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        video_logger = CSVLogger(\"vmas_logs\", tmpdir, video_format=\"mp4\")\n",
        "        print(\"Creating rendering env\")\n",
        "        env_with_render = TransformedEnv(env.base_env, env.transform.clone())\n",
        "        env_with_render = env_with_render.append_transform(\n",
        "            PixelRenderTransform(\n",
        "                out_keys=[\"pixels\"],\n",
        "                # the np.ndarray has a negative stride and needs to be copied before being cast to a tensor\n",
        "                preproc=lambda x: x.copy(),\n",
        "                as_non_tensor=True,\n",
        "                # asking for array rather than on-screen rendering\n",
        "                mode=\"rgb_array\",\n",
        "            )\n",
        "        )\n",
        "        env_with_render = env_with_render.append_transform(\n",
        "            VideoRecorder(logger=video_logger, tag=\"vmas_rendered\")\n",
        "        )\n",
        "        with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
        "            print(\"Rendering rollout...\")\n",
        "            env_with_render.rollout(100, policy=agents_exploration_policy)\n",
        "        print(\"Saving the video...\")\n",
        "        env_with_render.transform.dump()\n",
        "        print(\"Saved! Saved directory tree:\")\n",
        "        video_logger.print_log_dir()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define a permanent directory path (e.g., \"local_videos\")\n",
        "local_dir = \"local_videos\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "# Use the permanent directory instead of a temporary one\n",
        "video_logger = CSVLogger(\"vmas_logs\", local_dir, video_format=\"mp4\")\n",
        "print(\"Creating rendering env\")\n",
        "env_with_render = TransformedEnv(env.base_env, env.transform.clone())\n",
        "env_with_render = env_with_render.append_transform(\n",
        "    PixelRenderTransform(\n",
        "        out_keys=[\"pixels\"],\n",
        "        # the np.ndarray has a negative stride and needs to be copied before being cast to a tensor\n",
        "        preproc=lambda x: x.copy(),\n",
        "        as_non_tensor=True,\n",
        "        # asking for array rather than on-screen rendering\n",
        "        mode=\"rgb_array\",\n",
        "    )\n",
        ")\n",
        "env_with_render = env_with_render.append_transform(\n",
        "    VideoRecorder(logger=video_logger, tag=\"vmas_rendered\")\n",
        ")\n",
        "with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
        "    print(\"Rendering rollout...\")\n",
        "    env_with_render.rollout(100, policy=agents_exploration_policy)\n",
        "print(\"Saving the video...\")\n",
        "env_with_render.transform.dump()\n",
        "print(\"Saved! Saved directory tree:\")\n",
        "video_logger.print_log_dir()"
      ],
      "metadata": {
        "id": "ebVlbORDdSYM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}