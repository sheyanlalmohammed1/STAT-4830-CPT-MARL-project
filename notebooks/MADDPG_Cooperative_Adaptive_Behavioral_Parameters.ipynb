{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MADDPG with Dynamically Adaptive CPT-Adjusted Rewards in Cooperative Multi-Agent Systems\n",
        "\n",
        "## 1. Overview\n",
        "\n",
        "This Python script implements and tests a Multi-Agent Deep Deterministic Policy Gradient (MADDPG) algorithm within a cooperative multi-agent environment. Building upon the integration of Cumulative Prospect Theory (CPT), this version introduces a key innovation: **dynamically adaptive behavioral parameters**. Instead of relying solely on fixed, predefined CPT parameters for each agent, this script allows agents to *learn and adapt* their individual behavioral profiles (utility curvature, loss aversion, and probability weighting sensitivities) during the training process. This aims to create more flexible and potentially more realistic models of agent decision-making under risk and uncertainty, where agents can fine-tune their CPT characteristics based on their experiences within the environment and interactions with other agents.\n",
        "\n",
        "## 2. Core Algorithm: MADDPG\n",
        "\n",
        "The foundational reinforcement learning framework remains **MADDPG**, an adaptation of the Deep Deterministic Policy Gradient (DDPG) algorithm tailored for multi-agent scenarios. Its core components include:\n",
        "\n",
        "* **Actor-Critic Architecture:** Each agent maintains its own actor (policy) and critic (value function) networks.\n",
        "* **Centralized Critic, Decentralized Execution:** Critics utilize global information for training stability, while actors operate based on local observations for execution.\n",
        "* **Deterministic Policies with Exploration:** Actors learn deterministic policies, with exploration typically introduced via action noise.\n",
        "* **Experience Replay:** Past experiences are stored and sampled for training to improve data efficiency and stability.\n",
        "* **Soft Target Updates:** Target networks are updated slowly to stabilize the learning process.\n",
        "\n",
        "## 3. Key Innovation: Adaptive Cumulative Prospect Theory (CPT) Integration\n",
        "\n",
        "The central novelty of this script is the integration of CPT with **learnable, agent-specific behavioral parameters** and **dynamic probability weighting functions**.\n",
        "\n",
        "* **`AdaptiveBehavioralParameters(nn.Module)`:**\n",
        "    * This custom `torch.nn.Module` encapsulates the core CPT parameters for an agent:\n",
        "        * `alpha`: Curvature of the utility function.\n",
        "        * `lam`: Loss aversion coefficient.\n",
        "        * `w_plus_prime_gamma`: A learnable parameter (e.g., $\\gamma_+$) influencing the shape of the probability weighting function for gains.\n",
        "        * `w_minus_prime_gamma`: A learnable parameter (e.g., $\\gamma_-$) influencing the shape of the probability weighting function for losses.\n",
        "    * These parameters are defined as `nn.Parameter`, making them trainable via gradient descent.\n",
        "* **`adaptive_params = nn.ModuleDict`:**\n",
        "    * This dictionary holds an instance of `AdaptiveBehavioralParameters` for each agent (e.g., \"agent_0\", \"agent_1\"). This allows different agents to learn distinct behavioral profiles.\n",
        "* **Dynamic Probability Weighting Functions (`w_plus_prime_dynamic`, `w_minus_prime_dynamic`):**\n",
        "    * These functions replace the fixed `w_plus_prime_const` and `w_minus_prime_const` from previous versions.\n",
        "    * They compute probability weights based on:\n",
        "        1.  An input probability `p` (in the script, `p_star` is derived by applying a sigmoid function to rewards/values, normalizing them to a (0,1) range).\n",
        "        2.  The learnable `gamma` parameters (`w_plus_prime_gamma`, `w_minus_prime_gamma`) specific to each agent, retrieved from `adaptive_params`.\n",
        "    * The script uses a Prelec-like weighting function derivative: $w'(p) = \\exp(-(-\\log p)^\\gamma)$.\n",
        "* **Utility Functions (`stable_u_plus_agent`, `u_minus_agent`):**\n",
        "    * These functions calculate the utility of gains and losses, incorporating the learned `alpha` and `lam` for each agent. `stable_u_plus_agent` includes clamping and log-space computation for numerical stability.\n",
        "* **Dynamic CPT Transformations (`compute_phi_cross_dynamic`, `C_transform_cross_dynamic`):**\n",
        "    * `compute_phi_cross_dynamic`: Calculates the CPT sensitivity factor (`phi_factor`) that scales the actor's policy gradient. It uses the learned `alpha` and `lam` for utility and the learned `gamma`s via the dynamic weighting functions. The average sensitivity across agents is used.\n",
        "    * `C_transform_cross_dynamic`: Transforms the target Q-values for the critic's loss. It similarly applies the learned, agent-specific CPT parameters and dynamic weighting functions, averaging the transformed values.\n",
        "* **Custom DDPG Loss (`CPTDDPGLoss`):**\n",
        "    * **Value Loss:** The critic learns to predict Q-values that align with the `C_transform_cross_dynamic`-adjusted target values.\n",
        "    * **Actor Loss:** The actor's policy gradient is scaled by the `phi_factor` derived from `compute_phi_cross_dynamic`.\n",
        "* **Learning Adaptive Parameters:**\n",
        "    * **`optimizer_behavioral`:** A dedicated Adam optimizer is used to update the parameters within `adaptive_params`.\n",
        "    * **`compute_adaptive_loss`:** This function defines the objective for learning the behavioral parameters. The primary goal is to encourage the CPT-transformed target Q-value (`target_value_CPT`) to be close to the standard (non-CPT) target Q-value. The loss is typically the mean squared error between these two.\n",
        "    * **Regularization:** An L2 regularization term is added to the adaptive loss. This penalizes deviations of the learned parameters from their `initial_params`, encouraging them to stay within a reasonable range or to not stray too far from their initial behavioral assumptions.\n",
        "    * **Dynamic Scaling Factor:** The adaptive loss incorporates a `dynamic_factor` that scales the base adaptive loss. This factor is adjusted based on the change in the base loss from the previous update, potentially modulating the learning rate or importance of the adaptive component.\n",
        "\n",
        "### 3.1. Experimenting with Adaptive CPT Hyperparameters\n",
        "\n",
        "With adaptive parameters, the focus of experimentation shifts from manually setting fixed values to influencing the learning process and interpreting the learned profiles.\n",
        "\n",
        "1.  **Initial Values for Adaptive Parameters:**\n",
        "    * The `AdaptiveBehavioralParameters` class is initialized with `init_alpha`, `init_lam`, `init_w_plus_gamma`, and `init_w_minus_gamma`. These serve as the starting points for the learning process.\n",
        "        ```python\n",
        "        adaptive_params = nn.ModuleDict({\n",
        "            \"agent_0\": AdaptiveBehavioralParameters(init_alpha=1.2, init_lam=1.5, init_w_plus_gamma=0.5, init_w_minus_gamma=0.69),\n",
        "            \"agent_1\": AdaptiveBehavioralParameters(init_alpha=1.2, init_lam=1.2, init_w_plus_gamma=0.5, init_w_minus_gamma=0.69),\n",
        "        })\n",
        "        ```\n",
        "    * **Experiment:**\n",
        "        * Vary these initial values to see if the learning process converges to different behavioral profiles or if certain starting points lead to faster/more stable learning.\n",
        "        * Test different initial `gamma` values for the probability weighting functions. For Prelec, $\\gamma=1$ implies linear weighting (no distortion of probabilities). Values typically range between 0.5 and 1.\n",
        "\n",
        "2.  **Understanding Learned Parameters:**\n",
        "    * **`alpha` (Utility Curvature):**\n",
        "        * Learned `alpha = 1` suggests risk neutrality.\n",
        "        * Learned `0 < alpha < 1` suggests risk aversion for gains and risk-seeking for losses.\n",
        "        * Monitor how `alpha` evolves for each agent.\n",
        "    * **`lam` (Loss Aversion):**\n",
        "        * Learned `lam > 1` indicates loss aversion.\n",
        "        * Monitor its learned value to see the degree of loss aversion agents develop.\n",
        "    * **`w_plus_prime_gamma`, `w_minus_prime_gamma` (Probability Weighting Shape):**\n",
        "        * These learned `gamma` parameters determine the degree of optimism/pessimism in probability perception. For the Prelec function $w(p) = \\exp(-(-\\ln p)^\\gamma)$:\n",
        "            * $\\gamma = 1$: Linear weighting ($w(p) = p$).\n",
        "            * $\\gamma < 1$: Typically leads to overweighting of small probabilities and underweighting of large probabilities (inverse S-shape).\n",
        "            * $\\gamma > 1$: Can lead to underweighting of small probabilities and overweighting of large probabilities (S-shape).\n",
        "        * The script uses the *derivative* of a weighting function, so the interpretation of `gamma` is tied to how it shapes this sensitivity. Analyze the learned `gamma` values to understand how agents distort probabilities.\n",
        "\n",
        "3.  **Influencing the Learning Process:**\n",
        "    * **`optimizer_behavioral` Learning Rate:** The learning rate for `adaptive_params` (e.g., `1e-5` in the script) is crucial. A smaller rate leads to slower, potentially more stable adaptation.\n",
        "    * **`adaptive_update_frequency`:** How often the adaptive parameters are updated (e.g., every 10 iterations). More frequent updates mean faster adaptation but could be noisy.\n",
        "    * **`freeze_adaptive_until`:** Delays the start of adaptive parameter learning, allowing the base RL policies to stabilize first.\n",
        "    * **`scale_factor` for `base_adaptive_loss`:** (e.g., `1e-3`) This scales the main objective of matching CPT-transformed targets to standard targets. A smaller factor reduces the influence of this objective.\n",
        "    * **`reg_lambda` for Regularization:** (e.g., `1e-3`) Controls the strength of the L2 regularization penalty, which pulls learned parameters towards their initial values. Higher `reg_lambda` means less deviation from `initial_params`.\n",
        "    * **`dynamic_factor` in Adaptive Loss:** This factor, `1.0 + torch.abs(base_loss - previous_base_loss)`, amplifies the adaptive loss when the base adaptive loss changes significantly. This could accelerate learning when the system is in flux or stabilize it when changes are minor. Experiment with its formulation or remove it to see the impact.\n",
        "    * **Experiment:** Adjust these meta-parameters to control the speed, stability, and outcome of the behavioral parameter adaptation.\n",
        "\n",
        "4.  **Probability Input `p_star` for Weighting Functions:**\n",
        "    * The dynamic weighting functions (`w_plus_prime_dynamic`, `w_minus_prime_dynamic`) take a probability `p` as input. The script calculates this as `p_star = torch.sigmoid(R)` (or `torch.sigmoid(x)`), where `R` or `x` are rewards or Q-values.\n",
        "    * **Experiment:** The choice of normalization function (here, sigmoid) to map rewards/values to a [0,1] range can influence the behavior of the weighting functions. Consider other normalization methods if sigmoid doesn't yield desired properties.\n",
        "\n",
        "5.  **Choice of Dynamic Weighting Function:**\n",
        "    * The script uses `torch.exp(-(-torch.log(p)) ** gamma)`. This is related to the Prelec weighting function.\n",
        "    * **Experiment:** You could implement and test other forms of probability weighting functions (e.g., Tversky & Kahneman's 1992 weighting function) by modifying `w_plus_prime_dynamic` and `w_minus_prime_dynamic`.\n",
        "\n",
        "By observing the evolution of `adaptive_params` and correlating them with agent performance and behavior, you can gain insights into what behavioral profiles are effective or emerge in the given cooperative task.\n",
        "\n",
        "## 4. Environment\n",
        "\n",
        "The agents operate in a multi-agent environment, likely:\n",
        "\n",
        "* **`VmasEnv` (e.g., `simple_spread`) or `PettingZooEnv`:** Standard multi-agent RL environments. `simple_spread` typically involves cooperative tasks like covering landmarks.\n",
        "\n",
        "## 5. Implementation Details\n",
        "\n",
        "* **Libraries:** `torchRL` and `PyTorch` are the primary libraries.\n",
        "* **Training Loop:** Similar to standard MADDPG, but with an additional, decoupled update step for the `adaptive_params` using `optimizer_behavioral` and the `compute_adaptive_loss` function.\n",
        "* **Stability Measures:** `epsilon` and `min_val` are used in CPT calculations to prevent numerical issues like log(0) or division by zero.\n",
        "\n",
        "## 6. Goal of the Code\n",
        "\n",
        "The primary goal is to explore how agents in a cooperative multi-agent setting can **learn and adapt their own behavioral economic parameters (CPT profiles)**. This allows for an investigation into:\n",
        "\n",
        "* **Emergent Heterogeneity:** Do agents develop different CPT profiles even if they start from similar initializations?\n",
        "* **Adaptive Strategies:** How do learned CPT parameters correlate with task performance and cooperative strategies?\n",
        "* **Stability and Convergence:** How does the learning of behavioral parameters interact with the learning of policies and value functions?\n",
        "* **Realism:** Does allowing parameters to adapt lead to more nuanced or human-like decision-making patterns?\n",
        "\n",
        "This script provides a sophisticated framework for studying the interplay between reinforcement learning and behavioral economics in multi-agent systems.\n"
      ],
      "metadata": {
        "id": "-2Rcgv6EGbpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchrl==0.6.0\n",
        "!pip3 install vmas\n",
        "!pip3 install pettingzoo[mpe]==1.24.3\n",
        "!pip3 install tqdm"
      ],
      "metadata": {
        "id": "RPIbRmLC_fqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c51421-ca15-419a-d7b9-548694a07ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchrl==0.6.0 in /usr/local/lib/python3.11/dist-packages (0.6.0)\n",
            "Requirement already satisfied: torch>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from torchrl==0.6.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchrl==0.6.0) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from torchrl==0.6.0) (24.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from torchrl==0.6.0) (3.1.1)\n",
            "Requirement already satisfied: tensordict>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from torchrl==0.6.0) (0.7.2)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from tensordict>=0.6.0->torchrl==0.6.0) (3.10.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->torchrl==0.6.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.0->torchrl==0.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.0->torchrl==0.6.0) (3.0.2)\n",
            "Requirement already satisfied: vmas in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vmas) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from vmas) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyglet<=1.5.27 in /usr/local/lib/python3.11/dist-packages (from vmas) (1.5.27)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from vmas) (0.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from vmas) (1.17.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->vmas) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym->vmas) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->vmas) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->vmas) (3.0.2)\n",
            "Requirement already satisfied: pettingzoo==1.24.3 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[mpe]==1.24.3) (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (1.1.1)\n",
            "Requirement already satisfied: pygame==2.3.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[mpe]==1.24.3) (2.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (0.0.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yhec5pa7VzyM"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import tempfile\n",
        "\n",
        "import torch\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensordict import TensorDictBase\n",
        "\n",
        "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
        "from torch import multiprocessing\n",
        "\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
        "\n",
        "from torchrl.envs import (\n",
        "    check_env_specs,\n",
        "    ExplorationType,\n",
        "    PettingZooEnv,\n",
        "    RewardSum,\n",
        "    set_exploration_type,\n",
        "    TransformedEnv,\n",
        "    VmasEnv,\n",
        ")\n",
        "\n",
        "from torchrl.modules import (\n",
        "    AdditiveGaussianModule,\n",
        "    MultiAgentMLP,\n",
        "    ProbabilisticActor,\n",
        "    TanhDelta,\n",
        ")\n",
        "\n",
        "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
        "\n",
        "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    is_sphinx = __sphinx_build__\n",
        "except NameError:\n",
        "    is_sphinx = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import tempfile\n",
        "\n",
        "import torch\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tensordict import TensorDictBase, is_tensor_collection\n",
        "\n",
        "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
        "from torch import multiprocessing\n",
        "\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
        "\n",
        "from torchrl.envs import (\n",
        "    check_env_specs,\n",
        "    ExplorationType,\n",
        "    PettingZooEnv,\n",
        "    RewardSum,\n",
        "    set_exploration_type,\n",
        "    TransformedEnv,\n",
        "    VmasEnv,\n",
        ")\n",
        "\n",
        "from torchrl.modules import (\n",
        "    AdditiveGaussianModule,\n",
        "    MultiAgentMLP,\n",
        "    ProbabilisticActor,\n",
        "    TanhDelta,\n",
        ")\n",
        "\n",
        "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
        "\n",
        "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    is_sphinx = __sphinx_build__\n",
        "except NameError:\n",
        "    is_sphinx = False\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.compiler import is_compiling\n",
        "except ImportError:\n",
        "    from torch._dynamo import is_compiling"
      ],
      "metadata": {
        "id": "Se009O-PKMC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eDQqQ5YV3Sw"
      },
      "outputs": [],
      "source": [
        "# Seed\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Devices\n",
        "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
        "device = (\n",
        "    torch.device(0)\n",
        "    if torch.cuda.is_available() and not is_fork\n",
        "    else torch.device(\"cpu\")\n",
        ")\n",
        "\n",
        "# Sampling\n",
        "frames_per_batch = 1_000  # Number of team frames collected per sampling iteration\n",
        "n_iters = 200  # Number of sampling and training iterations\n",
        "total_frames = frames_per_batch * n_iters\n",
        "\n",
        "\n",
        "# Replay buffer\n",
        "memory_size = 1_000_000  # The replay buffer of each group can store this many frames\n",
        "\n",
        "# Training\n",
        "n_optimiser_steps = 100  # Number of optimization steps per training iteration\n",
        "train_batch_size = 128  # Number of frames trained in each optimiser step\n",
        "lr = 3e-4  # Learning rate\n",
        "max_grad_norm = 1.0  # Maximum norm for the gradients\n",
        "\n",
        "# DDPG\n",
        "gamma = 0.99  # Discount factor\n",
        "polyak_tau = 0.005  # Tau for the soft-update of the target network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dx0KswwV97f"
      },
      "outputs": [],
      "source": [
        "max_steps = 100  # Environment steps before done\n",
        "\n",
        "n_agents = 2\n",
        "n_landmarks = 1\n",
        "\n",
        "use_vmas = True  # Set this to True for a great performance speedup\n",
        "\n",
        "if not use_vmas:\n",
        "  base_env = PettingZooEnv(\n",
        "      task=\"simple_spread_v3\",\n",
        "      parallel=True,\n",
        "      seed=seed,\n",
        "      continuous_actions=True,\n",
        "      N = n_landmarks\n",
        "  )\n",
        "else:\n",
        "    num_vmas_envs = (\n",
        "        frames_per_batch // max_steps\n",
        "    )\n",
        "    base_env = VmasEnv(\n",
        "        scenario=\"simple_spread\",\n",
        "        num_envs=num_vmas_envs,\n",
        "        continuous_actions=True,\n",
        "        max_steps=max_steps,\n",
        "        local_ratio=0.5,\n",
        "        device=device,\n",
        "        seed=seed,\n",
        "        n_agents = n_agents\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9qc-A5yWGZJ",
        "outputId": "117c292a-b131-4a7c-eab9-4b5c523cbc80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "group_map: {'agents': ['agent_0', 'agent_1']}\n"
          ]
        }
      ],
      "source": [
        "print(f\"group_map: {base_env.group_map}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYBvfAjqWH3j",
        "outputId": "769f4859-583d-4def-efd9-69d7ce4df8d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action_spec: Composite(\n",
            "    agents: Composite(\n",
            "        action: BoundedContinuous(\n",
            "            shape=torch.Size([10, 2, 2]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([10, 2, 2]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([10, 2, 2]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([10, 2])),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([10]))\n",
            "reward_spec: Composite(\n",
            "    agents: Composite(\n",
            "        reward: UnboundedContinuous(\n",
            "            shape=torch.Size([10, 2, 1]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([10, 2, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([10, 2, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([10, 2])),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([10]))\n",
            "done_spec: Composite(\n",
            "    done: Categorical(\n",
            "        shape=torch.Size([10, 1]),\n",
            "        space=CategoricalBox(n=2),\n",
            "        device=cuda:0,\n",
            "        dtype=torch.bool,\n",
            "        domain=discrete),\n",
            "    terminated: Categorical(\n",
            "        shape=torch.Size([10, 1]),\n",
            "        space=CategoricalBox(n=2),\n",
            "        device=cuda:0,\n",
            "        dtype=torch.bool,\n",
            "        domain=discrete),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([10]))\n",
            "observation_spec: Composite(\n",
            "    agents: Composite(\n",
            "        observation: UnboundedContinuous(\n",
            "            shape=torch.Size([10, 2, 10]),\n",
            "            space=ContinuousBox(\n",
            "                low=Tensor(shape=torch.Size([10, 2, 10]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
            "                high=Tensor(shape=torch.Size([10, 2, 10]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
            "            device=cuda:0,\n",
            "            dtype=torch.float32,\n",
            "            domain=continuous),\n",
            "        device=cuda:0,\n",
            "        shape=torch.Size([10, 2])),\n",
            "    device=cuda:0,\n",
            "    shape=torch.Size([10]))\n"
          ]
        }
      ],
      "source": [
        "print(\"action_spec:\", base_env.full_action_spec)\n",
        "print(\"reward_spec:\", base_env.full_reward_spec)\n",
        "print(\"done_spec:\", base_env.full_done_spec)\n",
        "print(\"observation_spec:\", base_env.observation_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGyVd6LTWJgu",
        "outputId": "a167a97f-9db5-4f9f-d7a7-1b90dd827eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action_keys: [('agents', 'action')]\n",
            "reward_keys: [('agents', 'reward')]\n",
            "done_keys: ['done', 'terminated']\n"
          ]
        }
      ],
      "source": [
        "print(\"action_keys:\", base_env.action_keys)\n",
        "print(\"reward_keys:\", base_env.reward_keys)\n",
        "print(\"done_keys:\", base_env.done_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8e7iPn_WKvo"
      },
      "outputs": [],
      "source": [
        "env = TransformedEnv(\n",
        "    base_env,\n",
        "    RewardSum(\n",
        "        in_keys=base_env.reward_keys,\n",
        "        reset_keys=[\"_reset\"] * len(base_env.group_map.keys()),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3VPMV-UWLqf",
        "outputId": "dd43a598-f1c7-472c-96f8-bcf09d459560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-03-25 05:46:01,794 [torchrl][INFO] check_env_specs succeeded!\n"
          ]
        }
      ],
      "source": [
        "check_env_specs(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BKZ9yjkWNlJ",
        "outputId": "2a1a1a13-7cab-45c6-bca4-31aa9e5e643d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rollout of 5 steps: TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                action: Tensor(shape=torch.Size([10, 5, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                observation: Tensor(shape=torch.Size([10, 5, 2, 10]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "            batch_size=torch.Size([10, 5, 2]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([10, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                agents: TensorDict(\n",
            "                    fields={\n",
            "                        episode_reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        observation: Tensor(shape=torch.Size([10, 5, 2, 10]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                        reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "                    batch_size=torch.Size([10, 5, 2]),\n",
            "                    device=cuda:0,\n",
            "                    is_shared=True),\n",
            "                done: Tensor(shape=torch.Size([10, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "                terminated: Tensor(shape=torch.Size([10, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "            batch_size=torch.Size([10, 5]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([10, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([10, 5]),\n",
            "    device=cuda:0,\n",
            "    is_shared=True)\n",
            "Shape of the rollout TensorDict: torch.Size([10, 5])\n"
          ]
        }
      ],
      "source": [
        "n_rollout_steps = 5\n",
        "rollout = env.rollout(n_rollout_steps)\n",
        "print(f\"rollout of {n_rollout_steps} steps:\", rollout)\n",
        "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bmzos-xWPzE"
      },
      "outputs": [],
      "source": [
        "policy_modules = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    share_parameters_policy = False  # Can change this based on the group\n",
        "\n",
        "    policy_net = MultiAgentMLP(\n",
        "        n_agent_inputs=env.observation_spec[group, \"observation\"].shape[\n",
        "            -1\n",
        "        ],  # n_obs_per_agent\n",
        "        n_agent_outputs=env.full_action_spec[group, \"action\"].shape[\n",
        "            -1\n",
        "        ],  # n_actions_per_agents\n",
        "        n_agents=len(agents),  # Number of agents in the group\n",
        "        centralised=False,  # the policies are decentralised (i.e., each agent will act from its local observation)\n",
        "        share_params=share_parameters_policy,\n",
        "        device=device,\n",
        "        depth=2,\n",
        "        num_cells=256,\n",
        "        activation_class=torch.nn.Tanh,\n",
        "    )\n",
        "\n",
        "    # Wrap the neural network in a :class:`~tensordict.nn.TensorDictModule`.\n",
        "    # This is simply a module that will read the ``in_keys`` from a tensordict, feed them to the\n",
        "    # neural networks, and write the\n",
        "    # outputs in-place at the ``out_keys``.\n",
        "\n",
        "    policy_module = TensorDictModule(\n",
        "        policy_net,\n",
        "        in_keys=[(group, \"observation\")],\n",
        "        out_keys=[(group, \"param\")],\n",
        "    )  # We just name the input and output that the network will read and write to the input tensordict\n",
        "    policy_modules[group] = policy_module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbHpMLXRWROv"
      },
      "outputs": [],
      "source": [
        "policies = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    policy = ProbabilisticActor(\n",
        "        module=policy_modules[group],\n",
        "        spec=env.full_action_spec[group, \"action\"],\n",
        "        in_keys=[(group, \"param\")],\n",
        "        out_keys=[(group, \"action\")],\n",
        "        distribution_class=TanhDelta,\n",
        "        distribution_kwargs={\n",
        "            \"low\": env.full_action_spec[group, \"action\"].space.low,\n",
        "            \"high\": env.full_action_spec[group, \"action\"].space.high,\n",
        "        },\n",
        "        return_log_prob=False,\n",
        "    )\n",
        "    policies[group] = policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpDjZUtQWSeF"
      },
      "outputs": [],
      "source": [
        "exploration_policies = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    exploration_policy = TensorDictSequential(\n",
        "        policies[group],\n",
        "        AdditiveGaussianModule(\n",
        "            spec=policies[group].spec,\n",
        "            annealing_num_steps=total_frames\n",
        "            // 2,  # Number of frames after which sigma is sigma_end\n",
        "            action_key=(group, \"action\"),\n",
        "            sigma_init=0.9,  # Initial value of the sigma\n",
        "            sigma_end=0.1,  # Final value of the sigma\n",
        "        ),\n",
        "    )\n",
        "    exploration_policies[group] = exploration_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqVFCeRNWSfY"
      },
      "outputs": [],
      "source": [
        "critics = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    share_parameters_critic = True  # Can change for each group\n",
        "    MADDPG = True  # IDDPG if False, can change for each group\n",
        "\n",
        "    # This module applies the lambda function: reading the action and observation entries for the group\n",
        "    # and concatenating them in a new ``(group, \"obs_action\")`` entry\n",
        "    cat_module = TensorDictModule(\n",
        "        lambda obs, action: torch.cat([obs, action], dim=-1),\n",
        "        in_keys=[(group, \"observation\"), (group, \"action\")],\n",
        "        out_keys=[(group, \"obs_action\")],\n",
        "    )\n",
        "\n",
        "    critic_module = TensorDictModule(\n",
        "        module=MultiAgentMLP(\n",
        "            n_agent_inputs=env.observation_spec[group, \"observation\"].shape[-1]\n",
        "            + env.full_action_spec[group, \"action\"].shape[-1],\n",
        "            n_agent_outputs=1,  # 1 value per agent\n",
        "            n_agents=len(agents),\n",
        "            centralised=MADDPG,\n",
        "            share_params=share_parameters_critic,\n",
        "            device=device,\n",
        "            depth=2,\n",
        "            num_cells=256,\n",
        "            activation_class=torch.nn.Tanh,\n",
        "        ),\n",
        "        in_keys=[(group, \"obs_action\")],  # Read ``(group, \"obs_action\")``\n",
        "        out_keys=[\n",
        "            (group, \"state_action_value\")\n",
        "        ],  # Write ``(group, \"state_action_value\")``\n",
        "    )\n",
        "\n",
        "    critics[group] = TensorDictSequential(\n",
        "        cat_module, critic_module\n",
        "    )  # Run them in sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W7Qq-fBWZRq",
        "outputId": "cd53a91d-9ec4-49b2-e7ad-afc9313ab1f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running value and policy for group 'agents': TensorDict(\n",
            "    fields={\n",
            "        agents: TensorDict(\n",
            "            fields={\n",
            "                action: Tensor(shape=torch.Size([10, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                episode_reward: Tensor(shape=torch.Size([10, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                obs_action: Tensor(shape=torch.Size([10, 2, 12]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                observation: Tensor(shape=torch.Size([10, 2, 10]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                param: Tensor(shape=torch.Size([10, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
            "                state_action_value: Tensor(shape=torch.Size([10, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
            "            batch_size=torch.Size([10, 2]),\n",
            "            device=cuda:0,\n",
            "            is_shared=True),\n",
            "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
            "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
            "    batch_size=torch.Size([10]),\n",
            "    device=cuda:0,\n",
            "    is_shared=True)\n"
          ]
        }
      ],
      "source": [
        "reset_td = env.reset()\n",
        "for group, _agents in env.group_map.items():\n",
        "    print(\n",
        "        f\"Running value and policy for group '{group}':\",\n",
        "        critics[group](policies[group](reset_td)),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67asRrVjWa5O"
      },
      "outputs": [],
      "source": [
        "# Put exploration policies from each group in a sequence\n",
        "agents_exploration_policy = TensorDictSequential(*exploration_policies.values())\n",
        "\n",
        "collector = SyncDataCollector(\n",
        "    env,\n",
        "    agents_exploration_policy,\n",
        "    device=device,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NghMzg_3Wcky"
      },
      "outputs": [],
      "source": [
        "#Standard in off policy algos for efficient data collections\n",
        "replay_buffers = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    replay_buffer = ReplayBuffer(\n",
        "        storage=LazyMemmapStorage(memory_size, device=\"cpu\"),\n",
        "        sampler=RandomSampler(),\n",
        "        batch_size=train_batch_size,\n",
        "    )\n",
        "    replay_buffer.append_transform(lambda batch: batch.to(\"cuda:0\"))\n",
        "    replay_buffers[group] = replay_buffer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters for each agent in the cooperative group\n",
        "agent_params = {\n",
        "    \"agent_0\": {\n",
        "        \"alpha\": 0.7,\n",
        "        \"lam\": 0.8,\n",
        "        \"w_plus_prime_const\": 0.8,\n",
        "        \"w_minus_prime_const\": 0.2,\n",
        "    },\n",
        "    \"agent_1\": {\n",
        "        \"alpha\": 0.65,\n",
        "        \"lam\": 2.8,\n",
        "        \"w_plus_prime_const\": 0.25,\n",
        "        \"w_minus_prime_const\": 0.75,\n",
        "    },\n",
        "}\n"
      ],
      "metadata": {
        "id": "VyQcpsv2HGt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AdaptiveBehavioralParameters(nn.Module):\n",
        "    def __init__(self, init_alpha, init_lam, init_w_plus_gamma, init_w_minus_gamma):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.tensor(init_alpha, dtype=torch.float32))\n",
        "        self.lam = nn.Parameter(torch.tensor(init_lam, dtype=torch.float32))\n",
        "        # Instead of fixed w constants, now learn the gamma parameters for weighting functions:\n",
        "        self.w_plus_prime_gamma = nn.Parameter(torch.tensor(init_w_plus_gamma, dtype=torch.float32))\n",
        "        self.w_minus_prime_gamma = nn.Parameter(torch.tensor(init_w_minus_gamma, dtype=torch.float32))\n",
        "\n",
        "    def get_params(self):\n",
        "        return {\n",
        "            \"alpha\": self.alpha,\n",
        "            \"lam\": self.lam,\n",
        "            \"w_plus_prime_gamma\": self.w_plus_prime_gamma,\n",
        "            \"w_minus_prime_gamma\": self.w_minus_prime_gamma\n",
        "        }\n"
      ],
      "metadata": {
        "id": "GJHNzE-HMss_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adaptive_params = nn.ModuleDict({\n",
        "    \"agent_0\": AdaptiveBehavioralParameters(init_alpha=1.2, init_lam=1.5, init_w_plus_gamma=0.5, init_w_minus_gamma=0.69),\n",
        "    \"agent_1\": AdaptiveBehavioralParameters(init_alpha=1.2, init_lam=1.2, init_w_plus_gamma=0.5, init_w_minus_gamma=0.69),\n",
        "})"
      ],
      "metadata": {
        "id": "ZZLUJo-OMvmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_plus_prime_const = 0.2\n",
        "w_minus_prime_const = 0.8\n",
        "\n",
        "def w_plus_prime_dynamic(p, params, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Compute a dynamic weighting for gains.\n",
        "    p: a tensor of probabilities (values between 0 and 1)\n",
        "    params: dictionary containing a learnable parameter 'w_plus_prime_gamma'\n",
        "    \"\"\"\n",
        "    # Clamp p to avoid log(0)\n",
        "    p = torch.clamp(p, min=epsilon, max=1.0)\n",
        "    gamma = params.get(\"w_plus_prime_gamma\", torch.tensor(0.61, dtype=p.dtype, device=p.device))\n",
        "    # Prelec weighting derivative (an example formulation):\n",
        "    return torch.exp(-(-torch.log(p)) ** gamma)\n",
        "\n",
        "def w_minus_prime_dynamic(p, params, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Compute a dynamic weighting for losses.\n",
        "    p: a tensor of probabilities (values between 0 and 1)\n",
        "    params: dictionary containing a learnable parameter 'w_minus_prime_gamma'\n",
        "    \"\"\"\n",
        "    p = torch.clamp(p, min=epsilon, max=1.0)\n",
        "    gamma = params.get(\"w_minus_prime_gamma\", torch.tensor(0.69, dtype=p.dtype, device=p.device))\n",
        "    return torch.exp(-(-torch.log(p)) ** gamma)\n",
        "\n",
        "def compute_phi_linear(R):\n",
        "    \"\"\"\n",
        "    Compute linearized CPT sensitivity:\n",
        "    φ(R) ≈ w'_+(p*) * u^+(R) for R>=0, and -w'_-(p*) * u^-(R) for R<0.\n",
        "    \"\"\"\n",
        "    R = R.view(-1)\n",
        "    v = torch.where(R >= 0, u_plus(R), -u_minus(R))\n",
        "    phi = torch.where(R >= 0, w_plus_prime_const * v, -w_minus_prime_const * v)\n",
        "    return phi.mean()\n",
        "\n",
        "def compute_phi_linear_dynamic(R, adaptive_params, epsilon=1e-6):\n",
        "    \"\"\"\n",
        "    Compute linearized CPT sensitivity using dynamic weighting:\n",
        "      φ(R) ≈ w'_+(p*) * u^+(R)  for R >= 0,\n",
        "           ≈ -w'_-(p*) * u^-(R) for R < 0.\n",
        "    Here p* is obtained by normalizing R into (0,1) (using a sigmoid, for example).\n",
        "    adaptive_params is a ModuleDict where each module’s get_params() returns a dict that\n",
        "    includes learnable parameters for the weighting functions (e.g., w_plus_prime_gamma).\n",
        "    \"\"\"\n",
        "    R = R.view(-1)\n",
        "    # For example, normalize R to (0,1) via a sigmoid.\n",
        "    p_star = torch.sigmoid(R)\n",
        "\n",
        "    # Compute the basic utility:\n",
        "    # (Assuming you have functions u_plus(R) and u_minus(R) already defined.)\n",
        "    # If not, you can use your existing stable versions:\n",
        "    v = torch.where(R >= 0, u_plus(R), -u_minus(R))\n",
        "\n",
        "    phi_values = []\n",
        "    for agent_id, param_module in adaptive_params.items():\n",
        "        params = param_module.get_params()\n",
        "        # For gains, use dynamic weighting from our new function:\n",
        "        weight_gain = w_plus_prime_dynamic(p_star, params, epsilon)\n",
        "        # For losses:\n",
        "        weight_loss = w_minus_prime_dynamic(p_star, params, epsilon)\n",
        "        phi = torch.where(R >= 0, weight_gain * v, -weight_loss * v)\n",
        "        phi_values.append(phi)\n",
        "    phi_stack = torch.stack(phi_values)\n",
        "    return phi_stack.mean(dim=0)\n",
        "\n",
        "\n",
        "def stable_u_plus_agent(x, params, epsilon=1e-6, min_val=1e-3):\n",
        "    # Ensure that x + epsilon is not too small; then compute in log space.\n",
        "    y = torch.clamp(x + epsilon, min=min_val)\n",
        "    return torch.exp(params[\"alpha\"] * torch.log(y))\n",
        "\n",
        "def u_minus_agent(x, params, epsilon=1e-6, min_val=1e-3):\n",
        "    # For the negative branch, ensure -x + epsilon is not too small.\n",
        "    y = torch.clamp(-x + epsilon, min=min_val)\n",
        "    return params[\"lam\"] * torch.pow(y, params[\"alpha\"])\n",
        "\n",
        "def compute_phi_cross_dynamic(R, adaptive_params, epsilon=1e-6, min_val=1e-3):\n",
        "    \"\"\"\n",
        "    Compute the cross-agent CPT sensitivity factor dynamically.\n",
        "    For each reward in R, first normalize it to (0,1) via a sigmoid (p_star).\n",
        "    Then compute the utility using u_plus for gains and u_minus_agent for losses.\n",
        "    Finally, apply dynamic weighting using the learnable weighting functions and average across agents.\n",
        "\n",
        "    Args:\n",
        "        R (Tensor): A tensor of rewards.\n",
        "        adaptive_params (ModuleDict): A dictionary (ModuleDict) of adaptive parameter modules.\n",
        "        epsilon (float): A small constant to prevent log(0).\n",
        "        min_val (float): A minimum value to clamp inputs.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: The averaged sensitivity factor φ.\n",
        "    \"\"\"\n",
        "    R = R.view(-1)\n",
        "    # Normalize rewards to [0,1] for the weighting function:\n",
        "    p_star = torch.sigmoid(R)\n",
        "\n",
        "    phi_values = []\n",
        "    for agent_id, param_module in adaptive_params.items():\n",
        "        params = param_module.get_params()\n",
        "        # For gains: clamp R+epsilon, then compute stable u_plus:\n",
        "        y = torch.clamp(R + epsilon, min=min_val)\n",
        "        u_plus_val = torch.exp(params[\"alpha\"] * torch.log(y))\n",
        "        # For losses, use u_minus_agent (as defined elsewhere)\n",
        "        v = torch.where(R >= 0, u_plus_val, -u_minus_agent(R, params, epsilon, min_val))\n",
        "        # Now compute dynamic weights from p_star using our new functions:\n",
        "        weight_gain = w_plus_prime_dynamic(p_star, params, epsilon)\n",
        "        weight_loss = w_minus_prime_dynamic(p_star, params, epsilon)\n",
        "        phi = torch.where(R >= 0, weight_gain * v, -weight_loss * v)\n",
        "        phi_values.append(phi)\n",
        "\n",
        "    phi_stack = torch.stack(phi_values)\n",
        "    return phi_stack.mean(dim=0)\n",
        "\n",
        "def C_transform_cross_dynamic(x, adaptive_params, epsilon=1e-6, min_val=1e-3):\n",
        "    \"\"\"\n",
        "    Compute the CPT-transformed target value by averaging each agent’s transformation,\n",
        "    using dynamic weighting rather than fixed constants.\n",
        "    \"\"\"\n",
        "    transformed_vals = []\n",
        "    # Optionally, define a normalization for x to obtain a probability. For instance:\n",
        "    p_star = torch.sigmoid(x)\n",
        "    for agent_id, param_module in adaptive_params.items():\n",
        "        params = param_module.get_params()\n",
        "        y = torch.clamp(x + epsilon, min=min_val)\n",
        "        # Compute u_plus for gains:\n",
        "        u_plus_val = torch.exp(params[\"alpha\"] * torch.log(y))\n",
        "        # For losses, use u_minus_agent as before:\n",
        "        transformed = torch.where(\n",
        "            x >= 0,\n",
        "            w_plus_prime_dynamic(p_star, params, epsilon) * u_plus_val,\n",
        "            -w_minus_prime_dynamic(p_star, params, epsilon) * u_minus_agent(x, params, epsilon, min_val)\n",
        "        )\n",
        "        # Clamp the final output for safety:\n",
        "        transformed = torch.clamp(transformed, min=-1e6, max=1e6)\n",
        "        transformed_vals.append(transformed)\n",
        "    return torch.stack(transformed_vals).mean(dim=0)\n",
        "\n",
        "\n",
        "def C_transform(x):\n",
        "    \"\"\"\n",
        "    Simple CPT transformation on one-step return:\n",
        "    C(x) ≈ w'_+(p*) * u^+(x) if x >= 0, else -w'_-(p*) * u^-(x).\n",
        "    \"\"\"\n",
        "    return torch.where(x >= 0, w_plus_prime_const * u_plus(x), -w_minus_prime_const * u_minus(x))"
      ],
      "metadata": {
        "id": "h9DCI7ydFUq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from tensordict import TensorDict, TensorDictBase, TensorDictParams\n",
        "from tensordict.nn import dispatch, TensorDictModule\n",
        "\n",
        "from tensordict.utils import NestedKey, unravel_key\n",
        "from torchrl.modules.tensordict_module.actors import ActorCriticWrapper\n",
        "from torchrl.objectives.common import LossModule\n",
        "from torchrl.objectives.utils import (\n",
        "    _cache_values,\n",
        "    _GAMMA_LMBDA_DEPREC_ERROR,\n",
        "    _reduce,\n",
        "    default_value_kwargs,\n",
        "    distance_loss,\n",
        "    ValueEstimators,\n",
        ")\n",
        "from torchrl.objectives.value import TD0Estimator, TD1Estimator, TDLambdaEstimator\n",
        "\n",
        "\n",
        "class CPTDDPGLoss(LossModule):\n",
        "    \"\"\"The DDPG Loss class.\n",
        "\n",
        "    Args:\n",
        "        actor_network (TensorDictModule): a policy operator.\n",
        "        value_network (TensorDictModule): a Q value operator.\n",
        "        loss_function (str): loss function for the value discrepancy. Can be one of \"l1\", \"l2\" or \"smooth_l1\".\n",
        "        delay_actor (bool, optional): whether to separate the target actor networks from the actor networks used for\n",
        "            data collection. Default is ``False``.\n",
        "        delay_value (bool, optional): whether to separate the target value networks from the value networks used for\n",
        "            data collection. Default is ``True``.\n",
        "        separate_losses (bool, optional): if ``True``, shared parameters between\n",
        "            policy and critic will only be trained on the policy loss.\n",
        "            Defaults to ``False``, i.e., gradients are propagated to shared\n",
        "            parameters for both policy and critic losses.\n",
        "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
        "            ``\"none\"`` | ``\"mean\"`` | ``\"sum\"``. ``\"none\"``: no reduction will be applied,\n",
        "            ``\"mean\"``: the sum of the output will be divided by the number of\n",
        "            elements in the output, ``\"sum\"``: the output will be summed. Default: ``\"mean\"``.\n",
        "\n",
        "    Examples:\n",
        "        >>> import torch\n",
        "        >>> from torch import nn\n",
        "        >>> from torchrl.data import Bounded\n",
        "        >>> from torchrl.modules.tensordict_module.actors import Actor, ValueOperator\n",
        "        >>> from torchrl.objectives.ddpg import DDPGLoss\n",
        "        >>> from tensordict import TensorDict\n",
        "        >>> n_act, n_obs = 4, 3\n",
        "        >>> spec = Bounded(-torch.ones(n_act), torch.ones(n_act), (n_act,))\n",
        "        >>> actor = Actor(spec=spec, module=nn.Linear(n_obs, n_act))\n",
        "        >>> class ValueClass(nn.Module):\n",
        "        ...     def __init__(self):\n",
        "        ...         super().__init__()\n",
        "        ...         self.linear = nn.Linear(n_obs + n_act, 1)\n",
        "        ...     def forward(self, obs, act):\n",
        "        ...         return self.linear(torch.cat([obs, act], -1))\n",
        "        >>> module = ValueClass()\n",
        "        >>> value = ValueOperator(\n",
        "        ...     module=module,\n",
        "        ...     in_keys=[\"observation\", \"action\"])\n",
        "        >>> loss = DDPGLoss(actor, value)\n",
        "        >>> batch = [2, ]\n",
        "        >>> data = TensorDict({\n",
        "        ...        \"observation\": torch.randn(*batch, n_obs),\n",
        "        ...        \"action\": spec.rand(batch),\n",
        "        ...        (\"next\", \"done\"): torch.zeros(*batch, 1, dtype=torch.bool),\n",
        "        ...        (\"next\", \"terminated\"): torch.zeros(*batch, 1, dtype=torch.bool),\n",
        "        ...        (\"next\", \"reward\"): torch.randn(*batch, 1),\n",
        "        ...        (\"next\", \"observation\"): torch.randn(*batch, n_obs),\n",
        "        ...    }, batch)\n",
        "        >>> loss(data)\n",
        "        TensorDict(\n",
        "            fields={\n",
        "                loss_actor: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                loss_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                pred_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                pred_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                target_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
        "                target_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
        "            batch_size=torch.Size([]),\n",
        "            device=None,\n",
        "            is_shared=False)\n",
        "\n",
        "    This class is compatible with non-tensordict based modules too and can be\n",
        "    used without recurring to any tensordict-related primitive. In this case,\n",
        "    the expected keyword arguments are:\n",
        "    ``[\"next_reward\", \"next_done\", \"next_terminated\"]`` + in_keys of the actor_network and value_network.\n",
        "    The return value is a tuple of tensors in the following order:\n",
        "    ``[\"loss_actor\", \"loss_value\", \"pred_value\", \"target_value\", \"pred_value_max\", \"target_value_max\"]``\n",
        "\n",
        "    Examples:\n",
        "        >>> import torch\n",
        "        >>> from torch import nn\n",
        "        >>> from torchrl.data import Bounded\n",
        "        >>> from torchrl.modules.tensordict_module.actors import Actor, ValueOperator\n",
        "        >>> from torchrl.objectives.ddpg import DDPGLoss\n",
        "        >>> _ = torch.manual_seed(42)\n",
        "        >>> n_act, n_obs = 4, 3\n",
        "        >>> spec = Bounded(-torch.ones(n_act), torch.ones(n_act), (n_act,))\n",
        "        >>> actor = Actor(spec=spec, module=nn.Linear(n_obs, n_act))\n",
        "        >>> class ValueClass(nn.Module):\n",
        "        ...     def __init__(self):\n",
        "        ...         super().__init__()\n",
        "        ...         self.linear = nn.Linear(n_obs + n_act, 1)\n",
        "        ...     def forward(self, obs, act):\n",
        "        ...         return self.linear(torch.cat([obs, act], -1))\n",
        "        >>> module = ValueClass()\n",
        "        >>> value = ValueOperator(\n",
        "        ...     module=module,\n",
        "        ...     in_keys=[\"observation\", \"action\"])\n",
        "        >>> loss = DDPGLoss(actor, value)\n",
        "        >>> loss_actor, loss_value, pred_value, target_value, pred_value_max, target_value_max = loss(\n",
        "        ...     observation=torch.randn(n_obs),\n",
        "        ...     action=spec.rand(),\n",
        "        ...     next_done=torch.zeros(1, dtype=torch.bool),\n",
        "        ...     next_terminated=torch.zeros(1, dtype=torch.bool),\n",
        "        ...     next_observation=torch.randn(n_obs),\n",
        "        ...     next_reward=torch.randn(1))\n",
        "        >>> loss_actor.backward()\n",
        "\n",
        "    The output keys can also be filtered using the :meth:`DDPGLoss.select_out_keys`\n",
        "    method.\n",
        "\n",
        "    Examples:\n",
        "        >>> loss.select_out_keys('loss_actor', 'loss_value')\n",
        "        >>> loss_actor, loss_value = loss(\n",
        "        ...     observation=torch.randn(n_obs),\n",
        "        ...     action=spec.rand(),\n",
        "        ...     next_done=torch.zeros(1, dtype=torch.bool),\n",
        "        ...     next_terminated=torch.zeros(1, dtype=torch.bool),\n",
        "        ...     next_observation=torch.randn(n_obs),\n",
        "        ...     next_reward=torch.randn(1))\n",
        "        >>> loss_actor.backward()\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    @dataclass\n",
        "    class _AcceptedKeys:\n",
        "        \"\"\"Maintains default values for all configurable tensordict keys.\n",
        "\n",
        "        This class defines which tensordict keys can be set using '.set_keys(key_name=key_value)' and their\n",
        "        default values.\n",
        "\n",
        "        Attributes:\n",
        "            state_action_value (NestedKey): The input tensordict key where the\n",
        "                state action value is expected. Will be used for the underlying\n",
        "                value estimator as value key. Defaults to ``\"state_action_value\"``.\n",
        "            priority (NestedKey): The input tensordict key where the target\n",
        "                priority is written to. Defaults to ``\"td_error\"``.\n",
        "            reward (NestedKey): The input tensordict key where the reward is expected.\n",
        "                Will be used for the underlying value estimator. Defaults to ``\"reward\"``.\n",
        "            done (NestedKey): The key in the input TensorDict that indicates\n",
        "                whether a trajectory is done. Will be used for the underlying value estimator.\n",
        "                Defaults to ``\"done\"``.\n",
        "            terminated (NestedKey): The key in the input TensorDict that indicates\n",
        "                whether a trajectory is terminated. Will be used for the underlying value estimator.\n",
        "                Defaults to ``\"terminated\"``.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        state_action_value: NestedKey = \"state_action_value\"\n",
        "        priority: NestedKey = \"td_error\"\n",
        "        reward: NestedKey = \"reward\"\n",
        "        done: NestedKey = \"done\"\n",
        "        terminated: NestedKey = \"terminated\"\n",
        "\n",
        "    tensor_keys: _AcceptedKeys\n",
        "    default_keys = _AcceptedKeys\n",
        "    default_value_estimator: ValueEstimators = ValueEstimators.TD0\n",
        "    out_keys = [\n",
        "        \"loss_actor\",\n",
        "        \"loss_value\",\n",
        "        \"pred_value\",\n",
        "        \"target_value\",\n",
        "        \"pred_value_max\",\n",
        "        \"target_value_max\",\n",
        "    ]\n",
        "\n",
        "    actor_network: TensorDictModule\n",
        "    value_network: actor_network\n",
        "    actor_network_params: TensorDictParams\n",
        "    value_network_params: TensorDictParams\n",
        "    target_actor_network_params: TensorDictParams\n",
        "    target_value_network_params: TensorDictParams\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        actor_network: TensorDictModule,\n",
        "        value_network: TensorDictModule,\n",
        "        *,\n",
        "        loss_function: str = \"l2\",\n",
        "        delay_actor: bool = False,\n",
        "        delay_value: bool = True,\n",
        "        gamma: float = None,\n",
        "        separate_losses: bool = False,\n",
        "        reduction: str = None,\n",
        "    ) -> None:\n",
        "        self._in_keys = None\n",
        "        if reduction is None:\n",
        "            reduction = \"mean\"\n",
        "        super().__init__()\n",
        "        self.delay_actor = delay_actor\n",
        "        self.delay_value = delay_value\n",
        "\n",
        "        actor_critic = ActorCriticWrapper(actor_network, value_network)\n",
        "        params = TensorDict.from_module(actor_critic)\n",
        "        params_meta = params.apply(\n",
        "            self._make_meta_params, device=torch.device(\"meta\"), filter_empty=False\n",
        "        )\n",
        "        with params_meta.to_module(actor_critic):\n",
        "            self.__dict__[\"actor_critic\"] = deepcopy(actor_critic)\n",
        "\n",
        "        self.convert_to_functional(\n",
        "            actor_network,\n",
        "            \"actor_network\",\n",
        "            create_target_params=self.delay_actor,\n",
        "        )\n",
        "        if separate_losses:\n",
        "            # we want to make sure there are no duplicates in the params: the\n",
        "            # params of critic must be refs to actor if they're shared\n",
        "            policy_params = list(actor_network.parameters())\n",
        "        else:\n",
        "            policy_params = None\n",
        "        self.convert_to_functional(\n",
        "            value_network,\n",
        "            \"value_network\",\n",
        "            create_target_params=self.delay_value,\n",
        "            compare_against=policy_params,\n",
        "        )\n",
        "        self.actor_critic.module[0] = self.actor_network\n",
        "        self.actor_critic.module[1] = self.value_network\n",
        "\n",
        "        self.actor_in_keys = actor_network.in_keys\n",
        "        self.value_exclusive_keys = set(self.value_network.in_keys) - (\n",
        "            set(self.actor_in_keys) | set(self.actor_network.out_keys)\n",
        "        )\n",
        "\n",
        "        self.loss_function = loss_function\n",
        "        self.reduction = reduction\n",
        "        if gamma is not None:\n",
        "            raise TypeError(_GAMMA_LMBDA_DEPREC_ERROR)\n",
        "\n",
        "    def _forward_value_estimator_keys(self, **kwargs) -> None:\n",
        "        if self._value_estimator is not None:\n",
        "            self._value_estimator.set_keys(\n",
        "                value=self._tensor_keys.state_action_value,\n",
        "                reward=self._tensor_keys.reward,\n",
        "                done=self._tensor_keys.done,\n",
        "                terminated=self._tensor_keys.terminated,\n",
        "            )\n",
        "        self._set_in_keys()\n",
        "\n",
        "    def _set_in_keys(self):\n",
        "        in_keys = {\n",
        "            unravel_key((\"next\", self.tensor_keys.reward)),\n",
        "            unravel_key((\"next\", self.tensor_keys.done)),\n",
        "            unravel_key((\"next\", self.tensor_keys.terminated)),\n",
        "            *self.actor_in_keys,\n",
        "            *[unravel_key((\"next\", key)) for key in self.actor_in_keys],\n",
        "            *self.value_network.in_keys,\n",
        "            *[unravel_key((\"next\", key)) for key in self.value_network.in_keys],\n",
        "        }\n",
        "        self._in_keys = sorted(in_keys, key=str)\n",
        "\n",
        "    @property\n",
        "    def in_keys(self):\n",
        "        if self._in_keys is None:\n",
        "            self._set_in_keys()\n",
        "        return self._in_keys\n",
        "\n",
        "    @in_keys.setter\n",
        "    def in_keys(self, values):\n",
        "        self._in_keys = values\n",
        "\n",
        "\n",
        "    def _clear_weakrefs(self, *tds):\n",
        "        if is_compiling():\n",
        "            # Waiting for weakrefs reconstruct to be supported by compile\n",
        "            for td in tds:\n",
        "                if isinstance(td, str):\n",
        "                    td = getattr(self, td, None)\n",
        "                if not is_tensor_collection(td):\n",
        "                    continue\n",
        "                td.clear_refs_for_compile_()\n",
        "\n",
        "    @dispatch\n",
        "    def forward(self, tensordict: TensorDictBase) -> TensorDict:\n",
        "        \"\"\"Computes the DDPG losses given a tensordict sampled from the replay buffer.\n",
        "\n",
        "        This function will also write a \"td_error\" key that can be used by prioritized replay buffers to assign\n",
        "            a priority to items in the tensordict.\n",
        "\n",
        "        Args:\n",
        "            tensordict (TensorDictBase): a tensordict with keys [\"done\", \"terminated\", \"reward\"] and the in_keys of the actor\n",
        "                and value networks.\n",
        "\n",
        "        Returns:\n",
        "            a tuple of 2 tensors containing the DDPG loss.\n",
        "\n",
        "        \"\"\"\n",
        "        loss_value, metadata = self.loss_value(tensordict)\n",
        "        loss_actor, metadata_actor = self.loss_actor(tensordict)\n",
        "        metadata.update(metadata_actor)\n",
        "        td_out = TensorDict(\n",
        "            source={\"loss_actor\": loss_actor, \"loss_value\": loss_value, **metadata},\n",
        "            batch_size=[],\n",
        "        )\n",
        "        self._clear_weakrefs(\n",
        "            tensordict,\n",
        "            td_out,\n",
        "            \"value_network_params\",\n",
        "            \"target_value_network_params\",\n",
        "            \"target_actor_network_params\",\n",
        "            \"actor_network_params\",\n",
        "        )\n",
        "        return td_out\n",
        "\n",
        "    def loss_actor(self, tensordict: TensorDictBase) -> Tuple[torch.Tensor, dict]:\n",
        "        td_copy = tensordict.select(\n",
        "            *self.actor_in_keys, *self.value_exclusive_keys, strict=False\n",
        "        ).detach()\n",
        "\n",
        "        with self.actor_network_params.to_module(self.actor_network):\n",
        "            td_copy = self.actor_network(td_copy)\n",
        "\n",
        "        with self._cached_detached_value_params.to_module(self.value_network):\n",
        "            td_copy = self.value_network(td_copy)\n",
        "\n",
        "        actions = td_copy.get((self.actor_network.in_keys[0][0], \"action\"))\n",
        "        Q_values = td_copy.get(self.tensor_keys.state_action_value).squeeze(-1)\n",
        "        grad_Q = torch.autograd.grad(Q_values.sum(), actions, retain_graph=True)[0]\n",
        "\n",
        "        # Retrieve the cooperative reward (assuming common group 'agents')\n",
        "        returns = tensordict.get((\"agents\", \"episode_reward\")).view(-1)\n",
        "        # Now use the dynamic version that computes φ using your learnable weighting functions:\n",
        "        phi_factor = compute_phi_cross_dynamic(returns, adaptive_params)\n",
        "\n",
        "        policy_gradient = actions * grad_Q\n",
        "        loss_actor = -phi_factor.mean() * policy_gradient.mean()\n",
        "\n",
        "        return _reduce(loss_actor, self.reduction), {}\n",
        "\n",
        "\n",
        "    def loss_value(self, tensordict: TensorDictBase) -> Tuple[torch.Tensor, dict]:\n",
        "        td_copy = tensordict.select(*self.value_network.in_keys, strict=False).detach()\n",
        "        with self.value_network_params.to_module(self.value_network):\n",
        "            self.value_network(td_copy)\n",
        "        pred_val = td_copy.get(self.tensor_keys.state_action_value).squeeze(-1)\n",
        "\n",
        "        target_value = self.value_estimator.value_estimate(\n",
        "            tensordict, target_params=self._cached_target_params\n",
        "        ).squeeze(-1)\n",
        "        # Replace the old transformation with your new dynamic version:\n",
        "        target_value_CPT = C_transform_cross_dynamic(target_value, adaptive_params)\n",
        "\n",
        "        loss_value = distance_loss(pred_val, target_value_CPT, loss_function=self.loss_function)\n",
        "        tensordict.set(\"target_value_CPT\", target_value_CPT, inplace=True)\n",
        "\n",
        "        td_error = (pred_val - target_value_CPT).pow(2).detach()\n",
        "        tensordict.set(self.tensor_keys.priority, td_error, inplace=True)\n",
        "\n",
        "        metadata = {\n",
        "            \"td_error\": td_error,\n",
        "            \"pred_value\": pred_val,\n",
        "            \"target_value_CPT\": target_value_CPT,\n",
        "        }\n",
        "        return _reduce(loss_value, self.reduction), metadata\n",
        "\n",
        "\n",
        "    def make_value_estimator(self, value_type: ValueEstimators = None, **hyperparams):\n",
        "        if value_type is None:\n",
        "            value_type = self.default_value_estimator\n",
        "        self.value_type = value_type\n",
        "        hp = dict(default_value_kwargs(value_type))\n",
        "        if hasattr(self, \"gamma\"):\n",
        "            hp[\"gamma\"] = self.gamma\n",
        "        hp.update(hyperparams)\n",
        "        if value_type == ValueEstimators.TD1:\n",
        "            self._value_estimator = TD1Estimator(value_network=self.actor_critic, **hp)\n",
        "        elif value_type == ValueEstimators.TD0:\n",
        "            self._value_estimator = TD0Estimator(value_network=self.actor_critic, **hp)\n",
        "        elif value_type == ValueEstimators.GAE:\n",
        "            raise NotImplementedError(\n",
        "                f\"Value type {value_type} it not implemented for loss {type(self)}.\"\n",
        "            )\n",
        "        elif value_type == ValueEstimators.TDLambda:\n",
        "            self._value_estimator = TDLambdaEstimator(\n",
        "                value_network=self.actor_critic, **hp\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Unknown value type {value_type}\")\n",
        "\n",
        "        tensor_keys = {\n",
        "            \"value\": self.tensor_keys.state_action_value,\n",
        "            \"reward\": self.tensor_keys.reward,\n",
        "            \"done\": self.tensor_keys.done,\n",
        "            \"terminated\": self.tensor_keys.terminated,\n",
        "        }\n",
        "        self._value_estimator.set_keys(**tensor_keys)\n",
        "\n",
        "    @property\n",
        "    @_cache_values\n",
        "    def _cached_target_params(self):\n",
        "        target_params = TensorDict(\n",
        "            {\n",
        "                \"module\": {\n",
        "                    \"0\": self.target_actor_network_params,\n",
        "                    \"1\": self.target_value_network_params,\n",
        "                }\n",
        "            },\n",
        "            batch_size=self.target_actor_network_params.batch_size,\n",
        "            device=self.target_actor_network_params.device,\n",
        "        )\n",
        "        return target_params\n",
        "\n",
        "    @property\n",
        "    @_cache_values\n",
        "    def _cached_detached_value_params(self):\n",
        "        return self.value_network_params.detach()"
      ],
      "metadata": {
        "id": "FRx2-pu1FZhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ4u6U6EWfBT"
      },
      "outputs": [],
      "source": [
        "losses = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    loss_module = CPTDDPGLoss(\n",
        "        actor_network=policies[group],  # Use the non-explorative policies\n",
        "        value_network=critics[group],\n",
        "        delay_value=True,  # Whether to use a target network for the value\n",
        "        loss_function=\"l2\",\n",
        "    )\n",
        "    loss_module.set_keys(\n",
        "        state_action_value=(group, \"state_action_value\"),\n",
        "        reward=(group, \"reward\"),\n",
        "        done=(group, \"done\"),\n",
        "        terminated=(group, \"terminated\"),\n",
        "    )\n",
        "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
        "\n",
        "    losses[group] = loss_module\n",
        "\n",
        "target_updaters = {\n",
        "    group: SoftUpdate(loss, tau=polyak_tau) for group, loss in losses.items()\n",
        "}\n",
        "\n",
        "optimisers = {\n",
        "    group: {\n",
        "        \"loss_actor\": torch.optim.Adam(\n",
        "            loss.actor_network_params.flatten_keys().values(), lr=lr\n",
        "        ),\n",
        "        \"loss_value\": torch.optim.Adam(\n",
        "            loss.value_network_params.flatten_keys().values(), lr=lr\n",
        "        ),\n",
        "    }\n",
        "    for group, loss in losses.items()\n",
        "}\n",
        "\n",
        "optimizer_behavioral = torch.optim.Adam(adaptive_params.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjBvnOLGWgnp"
      },
      "outputs": [],
      "source": [
        "def process_batch(batch: TensorDictBase) -> TensorDictBase:\n",
        "    \"\"\"\n",
        "    If the `(group, \"terminated\")` and `(group, \"done\")` keys are not present, create them by expanding\n",
        "    `\"terminated\"` and `\"done\"`.\n",
        "    This is needed to present them with the same shape as the reward to the loss.\n",
        "    \"\"\"\n",
        "    for group in env.group_map.keys():\n",
        "        keys = list(batch.keys(True, True))\n",
        "        group_shape = batch.get_item_shape(group)\n",
        "        nested_done_key = (\"next\", group, \"done\")\n",
        "        nested_terminated_key = (\"next\", group, \"terminated\")\n",
        "        if nested_done_key not in keys:\n",
        "            batch.set(\n",
        "                nested_done_key,\n",
        "                batch.get((\"next\", \"done\")).unsqueeze(-1).expand((*group_shape, 1)),\n",
        "            )\n",
        "        if nested_terminated_key not in keys:\n",
        "            batch.set(\n",
        "                nested_terminated_key,\n",
        "                batch.get((\"next\", \"terminated\"))\n",
        "                .unsqueeze(-1)\n",
        "                .expand((*group_shape, 1)),\n",
        "            )\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_adaptive_loss(replay_sample, value_estimator, adaptive_params, target_params, epsilon=1e-6, min_val=1e-3):\n",
        "    # Compute the standard target Q-value:\n",
        "    target_value = value_estimator.value_estimate(replay_sample, target_params=target_params).squeeze(-1)\n",
        "    # Compute the CPT-transformed target using adaptive parameters:\n",
        "    target_value_CPT = C_transform_cross_dynamic(target_value, adaptive_params, epsilon, min_val)\n",
        "    base_adaptive_loss = torch.mean((target_value_CPT - target_value) ** 2)\n",
        "    return base_adaptive_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "n_e_UTSVSwLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwLjnzsLWjzS",
        "outputId": "8cab05d8-4544-4bdc-900b-80d6638b2eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "episode_reward_mean_agents = -408.6085510253906:  15%|█▌        | 30/200 [04:03<22:57,  8.10s/it]\n",
            "episode_reward_mean_agents = -416.01910400390625:   0%|          | 1/200 [00:09<31:16,  9.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0 - agent_0 alpha: 1.200000, grad mean: 7.768896\n",
            "Iter 0 - agent_0 lam: 1.500000, grad mean: 0.288184\n",
            "Iter 0 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000000\n",
            "Iter 0 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 23.335531\n",
            "Iter 0 - agent_1 alpha: 1.200000, grad mean: 6.215115\n",
            "Iter 0 - agent_1 lam: 1.200000, grad mean: 0.288184\n",
            "Iter 0 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000000\n",
            "Iter 0 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 18.668434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -528.5809936523438:   1%|          | 2/200 [00:18<31:09,  9.44s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 1 - agent_0 alpha: 1.200000, grad mean: 17.017302\n",
            "Iter 1 - agent_0 lam: 1.500000, grad mean: 1.084136\n",
            "Iter 1 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000001\n",
            "Iter 1 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 51.018509\n",
            "Iter 1 - agent_1 alpha: 1.200000, grad mean: 13.613836\n",
            "Iter 1 - agent_1 lam: 1.200000, grad mean: 1.084136\n",
            "Iter 1 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000001\n",
            "Iter 1 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 40.814781\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -424.1863708496094:   2%|▏         | 3/200 [00:23<24:04,  7.33s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 2 - agent_0 alpha: 1.200000, grad mean: 25.307686\n",
            "Iter 2 - agent_0 lam: 1.500000, grad mean: 1.816242\n",
            "Iter 2 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000001\n",
            "Iter 2 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 75.562805\n",
            "Iter 2 - agent_1 alpha: 1.200000, grad mean: 20.246155\n",
            "Iter 2 - agent_1 lam: 1.200000, grad mean: 1.816242\n",
            "Iter 2 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000001\n",
            "Iter 2 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 60.450218\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -446.01373291015625:   2%|▏         | 4/200 [00:28<20:34,  6.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 3 - agent_0 alpha: 1.200000, grad mean: 34.181435\n",
            "Iter 3 - agent_0 lam: 1.500000, grad mean: 2.619662\n",
            "Iter 3 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000001\n",
            "Iter 3 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 102.085045\n",
            "Iter 3 - agent_1 alpha: 1.200000, grad mean: 27.345154\n",
            "Iter 3 - agent_1 lam: 1.200000, grad mean: 2.619662\n",
            "Iter 3 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000001\n",
            "Iter 3 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 81.668053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -522.5723876953125:   2%|▎         | 5/200 [00:33<19:22,  5.96s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 4 - agent_0 alpha: 1.200000, grad mean: 43.946621\n",
            "Iter 4 - agent_0 lam: 1.500000, grad mean: 3.538949\n",
            "Iter 4 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000001\n",
            "Iter 4 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 130.896408\n",
            "Iter 4 - agent_1 alpha: 1.200000, grad mean: 35.157288\n",
            "Iter 4 - agent_1 lam: 1.200000, grad mean: 3.538949\n",
            "Iter 4 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000001\n",
            "Iter 4 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 104.717178\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -559.8995971679688:   3%|▎         | 6/200 [00:38<17:55,  5.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 5 - agent_0 alpha: 1.200000, grad mean: 53.049320\n",
            "Iter 5 - agent_0 lam: 1.500000, grad mean: 4.419077\n",
            "Iter 5 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000002\n",
            "Iter 5 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 157.726501\n",
            "Iter 5 - agent_1 alpha: 1.200000, grad mean: 42.439430\n",
            "Iter 5 - agent_1 lam: 1.200000, grad mean: 4.419077\n",
            "Iter 5 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000002\n",
            "Iter 5 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 126.181252\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -693.171142578125:   4%|▎         | 7/200 [00:43<17:32,  5.45s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 6 - agent_0 alpha: 1.200000, grad mean: 61.777294\n",
            "Iter 6 - agent_0 lam: 1.500000, grad mean: 5.282745\n",
            "Iter 6 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000002\n",
            "Iter 6 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 183.008682\n",
            "Iter 6 - agent_1 alpha: 1.200000, grad mean: 49.421806\n",
            "Iter 6 - agent_1 lam: 1.200000, grad mean: 5.282745\n",
            "Iter 6 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000002\n",
            "Iter 6 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 146.407043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -525.5447387695312:   4%|▍         | 8/200 [00:48<16:42,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 7 - agent_0 alpha: 1.200000, grad mean: 69.802643\n",
            "Iter 7 - agent_0 lam: 1.500000, grad mean: 6.067434\n",
            "Iter 7 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000002\n",
            "Iter 7 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 206.275375\n",
            "Iter 7 - agent_1 alpha: 1.200000, grad mean: 55.842094\n",
            "Iter 7 - agent_1 lam: 1.200000, grad mean: 6.067434\n",
            "Iter 7 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000002\n",
            "Iter 7 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 165.020493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -628.6935424804688:   4%|▍         | 9/200 [00:53<16:12,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 8 - agent_0 alpha: 1.200000, grad mean: 77.536430\n",
            "Iter 8 - agent_0 lam: 1.500000, grad mean: 6.812874\n",
            "Iter 8 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000002\n",
            "Iter 8 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 228.660812\n",
            "Iter 8 - agent_1 alpha: 1.200000, grad mean: 62.029087\n",
            "Iter 8 - agent_1 lam: 1.200000, grad mean: 6.812874\n",
            "Iter 8 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000002\n",
            "Iter 8 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 182.928894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -570.34912109375:   5%|▌         | 10/200 [00:58<16:17,  5.15s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 9 - agent_0 alpha: 1.200000, grad mean: 85.974892\n",
            "Iter 9 - agent_0 lam: 1.500000, grad mean: 7.633640\n",
            "Iter 9 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 9 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 252.993805\n",
            "Iter 9 - agent_1 alpha: 1.200000, grad mean: 68.779816\n",
            "Iter 9 - agent_1 lam: 1.200000, grad mean: 7.633640\n",
            "Iter 9 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 9 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 202.395294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -685.1183471679688:   6%|▌         | 11/200 [01:03<15:46,  5.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 10 - agent_0 alpha: 1.200000, grad mean: 94.028763\n",
            "Iter 10 - agent_0 lam: 1.500000, grad mean: 8.422990\n",
            "Iter 10 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 10 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 276.088898\n",
            "Iter 10 - agent_1 alpha: 1.200000, grad mean: 75.222954\n",
            "Iter 10 - agent_1 lam: 1.200000, grad mean: 8.422990\n",
            "Iter 10 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 10 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 220.871292\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -649.6156005859375:   6%|▌         | 12/200 [01:08<15:56,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 11 - agent_0 alpha: 1.200000, grad mean: 102.016991\n",
            "Iter 11 - agent_0 lam: 1.500000, grad mean: 9.220422\n",
            "Iter 11 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 11 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 298.899841\n",
            "Iter 11 - agent_1 alpha: 1.200000, grad mean: 81.613510\n",
            "Iter 11 - agent_1 lam: 1.200000, grad mean: 9.220422\n",
            "Iter 11 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 11 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 239.119919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -645.6112670898438:   6%|▋         | 13/200 [01:13<15:29,  4.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 12 - agent_0 alpha: 1.200000, grad mean: 110.478668\n",
            "Iter 12 - agent_0 lam: 1.500000, grad mean: 10.075643\n",
            "Iter 12 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 12 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 322.894562\n",
            "Iter 12 - agent_1 alpha: 1.200000, grad mean: 88.382858\n",
            "Iter 12 - agent_1 lam: 1.200000, grad mean: 10.075643\n",
            "Iter 12 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 12 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 258.315582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -642.0201416015625:   7%|▋         | 14/200 [01:18<15:27,  4.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 13 - agent_0 alpha: 1.200000, grad mean: 119.205757\n",
            "Iter 13 - agent_0 lam: 1.500000, grad mean: 10.958964\n",
            "Iter 13 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 13 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 347.639862\n",
            "Iter 13 - agent_1 alpha: 1.200000, grad mean: 95.364563\n",
            "Iter 13 - agent_1 lam: 1.200000, grad mean: 10.958964\n",
            "Iter 13 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000003\n",
            "Iter 13 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 278.111908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -624.6288452148438:   8%|▊         | 15/200 [01:23<15:25,  5.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 14 - agent_0 alpha: 1.200000, grad mean: 127.965630\n",
            "Iter 14 - agent_0 lam: 1.500000, grad mean: 11.841891\n",
            "Iter 14 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000004\n",
            "Iter 14 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 372.457336\n",
            "Iter 14 - agent_1 alpha: 1.200000, grad mean: 102.372452\n",
            "Iter 14 - agent_1 lam: 1.200000, grad mean: 11.841891\n",
            "Iter 14 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000004\n",
            "Iter 14 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 297.966156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -550.1260375976562:   8%|▊         | 16/200 [01:28<15:07,  4.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 15 - agent_0 alpha: 1.200000, grad mean: 137.374512\n",
            "Iter 15 - agent_0 lam: 1.500000, grad mean: 12.803874\n",
            "Iter 15 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000004\n",
            "Iter 15 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 398.996765\n",
            "Iter 15 - agent_1 alpha: 1.200000, grad mean: 109.899727\n",
            "Iter 15 - agent_1 lam: 1.200000, grad mean: 12.803874\n",
            "Iter 15 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000004\n",
            "Iter 15 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 319.197632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -484.6930236816406:   8%|▊         | 17/200 [01:33<15:23,  5.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 16 - agent_0 alpha: 1.200000, grad mean: 147.293961\n",
            "Iter 16 - agent_0 lam: 1.500000, grad mean: 13.822782\n",
            "Iter 16 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000004\n",
            "Iter 16 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 427.043060\n",
            "Iter 16 - agent_1 alpha: 1.200000, grad mean: 117.835419\n",
            "Iter 16 - agent_1 lam: 1.200000, grad mean: 13.822782\n",
            "Iter 16 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000004\n",
            "Iter 16 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 341.634674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -396.0712585449219:   9%|▉         | 18/200 [01:38<15:03,  4.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 17 - agent_0 alpha: 1.200000, grad mean: 157.260239\n",
            "Iter 17 - agent_0 lam: 1.500000, grad mean: 14.855792\n",
            "Iter 17 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000004\n",
            "Iter 17 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 455.179382\n",
            "Iter 17 - agent_1 alpha: 1.200000, grad mean: 125.808418\n",
            "Iter 17 - agent_1 lam: 1.200000, grad mean: 14.855792\n",
            "Iter 17 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000004\n",
            "Iter 17 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 364.143677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -602.304443359375:  10%|▉         | 19/200 [01:43<15:16,  5.07s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 18 - agent_0 alpha: 1.200000, grad mean: 166.895462\n",
            "Iter 18 - agent_0 lam: 1.500000, grad mean: 15.861482\n",
            "Iter 18 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000005\n",
            "Iter 18 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 482.436340\n",
            "Iter 18 - agent_1 alpha: 1.200000, grad mean: 133.516617\n",
            "Iter 18 - agent_1 lam: 1.200000, grad mean: 15.861482\n",
            "Iter 18 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000005\n",
            "Iter 18 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 385.948883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -551.0182495117188:  10%|█         | 20/200 [01:48<14:49,  4.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 19 - agent_0 alpha: 1.200000, grad mean: 176.788116\n",
            "Iter 19 - agent_0 lam: 1.500000, grad mean: 16.893711\n",
            "Iter 19 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000005\n",
            "Iter 19 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 510.434998\n",
            "Iter 19 - agent_1 alpha: 1.200000, grad mean: 141.430740\n",
            "Iter 19 - agent_1 lam: 1.200000, grad mean: 16.893711\n",
            "Iter 19 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000005\n",
            "Iter 19 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 408.347870\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -494.6054382324219:  10%|█         | 21/200 [01:52<14:31,  4.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 20 - agent_0 alpha: 1.200000, grad mean: 185.801422\n",
            "Iter 20 - agent_0 lam: 1.500000, grad mean: 17.830008\n",
            "Iter 20 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000005\n",
            "Iter 20 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 536.017029\n",
            "Iter 20 - agent_1 alpha: 1.200000, grad mean: 148.641388\n",
            "Iter 20 - agent_1 lam: 1.200000, grad mean: 17.830008\n",
            "Iter 20 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000005\n",
            "Iter 20 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 428.813232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -611.5798950195312:  11%|█         | 22/200 [01:59<15:58,  5.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 21 - agent_0 alpha: 1.200000, grad mean: 195.059082\n",
            "Iter 21 - agent_0 lam: 1.500000, grad mean: 18.799250\n",
            "Iter 21 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000005\n",
            "Iter 21 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 562.230896\n",
            "Iter 21 - agent_1 alpha: 1.200000, grad mean: 156.047516\n",
            "Iter 21 - agent_1 lam: 1.200000, grad mean: 18.799250\n",
            "Iter 21 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000005\n",
            "Iter 21 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 449.784088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -492.9001159667969:  12%|█▏        | 23/200 [02:04<15:28,  5.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 22 - agent_0 alpha: 1.200000, grad mean: 204.398560\n",
            "Iter 22 - agent_0 lam: 1.500000, grad mean: 19.785944\n",
            "Iter 22 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000006\n",
            "Iter 22 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 588.665283\n",
            "Iter 22 - agent_1 alpha: 1.200000, grad mean: 163.519180\n",
            "Iter 22 - agent_1 lam: 1.200000, grad mean: 19.785944\n",
            "Iter 22 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000006\n",
            "Iter 22 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 470.931610\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -622.6068725585938:  12%|█▏        | 24/200 [02:09<15:30,  5.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 23 - agent_0 alpha: 1.200000, grad mean: 213.093582\n",
            "Iter 23 - agent_0 lam: 1.500000, grad mean: 20.692316\n",
            "Iter 23 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000006\n",
            "Iter 23 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 613.260437\n",
            "Iter 23 - agent_1 alpha: 1.200000, grad mean: 170.475235\n",
            "Iter 23 - agent_1 lam: 1.200000, grad mean: 20.692316\n",
            "Iter 23 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000006\n",
            "Iter 23 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 490.607849\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -541.3839111328125:  12%|█▎        | 25/200 [02:14<14:55,  5.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 24 - agent_0 alpha: 1.200000, grad mean: 222.233429\n",
            "Iter 24 - agent_0 lam: 1.500000, grad mean: 21.663507\n",
            "Iter 24 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000006\n",
            "Iter 24 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 639.107361\n",
            "Iter 24 - agent_1 alpha: 1.200000, grad mean: 177.787109\n",
            "Iter 24 - agent_1 lam: 1.200000, grad mean: 21.663507\n",
            "Iter 24 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000006\n",
            "Iter 24 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 511.285370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -421.8601989746094:  13%|█▎        | 26/200 [02:19<14:55,  5.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 25 - agent_0 alpha: 1.200000, grad mean: 231.124939\n",
            "Iter 25 - agent_0 lam: 1.500000, grad mean: 22.600140\n",
            "Iter 25 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000006\n",
            "Iter 25 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 664.234497\n",
            "Iter 25 - agent_1 alpha: 1.200000, grad mean: 184.900284\n",
            "Iter 25 - agent_1 lam: 1.200000, grad mean: 22.600140\n",
            "Iter 25 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000006\n",
            "Iter 25 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 531.387451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -520.3178100585938:  14%|█▎        | 27/200 [02:24<14:35,  5.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 26 - agent_0 alpha: 1.200000, grad mean: 240.071747\n",
            "Iter 26 - agent_0 lam: 1.500000, grad mean: 23.551409\n",
            "Iter 26 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000007\n",
            "Iter 26 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 689.567444\n",
            "Iter 26 - agent_1 alpha: 1.200000, grad mean: 192.057785\n",
            "Iter 26 - agent_1 lam: 1.200000, grad mean: 23.551409\n",
            "Iter 26 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000007\n",
            "Iter 26 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 551.653931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -456.02130126953125:  14%|█▍        | 28/200 [02:29<14:12,  4.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 27 - agent_0 alpha: 1.200000, grad mean: 249.190598\n",
            "Iter 27 - agent_0 lam: 1.500000, grad mean: 24.520519\n",
            "Iter 27 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000007\n",
            "Iter 27 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 715.481750\n",
            "Iter 27 - agent_1 alpha: 1.200000, grad mean: 199.352783\n",
            "Iter 27 - agent_1 lam: 1.200000, grad mean: 24.520519\n",
            "Iter 27 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000007\n",
            "Iter 27 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 572.385315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -477.15460205078125:  14%|█▍        | 29/200 [02:34<14:29,  5.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 28 - agent_0 alpha: 1.200000, grad mean: 258.301666\n",
            "Iter 28 - agent_0 lam: 1.500000, grad mean: 25.493246\n",
            "Iter 28 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000007\n",
            "Iter 28 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 741.296143\n",
            "Iter 28 - agent_1 alpha: 1.200000, grad mean: 206.641541\n",
            "Iter 28 - agent_1 lam: 1.200000, grad mean: 25.493246\n",
            "Iter 28 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000007\n",
            "Iter 28 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 593.036560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -522.0283203125:  15%|█▌        | 30/200 [02:39<14:04,  4.96s/it]    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 29 - agent_0 alpha: 1.200000, grad mean: 267.441254\n",
            "Iter 29 - agent_0 lam: 1.500000, grad mean: 26.465683\n",
            "Iter 29 - agent_0 w_plus_prime_gamma: 0.500000, grad mean: 0.000007\n",
            "Iter 29 - agent_0 w_minus_prime_gamma: 0.690000, grad mean: 767.208618\n",
            "Iter 29 - agent_1 alpha: 1.200000, grad mean: 213.953262\n",
            "Iter 29 - agent_1 lam: 1.200000, grad mean: 26.465683\n",
            "Iter 29 - agent_1 w_plus_prime_gamma: 0.500000, grad mean: 0.000007\n",
            "Iter 29 - agent_1 w_minus_prime_gamma: 0.690000, grad mean: 613.766602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -408.2050476074219:  16%|█▌        | 31/200 [02:44<14:21,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 30: Adaptive loss = 0.03160744160413742, dynamic factor = 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -371.7554626464844:  16%|█▌        | 32/200 [02:49<14:00,  5.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 31 - agent_0 alpha: 1.200010, grad mean: 9.335076\n",
            "Iter 31 - agent_0 lam: 1.500010, grad mean: 1.002686\n",
            "Iter 31 - agent_0 w_plus_prime_gamma: 0.500004, grad mean: 0.000000\n",
            "Iter 31 - agent_0 w_minus_prime_gamma: 0.689990, grad mean: 26.605682\n",
            "Iter 31 - agent_1 alpha: 1.200010, grad mean: 7.468073\n",
            "Iter 31 - agent_1 lam: 1.200010, grad mean: 1.002686\n",
            "Iter 31 - agent_1 w_plus_prime_gamma: 0.500004, grad mean: 0.000000\n",
            "Iter 31 - agent_1 w_minus_prime_gamma: 0.689990, grad mean: 21.284578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -421.3049011230469:  16%|█▋        | 33/200 [02:54<13:49,  4.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 32 - agent_0 alpha: 1.200010, grad mean: 18.442774\n",
            "Iter 32 - agent_0 lam: 1.500010, grad mean: 1.977702\n",
            "Iter 32 - agent_0 w_plus_prime_gamma: 0.500004, grad mean: 0.000001\n",
            "Iter 32 - agent_0 w_minus_prime_gamma: 0.689990, grad mean: 52.546242\n",
            "Iter 32 - agent_1 alpha: 1.200010, grad mean: 14.754236\n",
            "Iter 32 - agent_1 lam: 1.200010, grad mean: 1.977702\n",
            "Iter 32 - agent_1 w_plus_prime_gamma: 0.500004, grad mean: 0.000001\n",
            "Iter 32 - agent_1 w_minus_prime_gamma: 0.689990, grad mean: 42.037045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -464.3368225097656:  17%|█▋        | 34/200 [02:59<13:52,  5.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 33 - agent_0 alpha: 1.200010, grad mean: 27.613268\n",
            "Iter 33 - agent_0 lam: 1.500010, grad mean: 2.963539\n",
            "Iter 33 - agent_0 w_plus_prime_gamma: 0.500004, grad mean: 0.000001\n",
            "Iter 33 - agent_0 w_minus_prime_gamma: 0.689990, grad mean: 78.675110\n",
            "Iter 33 - agent_1 alpha: 1.200010, grad mean: 22.090649\n",
            "Iter 33 - agent_1 lam: 1.200010, grad mean: 2.963539\n",
            "Iter 33 - agent_1 w_plus_prime_gamma: 0.500004, grad mean: 0.000001\n",
            "Iter 33 - agent_1 w_minus_prime_gamma: 0.689990, grad mean: 62.940159\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -352.2590637207031:  18%|█▊        | 35/200 [03:04<13:40,  4.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 34 - agent_0 alpha: 1.200010, grad mean: 36.864113\n",
            "Iter 34 - agent_0 lam: 1.500010, grad mean: 3.966151\n",
            "Iter 34 - agent_0 w_plus_prime_gamma: 0.500004, grad mean: 0.000001\n",
            "Iter 34 - agent_0 w_minus_prime_gamma: 0.689990, grad mean: 105.045303\n",
            "Iter 34 - agent_1 alpha: 1.200010, grad mean: 29.491325\n",
            "Iter 34 - agent_1 lam: 1.200010, grad mean: 3.966151\n",
            "Iter 34 - agent_1 w_plus_prime_gamma: 0.500004, grad mean: 0.000001\n",
            "Iter 34 - agent_1 w_minus_prime_gamma: 0.689990, grad mean: 84.036339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -384.51953125:  18%|█▊        | 36/200 [03:09<13:59,  5.12s/it]     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 35 - agent_0 alpha: 1.200010, grad mean: 46.261086\n",
            "Iter 35 - agent_0 lam: 1.500010, grad mean: 4.980704\n",
            "Iter 35 - agent_0 w_plus_prime_gamma: 0.500004, grad mean: 0.000001\n",
            "Iter 35 - agent_0 w_minus_prime_gamma: 0.689990, grad mean: 131.874207\n",
            "Iter 35 - agent_1 alpha: 1.200010, grad mean: 37.008892\n",
            "Iter 35 - agent_1 lam: 1.200010, grad mean: 4.980704\n",
            "Iter 35 - agent_1 w_plus_prime_gamma: 0.500004, grad mean: 0.000001\n",
            "Iter 35 - agent_1 w_minus_prime_gamma: 0.689990, grad mean: 105.499557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -383.1808166503906:  18%|█▊        | 37/200 [03:14<13:44,  5.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 36 - agent_0 alpha: 1.200010, grad mean: 56.048222\n",
            "Iter 36 - agent_0 lam: 1.500010, grad mean: 6.046434\n",
            "Iter 36 - agent_0 w_plus_prime_gamma: 0.500004, grad mean: 0.000002\n",
            "Iter 36 - agent_0 w_minus_prime_gamma: 0.689990, grad mean: 159.719406\n",
            "Iter 36 - agent_1 alpha: 1.200010, grad mean: 44.838566\n",
            "Iter 36 - agent_1 lam: 1.200010, grad mean: 6.046434\n",
            "Iter 36 - agent_1 w_plus_prime_gamma: 0.500004, grad mean: 0.000002\n",
            "Iter 36 - agent_1 w_minus_prime_gamma: 0.689990, grad mean: 127.775803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -255.61241149902344:  19%|█▉        | 38/200 [03:20<13:56,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 37 - agent_0 alpha: 1.200010, grad mean: 66.466476\n",
            "Iter 37 - agent_0 lam: 1.500010, grad mean: 7.191311\n",
            "Iter 37 - agent_0 w_plus_prime_gamma: 0.500004, grad mean: 0.000002\n",
            "Iter 37 - agent_0 w_minus_prime_gamma: 0.689990, grad mean: 189.458771\n",
            "Iter 37 - agent_1 alpha: 1.200010, grad mean: 53.173138\n",
            "Iter 37 - agent_1 lam: 1.200010, grad mean: 7.191311\n",
            "Iter 37 - agent_1 w_plus_prime_gamma: 0.500004, grad mean: 0.000002\n",
            "Iter 37 - agent_1 w_minus_prime_gamma: 0.689990, grad mean: 151.567307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -321.934326171875:  20%|█▉        | 39/200 [03:25<14:13,  5.30s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 38 - agent_0 alpha: 1.200010, grad mean: 77.066063\n",
            "Iter 38 - agent_0 lam: 1.500010, grad mean: 8.349368\n",
            "Iter 38 - agent_0 w_plus_prime_gamma: 0.500004, grad mean: 0.000002\n",
            "Iter 38 - agent_0 w_minus_prime_gamma: 0.689990, grad mean: 219.711151\n",
            "Iter 38 - agent_1 alpha: 1.200010, grad mean: 61.652855\n",
            "Iter 38 - agent_1 lam: 1.200010, grad mean: 8.349368\n",
            "Iter 38 - agent_1 w_plus_prime_gamma: 0.500004, grad mean: 0.000002\n",
            "Iter 38 - agent_1 w_minus_prime_gamma: 0.689990, grad mean: 175.769241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -314.48101806640625:  20%|██        | 40/200 [03:30<13:53,  5.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 39 - agent_0 alpha: 1.200010, grad mean: 87.610199\n",
            "Iter 39 - agent_0 lam: 1.500010, grad mean: 9.502342\n",
            "Iter 39 - agent_0 w_plus_prime_gamma: 0.500004, grad mean: 0.000003\n",
            "Iter 39 - agent_0 w_minus_prime_gamma: 0.689990, grad mean: 249.866989\n",
            "Iter 39 - agent_1 alpha: 1.200010, grad mean: 70.088165\n",
            "Iter 39 - agent_1 lam: 1.200010, grad mean: 9.502342\n",
            "Iter 39 - agent_1 w_plus_prime_gamma: 0.500004, grad mean: 0.000003\n",
            "Iter 39 - agent_1 w_minus_prime_gamma: 0.689990, grad mean: 199.893936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -319.93218994140625:  20%|██        | 41/200 [03:36<14:01,  5.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 40: Adaptive loss = 0.11997047066688538, dynamic factor = 4.227262496948242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -300.7047424316406:  21%|██        | 42/200 [03:41<13:32,  5.14s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 41 - agent_0 alpha: 1.200020, grad mean: 10.386022\n",
            "Iter 41 - agent_0 lam: 1.500020, grad mean: 1.149105\n",
            "Iter 41 - agent_0 w_plus_prime_gamma: 0.500008, grad mean: 0.000000\n",
            "Iter 41 - agent_0 w_minus_prime_gamma: 0.689980, grad mean: 29.850142\n",
            "Iter 41 - agent_1 alpha: 1.200020, grad mean: 8.308845\n",
            "Iter 41 - agent_1 lam: 1.200020, grad mean: 1.149105\n",
            "Iter 41 - agent_1 w_plus_prime_gamma: 0.500008, grad mean: 0.000000\n",
            "Iter 41 - agent_1 w_minus_prime_gamma: 0.689980, grad mean: 23.880194\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -238.47683715820312:  22%|██▏       | 43/200 [03:46<13:45,  5.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 42 - agent_0 alpha: 1.200020, grad mean: 20.490887\n",
            "Iter 42 - agent_0 lam: 1.500020, grad mean: 2.261329\n",
            "Iter 42 - agent_0 w_plus_prime_gamma: 0.500008, grad mean: 0.000001\n",
            "Iter 42 - agent_0 w_minus_prime_gamma: 0.689980, grad mean: 58.877087\n",
            "Iter 42 - agent_1 alpha: 1.200020, grad mean: 16.392754\n",
            "Iter 42 - agent_1 lam: 1.200020, grad mean: 2.261329\n",
            "Iter 42 - agent_1 w_plus_prime_gamma: 0.500008, grad mean: 0.000001\n",
            "Iter 42 - agent_1 w_minus_prime_gamma: 0.689980, grad mean: 47.101814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -288.0360412597656:  22%|██▏       | 44/200 [03:51<13:18,  5.12s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 43 - agent_0 alpha: 1.200020, grad mean: 30.500572\n",
            "Iter 43 - agent_0 lam: 1.500020, grad mean: 3.368545\n",
            "Iter 43 - agent_0 w_plus_prime_gamma: 0.500008, grad mean: 0.000001\n",
            "Iter 43 - agent_0 w_minus_prime_gamma: 0.689980, grad mean: 87.679420\n",
            "Iter 43 - agent_1 alpha: 1.200020, grad mean: 24.400524\n",
            "Iter 43 - agent_1 lam: 1.200020, grad mean: 3.368545\n",
            "Iter 43 - agent_1 w_plus_prime_gamma: 0.500008, grad mean: 0.000001\n",
            "Iter 43 - agent_1 w_minus_prime_gamma: 0.689980, grad mean: 70.143799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -279.054443359375:  22%|██▎       | 45/200 [03:56<13:27,  5.21s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 44 - agent_0 alpha: 1.200020, grad mean: 40.317997\n",
            "Iter 44 - agent_0 lam: 1.500020, grad mean: 4.449492\n",
            "Iter 44 - agent_0 w_plus_prime_gamma: 0.500008, grad mean: 0.000001\n",
            "Iter 44 - agent_0 w_minus_prime_gamma: 0.689980, grad mean: 115.967316\n",
            "Iter 44 - agent_1 alpha: 1.200020, grad mean: 32.254482\n",
            "Iter 44 - agent_1 lam: 1.200020, grad mean: 4.449492\n",
            "Iter 44 - agent_1 w_plus_prime_gamma: 0.500008, grad mean: 0.000001\n",
            "Iter 44 - agent_1 w_minus_prime_gamma: 0.689980, grad mean: 92.774162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -318.0399169921875:  23%|██▎       | 46/200 [04:01<13:07,  5.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 45 - agent_0 alpha: 1.200020, grad mean: 50.042191\n",
            "Iter 45 - agent_0 lam: 1.500020, grad mean: 5.524855\n",
            "Iter 45 - agent_0 w_plus_prime_gamma: 0.500008, grad mean: 0.000001\n",
            "Iter 45 - agent_0 w_minus_prime_gamma: 0.689980, grad mean: 144.002106\n",
            "Iter 45 - agent_1 alpha: 1.200020, grad mean: 40.033863\n",
            "Iter 45 - agent_1 lam: 1.200020, grad mean: 5.524855\n",
            "Iter 45 - agent_1 w_plus_prime_gamma: 0.500008, grad mean: 0.000001\n",
            "Iter 45 - agent_1 w_minus_prime_gamma: 0.689980, grad mean: 115.202011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -371.1559753417969:  24%|██▎       | 47/200 [04:06<12:49,  5.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 46 - agent_0 alpha: 1.200020, grad mean: 59.497189\n",
            "Iter 46 - agent_0 lam: 1.500020, grad mean: 6.566241\n",
            "Iter 46 - agent_0 w_plus_prime_gamma: 0.500008, grad mean: 0.000002\n",
            "Iter 46 - agent_0 w_minus_prime_gamma: 0.689980, grad mean: 171.324677\n",
            "Iter 46 - agent_1 alpha: 1.200020, grad mean: 47.597900\n",
            "Iter 46 - agent_1 lam: 1.200020, grad mean: 6.566241\n",
            "Iter 46 - agent_1 w_plus_prime_gamma: 0.500008, grad mean: 0.000002\n",
            "Iter 46 - agent_1 w_minus_prime_gamma: 0.689980, grad mean: 137.060028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -355.0579528808594:  24%|██▍       | 48/200 [04:11<12:58,  5.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 47 - agent_0 alpha: 1.200020, grad mean: 68.926735\n",
            "Iter 47 - agent_0 lam: 1.500020, grad mean: 7.605945\n",
            "Iter 47 - agent_0 w_plus_prime_gamma: 0.500008, grad mean: 0.000002\n",
            "Iter 47 - agent_0 w_minus_prime_gamma: 0.689980, grad mean: 198.520905\n",
            "Iter 47 - agent_1 alpha: 1.200020, grad mean: 55.141560\n",
            "Iter 47 - agent_1 lam: 1.200020, grad mean: 7.605945\n",
            "Iter 47 - agent_1 w_plus_prime_gamma: 0.500008, grad mean: 0.000002\n",
            "Iter 47 - agent_1 w_minus_prime_gamma: 0.689980, grad mean: 158.816956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -374.4197692871094:  24%|██▍       | 49/200 [04:16<12:38,  5.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 48 - agent_0 alpha: 1.200020, grad mean: 78.045967\n",
            "Iter 48 - agent_0 lam: 1.500020, grad mean: 8.611871\n",
            "Iter 48 - agent_0 w_plus_prime_gamma: 0.500008, grad mean: 0.000002\n",
            "Iter 48 - agent_0 w_minus_prime_gamma: 0.689980, grad mean: 224.899231\n",
            "Iter 48 - agent_1 alpha: 1.200020, grad mean: 62.436932\n",
            "Iter 48 - agent_1 lam: 1.200020, grad mean: 8.611871\n",
            "Iter 48 - agent_1 w_plus_prime_gamma: 0.500008, grad mean: 0.000002\n",
            "Iter 48 - agent_1 w_minus_prime_gamma: 0.689980, grad mean: 179.919678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -337.7549743652344:  25%|██▌       | 50/200 [04:22<12:54,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 49 - agent_0 alpha: 1.200020, grad mean: 86.743652\n",
            "Iter 49 - agent_0 lam: 1.500020, grad mean: 9.569551\n",
            "Iter 49 - agent_0 w_plus_prime_gamma: 0.500008, grad mean: 0.000003\n",
            "Iter 49 - agent_0 w_minus_prime_gamma: 0.689980, grad mean: 250.136337\n",
            "Iter 49 - agent_1 alpha: 1.200020, grad mean: 69.395073\n",
            "Iter 49 - agent_1 lam: 1.200020, grad mean: 9.569551\n",
            "Iter 49 - agent_1 w_plus_prime_gamma: 0.500008, grad mean: 0.000003\n",
            "Iter 49 - agent_1 w_minus_prime_gamma: 0.689980, grad mean: 200.109528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -355.4058837890625:  26%|██▌       | 51/200 [04:27<12:36,  5.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 50: Adaptive loss = 0.24668258428573608, dynamic factor = 7.1450042724609375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -448.6462097167969:  26%|██▌       | 52/200 [04:32<12:35,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 51 - agent_0 alpha: 1.200030, grad mean: 8.515456\n",
            "Iter 51 - agent_0 lam: 1.500030, grad mean: 0.931488\n",
            "Iter 51 - agent_0 w_plus_prime_gamma: 0.500013, grad mean: 0.000000\n",
            "Iter 51 - agent_0 w_minus_prime_gamma: 0.689970, grad mean: 24.697744\n",
            "Iter 51 - agent_1 alpha: 1.200030, grad mean: 6.812396\n",
            "Iter 51 - agent_1 lam: 1.200030, grad mean: 0.931488\n",
            "Iter 51 - agent_1 w_plus_prime_gamma: 0.500013, grad mean: 0.000000\n",
            "Iter 51 - agent_1 w_minus_prime_gamma: 0.689970, grad mean: 19.758291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -272.0198059082031:  26%|██▋       | 53/200 [04:37<12:35,  5.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 52 - agent_0 alpha: 1.200030, grad mean: 16.932806\n",
            "Iter 52 - agent_0 lam: 1.500030, grad mean: 1.863405\n",
            "Iter 52 - agent_0 w_plus_prime_gamma: 0.500013, grad mean: 0.000000\n",
            "Iter 52 - agent_0 w_minus_prime_gamma: 0.689970, grad mean: 49.045643\n",
            "Iter 52 - agent_1 alpha: 1.200030, grad mean: 13.546307\n",
            "Iter 52 - agent_1 lam: 1.200030, grad mean: 1.863405\n",
            "Iter 52 - agent_1 w_plus_prime_gamma: 0.500013, grad mean: 0.000000\n",
            "Iter 52 - agent_1 w_minus_prime_gamma: 0.689970, grad mean: 39.236660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -317.9395751953125:  27%|██▋       | 54/200 [04:42<12:20,  5.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 53 - agent_0 alpha: 1.200030, grad mean: 24.881948\n",
            "Iter 53 - agent_0 lam: 1.500030, grad mean: 2.733007\n",
            "Iter 53 - agent_0 w_plus_prime_gamma: 0.500013, grad mean: 0.000001\n",
            "Iter 53 - agent_0 w_minus_prime_gamma: 0.689970, grad mean: 72.190193\n",
            "Iter 53 - agent_1 alpha: 1.200030, grad mean: 19.905655\n",
            "Iter 53 - agent_1 lam: 1.200030, grad mean: 2.733007\n",
            "Iter 53 - agent_1 w_plus_prime_gamma: 0.500013, grad mean: 0.000001\n",
            "Iter 53 - agent_1 w_minus_prime_gamma: 0.689970, grad mean: 57.752392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -312.3946228027344:  28%|██▊       | 55/200 [04:47<12:36,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 54 - agent_0 alpha: 1.200030, grad mean: 32.965450\n",
            "Iter 54 - agent_0 lam: 1.500030, grad mean: 3.623982\n",
            "Iter 54 - agent_0 w_plus_prime_gamma: 0.500013, grad mean: 0.000001\n",
            "Iter 54 - agent_0 w_minus_prime_gamma: 0.689970, grad mean: 95.635643\n",
            "Iter 54 - agent_1 alpha: 1.200030, grad mean: 26.372494\n",
            "Iter 54 - agent_1 lam: 1.200030, grad mean: 3.623982\n",
            "Iter 54 - agent_1 w_plus_prime_gamma: 0.500013, grad mean: 0.000001\n",
            "Iter 54 - agent_1 w_minus_prime_gamma: 0.689970, grad mean: 76.508820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -357.883056640625:  28%|██▊       | 56/200 [04:52<12:10,  5.07s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 55 - agent_0 alpha: 1.200030, grad mean: 40.807468\n",
            "Iter 55 - agent_0 lam: 1.500030, grad mean: 4.488925\n",
            "Iter 55 - agent_0 w_plus_prime_gamma: 0.500013, grad mean: 0.000001\n",
            "Iter 55 - agent_0 w_minus_prime_gamma: 0.689970, grad mean: 118.436722\n",
            "Iter 55 - agent_1 alpha: 1.200030, grad mean: 32.646122\n",
            "Iter 55 - agent_1 lam: 1.200030, grad mean: 4.488925\n",
            "Iter 55 - agent_1 w_plus_prime_gamma: 0.500013, grad mean: 0.000001\n",
            "Iter 55 - agent_1 w_minus_prime_gamma: 0.689970, grad mean: 94.749771\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -337.53857421875:  28%|██▊       | 57/200 [04:57<12:18,  5.16s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 56 - agent_0 alpha: 1.200030, grad mean: 48.618286\n",
            "Iter 56 - agent_0 lam: 1.500030, grad mean: 5.352906\n",
            "Iter 56 - agent_0 w_plus_prime_gamma: 0.500013, grad mean: 0.000001\n",
            "Iter 56 - agent_0 w_minus_prime_gamma: 0.689970, grad mean: 141.241959\n",
            "Iter 56 - agent_1 alpha: 1.200030, grad mean: 38.894810\n",
            "Iter 56 - agent_1 lam: 1.200030, grad mean: 5.352906\n",
            "Iter 56 - agent_1 w_plus_prime_gamma: 0.500013, grad mean: 0.000001\n",
            "Iter 56 - agent_1 w_minus_prime_gamma: 0.689970, grad mean: 112.994011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -318.4130554199219:  29%|██▉       | 58/200 [05:02<11:59,  5.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 57 - agent_0 alpha: 1.200030, grad mean: 56.291779\n",
            "Iter 57 - agent_0 lam: 1.500030, grad mean: 6.198269\n",
            "Iter 57 - agent_0 w_plus_prime_gamma: 0.500013, grad mean: 0.000002\n",
            "Iter 57 - agent_0 w_minus_prime_gamma: 0.689970, grad mean: 163.587112\n",
            "Iter 57 - agent_1 alpha: 1.200030, grad mean: 45.033627\n",
            "Iter 57 - agent_1 lam: 1.200030, grad mean: 6.198269\n",
            "Iter 57 - agent_1 w_plus_prime_gamma: 0.500013, grad mean: 0.000002\n",
            "Iter 57 - agent_1 w_minus_prime_gamma: 0.689970, grad mean: 130.870209\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -303.0152893066406:  30%|██▉       | 59/200 [05:07<11:53,  5.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 58 - agent_0 alpha: 1.200030, grad mean: 63.793659\n",
            "Iter 58 - agent_0 lam: 1.500030, grad mean: 7.028088\n",
            "Iter 58 - agent_0 w_plus_prime_gamma: 0.500013, grad mean: 0.000002\n",
            "Iter 58 - agent_0 w_minus_prime_gamma: 0.689970, grad mean: 185.501877\n",
            "Iter 58 - agent_1 alpha: 1.200030, grad mean: 51.035164\n",
            "Iter 58 - agent_1 lam: 1.200030, grad mean: 7.028088\n",
            "Iter 58 - agent_1 w_plus_prime_gamma: 0.500013, grad mean: 0.000002\n",
            "Iter 58 - agent_1 w_minus_prime_gamma: 0.689970, grad mean: 148.402100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -415.1454772949219:  30%|███       | 60/200 [05:13<11:58,  5.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 59 - agent_0 alpha: 1.200030, grad mean: 71.297203\n",
            "Iter 59 - agent_0 lam: 1.500030, grad mean: 7.855323\n",
            "Iter 59 - agent_0 w_plus_prime_gamma: 0.500013, grad mean: 0.000002\n",
            "Iter 59 - agent_0 w_minus_prime_gamma: 0.689970, grad mean: 207.355560\n",
            "Iter 59 - agent_1 alpha: 1.200030, grad mean: 57.038044\n",
            "Iter 59 - agent_1 lam: 1.200030, grad mean: 7.855323\n",
            "Iter 59 - agent_1 w_plus_prime_gamma: 0.500013, grad mean: 0.000002\n",
            "Iter 59 - agent_1 w_minus_prime_gamma: 0.689970, grad mean: 165.885010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -420.90869140625:  30%|███       | 61/200 [05:18<11:42,  5.06s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 60: Adaptive loss = 0.1401955783367157, dynamic factor = 4.521968841552734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -369.1796569824219:  31%|███       | 62/200 [05:23<11:55,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 61 - agent_0 alpha: 1.200040, grad mean: 7.230236\n",
            "Iter 61 - agent_0 lam: 1.500040, grad mean: 0.800629\n",
            "Iter 61 - agent_0 w_plus_prime_gamma: 0.500017, grad mean: 0.000000\n",
            "Iter 61 - agent_0 w_minus_prime_gamma: 0.689960, grad mean: 21.130188\n",
            "Iter 61 - agent_1 alpha: 1.200040, grad mean: 5.784224\n",
            "Iter 61 - agent_1 lam: 1.200040, grad mean: 0.800629\n",
            "Iter 61 - agent_1 w_plus_prime_gamma: 0.500017, grad mean: 0.000000\n",
            "Iter 61 - agent_1 w_minus_prime_gamma: 0.689960, grad mean: 16.904268\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -477.0048522949219:  32%|███▏      | 63/200 [05:28<11:38,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 62 - agent_0 alpha: 1.200040, grad mean: 14.286224\n",
            "Iter 62 - agent_0 lam: 1.500040, grad mean: 1.577875\n",
            "Iter 62 - agent_0 w_plus_prime_gamma: 0.500017, grad mean: 0.000000\n",
            "Iter 62 - agent_0 w_minus_prime_gamma: 0.689960, grad mean: 41.771271\n",
            "Iter 62 - agent_1 alpha: 1.200040, grad mean: 11.429055\n",
            "Iter 62 - agent_1 lam: 1.200040, grad mean: 1.577875\n",
            "Iter 62 - agent_1 w_plus_prime_gamma: 0.500017, grad mean: 0.000000\n",
            "Iter 62 - agent_1 w_minus_prime_gamma: 0.689960, grad mean: 33.417271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -352.9727783203125:  32%|███▏      | 64/200 [05:33<11:45,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 63 - agent_0 alpha: 1.200040, grad mean: 21.538687\n",
            "Iter 63 - agent_0 lam: 1.500040, grad mean: 2.381887\n",
            "Iter 63 - agent_0 w_plus_prime_gamma: 0.500017, grad mean: 0.000001\n",
            "Iter 63 - agent_0 w_minus_prime_gamma: 0.689960, grad mean: 62.947418\n",
            "Iter 63 - agent_1 alpha: 1.200040, grad mean: 17.231071\n",
            "Iter 63 - agent_1 lam: 1.200040, grad mean: 2.381887\n",
            "Iter 63 - agent_1 w_plus_prime_gamma: 0.500017, grad mean: 0.000001\n",
            "Iter 63 - agent_1 w_minus_prime_gamma: 0.689960, grad mean: 50.358311\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -380.178466796875:  32%|███▎      | 65/200 [05:38<11:31,  5.13s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 64 - agent_0 alpha: 1.200040, grad mean: 28.579020\n",
            "Iter 64 - agent_0 lam: 1.500040, grad mean: 3.155495\n",
            "Iter 64 - agent_0 w_plus_prime_gamma: 0.500017, grad mean: 0.000001\n",
            "Iter 64 - agent_0 w_minus_prime_gamma: 0.689960, grad mean: 83.549347\n",
            "Iter 64 - agent_1 alpha: 1.200040, grad mean: 22.863369\n",
            "Iter 64 - agent_1 lam: 1.200040, grad mean: 3.155495\n",
            "Iter 64 - agent_1 w_plus_prime_gamma: 0.500017, grad mean: 0.000001\n",
            "Iter 64 - agent_1 w_minus_prime_gamma: 0.689960, grad mean: 66.839928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -424.1358337402344:  33%|███▎      | 66/200 [05:43<11:27,  5.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 65 - agent_0 alpha: 1.200040, grad mean: 35.192211\n",
            "Iter 65 - agent_0 lam: 1.500040, grad mean: 3.885060\n",
            "Iter 65 - agent_0 w_plus_prime_gamma: 0.500017, grad mean: 0.000001\n",
            "Iter 65 - agent_0 w_minus_prime_gamma: 0.689960, grad mean: 102.944466\n",
            "Iter 65 - agent_1 alpha: 1.200040, grad mean: 28.153973\n",
            "Iter 65 - agent_1 lam: 1.200040, grad mean: 3.885060\n",
            "Iter 65 - agent_1 w_plus_prime_gamma: 0.500017, grad mean: 0.000001\n",
            "Iter 65 - agent_1 w_minus_prime_gamma: 0.689960, grad mean: 82.356163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -630.3722534179688:  34%|███▎      | 67/200 [05:49<11:36,  5.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 66 - agent_0 alpha: 1.200040, grad mean: 41.901543\n",
            "Iter 66 - agent_0 lam: 1.500040, grad mean: 4.617319\n",
            "Iter 66 - agent_0 w_plus_prime_gamma: 0.500017, grad mean: 0.000001\n",
            "Iter 66 - agent_0 w_minus_prime_gamma: 0.689960, grad mean: 122.557457\n",
            "Iter 66 - agent_1 alpha: 1.200040, grad mean: 33.521477\n",
            "Iter 66 - agent_1 lam: 1.200040, grad mean: 4.617319\n",
            "Iter 66 - agent_1 w_plus_prime_gamma: 0.500017, grad mean: 0.000001\n",
            "Iter 66 - agent_1 w_minus_prime_gamma: 0.689960, grad mean: 98.046684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -378.7799987792969:  34%|███▍      | 68/200 [05:54<11:16,  5.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 67 - agent_0 alpha: 1.200040, grad mean: 48.574947\n",
            "Iter 67 - agent_0 lam: 1.500040, grad mean: 5.349730\n",
            "Iter 67 - agent_0 w_plus_prime_gamma: 0.500017, grad mean: 0.000002\n",
            "Iter 67 - agent_0 w_minus_prime_gamma: 0.689960, grad mean: 142.080902\n",
            "Iter 67 - agent_1 alpha: 1.200040, grad mean: 38.860245\n",
            "Iter 67 - agent_1 lam: 1.200040, grad mean: 5.349730\n",
            "Iter 67 - agent_1 w_plus_prime_gamma: 0.500017, grad mean: 0.000002\n",
            "Iter 67 - agent_1 w_minus_prime_gamma: 0.689960, grad mean: 113.665512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -507.8124084472656:  34%|███▍      | 69/200 [05:59<11:24,  5.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 68 - agent_0 alpha: 1.200040, grad mean: 55.214214\n",
            "Iter 68 - agent_0 lam: 1.500040, grad mean: 6.073960\n",
            "Iter 68 - agent_0 w_plus_prime_gamma: 0.500017, grad mean: 0.000002\n",
            "Iter 68 - agent_0 w_minus_prime_gamma: 0.689960, grad mean: 161.449600\n",
            "Iter 68 - agent_1 alpha: 1.200040, grad mean: 44.171703\n",
            "Iter 68 - agent_1 lam: 1.200040, grad mean: 6.073960\n",
            "Iter 68 - agent_1 w_plus_prime_gamma: 0.500017, grad mean: 0.000002\n",
            "Iter 68 - agent_1 w_minus_prime_gamma: 0.689960, grad mean: 129.160645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -586.3829956054688:  35%|███▌      | 70/200 [06:04<11:02,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 69 - agent_0 alpha: 1.200040, grad mean: 61.961433\n",
            "Iter 69 - agent_0 lam: 1.500040, grad mean: 6.814203\n",
            "Iter 69 - agent_0 w_plus_prime_gamma: 0.500017, grad mean: 0.000002\n",
            "Iter 69 - agent_0 w_minus_prime_gamma: 0.689960, grad mean: 181.175171\n",
            "Iter 69 - agent_1 alpha: 1.200040, grad mean: 49.569534\n",
            "Iter 69 - agent_1 lam: 1.200040, grad mean: 6.814203\n",
            "Iter 69 - agent_1 w_plus_prime_gamma: 0.500017, grad mean: 0.000002\n",
            "Iter 69 - agent_1 w_minus_prime_gamma: 0.689960, grad mean: 144.941223\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -447.7021179199219:  36%|███▌      | 71/200 [06:09<11:04,  5.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 70: Adaptive loss = 0.11009025573730469, dynamic factor = 3.920167922973633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -326.92181396484375:  36%|███▌      | 72/200 [06:14<10:53,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 71 - agent_0 alpha: 1.200050, grad mean: 6.362128\n",
            "Iter 71 - agent_0 lam: 1.500050, grad mean: 0.698711\n",
            "Iter 71 - agent_0 w_plus_prime_gamma: 0.500021, grad mean: 0.000000\n",
            "Iter 71 - agent_0 w_minus_prime_gamma: 0.689950, grad mean: 18.608772\n",
            "Iter 71 - agent_1 alpha: 1.200050, grad mean: 5.089746\n",
            "Iter 71 - agent_1 lam: 1.200050, grad mean: 0.698711\n",
            "Iter 71 - agent_1 w_plus_prime_gamma: 0.500021, grad mean: 0.000000\n",
            "Iter 71 - agent_1 w_minus_prime_gamma: 0.689950, grad mean: 14.887146\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -469.6686706542969:  36%|███▋      | 73/200 [06:19<10:40,  5.04s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 72 - agent_0 alpha: 1.200050, grad mean: 12.841614\n",
            "Iter 72 - agent_0 lam: 1.500050, grad mean: 1.408205\n",
            "Iter 72 - agent_0 w_plus_prime_gamma: 0.500021, grad mean: 0.000000\n",
            "Iter 72 - agent_0 w_minus_prime_gamma: 0.689950, grad mean: 37.518902\n",
            "Iter 72 - agent_1 alpha: 1.200050, grad mean: 10.273374\n",
            "Iter 72 - agent_1 lam: 1.200050, grad mean: 1.408205\n",
            "Iter 72 - agent_1 w_plus_prime_gamma: 0.500021, grad mean: 0.000000\n",
            "Iter 72 - agent_1 w_minus_prime_gamma: 0.689950, grad mean: 30.015371\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -359.318359375:  37%|███▋      | 74/200 [06:25<10:54,  5.19s/it]    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 73 - agent_0 alpha: 1.200050, grad mean: 19.161951\n",
            "Iter 73 - agent_0 lam: 1.500050, grad mean: 2.094408\n",
            "Iter 73 - agent_0 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 73 - agent_0 w_minus_prime_gamma: 0.689950, grad mean: 55.980244\n",
            "Iter 73 - agent_1 alpha: 1.200050, grad mean: 15.329679\n",
            "Iter 73 - agent_1 lam: 1.200050, grad mean: 2.094408\n",
            "Iter 73 - agent_1 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 73 - agent_1 w_minus_prime_gamma: 0.689950, grad mean: 44.784538\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -447.9784851074219:  38%|███▊      | 75/200 [06:30<10:35,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 74 - agent_0 alpha: 1.200050, grad mean: 25.475798\n",
            "Iter 74 - agent_0 lam: 1.500050, grad mean: 2.789035\n",
            "Iter 74 - agent_0 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 74 - agent_0 w_minus_prime_gamma: 0.689950, grad mean: 74.460030\n",
            "Iter 74 - agent_1 alpha: 1.200050, grad mean: 20.380808\n",
            "Iter 74 - agent_1 lam: 1.200050, grad mean: 2.789035\n",
            "Iter 74 - agent_1 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 74 - agent_1 w_minus_prime_gamma: 0.689950, grad mean: 59.568485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -417.88580322265625:  38%|███▊      | 76/200 [06:35<10:46,  5.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 75 - agent_0 alpha: 1.200050, grad mean: 31.997332\n",
            "Iter 75 - agent_0 lam: 1.500050, grad mean: 3.501812\n",
            "Iter 75 - agent_0 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 75 - agent_0 w_minus_prime_gamma: 0.689950, grad mean: 93.486740\n",
            "Iter 75 - agent_1 alpha: 1.200050, grad mean: 25.598082\n",
            "Iter 75 - agent_1 lam: 1.200050, grad mean: 3.501812\n",
            "Iter 75 - agent_1 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 75 - agent_1 w_minus_prime_gamma: 0.689950, grad mean: 74.790016\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -466.9668884277344:  38%|███▊      | 77/200 [06:40<10:24,  5.08s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 76 - agent_0 alpha: 1.200050, grad mean: 38.538662\n",
            "Iter 76 - agent_0 lam: 1.500050, grad mean: 4.219863\n",
            "Iter 76 - agent_0 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 76 - agent_0 w_minus_prime_gamma: 0.689950, grad mean: 112.556763\n",
            "Iter 76 - agent_1 alpha: 1.200050, grad mean: 30.831190\n",
            "Iter 76 - agent_1 lam: 1.200050, grad mean: 4.219863\n",
            "Iter 76 - agent_1 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 76 - agent_1 w_minus_prime_gamma: 0.689950, grad mean: 90.046143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -408.792236328125:  39%|███▉      | 78/200 [06:45<10:21,  5.10s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 77 - agent_0 alpha: 1.200050, grad mean: 44.645416\n",
            "Iter 77 - agent_0 lam: 1.500050, grad mean: 4.884451\n",
            "Iter 77 - agent_0 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 77 - agent_0 w_minus_prime_gamma: 0.689950, grad mean: 130.486740\n",
            "Iter 77 - agent_1 alpha: 1.200050, grad mean: 35.716660\n",
            "Iter 77 - agent_1 lam: 1.200050, grad mean: 4.884451\n",
            "Iter 77 - agent_1 w_plus_prime_gamma: 0.500021, grad mean: 0.000001\n",
            "Iter 77 - agent_1 w_minus_prime_gamma: 0.689950, grad mean: 104.390259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -512.6492919921875:  40%|███▉      | 79/200 [06:50<10:19,  5.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 78 - agent_0 alpha: 1.200050, grad mean: 50.979336\n",
            "Iter 78 - agent_0 lam: 1.500050, grad mean: 5.584033\n",
            "Iter 78 - agent_0 w_plus_prime_gamma: 0.500021, grad mean: 0.000002\n",
            "Iter 78 - agent_0 w_minus_prime_gamma: 0.689950, grad mean: 148.882263\n",
            "Iter 78 - agent_1 alpha: 1.200050, grad mean: 40.783855\n",
            "Iter 78 - agent_1 lam: 1.200050, grad mean: 5.584033\n",
            "Iter 78 - agent_1 w_plus_prime_gamma: 0.500021, grad mean: 0.000002\n",
            "Iter 78 - agent_1 w_minus_prime_gamma: 0.689950, grad mean: 119.106857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -460.3768615722656:  40%|████      | 80/200 [06:55<10:07,  5.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 79 - agent_0 alpha: 1.200050, grad mean: 57.116764\n",
            "Iter 79 - agent_0 lam: 1.500050, grad mean: 6.250177\n",
            "Iter 79 - agent_0 w_plus_prime_gamma: 0.500021, grad mean: 0.000002\n",
            "Iter 79 - agent_0 w_minus_prime_gamma: 0.689950, grad mean: 166.856018\n",
            "Iter 79 - agent_1 alpha: 1.200050, grad mean: 45.693821\n",
            "Iter 79 - agent_1 lam: 1.200050, grad mean: 6.250177\n",
            "Iter 79 - agent_1 w_plus_prime_gamma: 0.500021, grad mean: 0.000002\n",
            "Iter 79 - agent_1 w_minus_prime_gamma: 0.689950, grad mean: 133.486053\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -424.61846923828125:  40%|████      | 81/200 [07:01<10:14,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 80: Adaptive loss = 0.12108708918094635, dynamic factor = 5.035272598266602\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -418.48297119140625:  41%|████      | 82/200 [07:05<09:56,  5.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 81 - agent_0 alpha: 1.200060, grad mean: 5.935126\n",
            "Iter 81 - agent_0 lam: 1.500060, grad mean: 0.644738\n",
            "Iter 81 - agent_0 w_plus_prime_gamma: 0.500026, grad mean: 0.000000\n",
            "Iter 81 - agent_0 w_minus_prime_gamma: 0.689940, grad mean: 17.344540\n",
            "Iter 81 - agent_1 alpha: 1.200060, grad mean: 4.748147\n",
            "Iter 81 - agent_1 lam: 1.200060, grad mean: 0.644738\n",
            "Iter 81 - agent_1 w_plus_prime_gamma: 0.500026, grad mean: 0.000000\n",
            "Iter 81 - agent_1 w_minus_prime_gamma: 0.689940, grad mean: 13.875780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -401.0982360839844:  42%|████▏     | 83/200 [07:11<10:08,  5.20s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 82 - agent_0 alpha: 1.200060, grad mean: 11.805928\n",
            "Iter 82 - agent_0 lam: 1.500060, grad mean: 1.285511\n",
            "Iter 82 - agent_0 w_plus_prime_gamma: 0.500026, grad mean: 0.000000\n",
            "Iter 82 - agent_0 w_minus_prime_gamma: 0.689940, grad mean: 34.511974\n",
            "Iter 82 - agent_1 alpha: 1.200060, grad mean: 9.444835\n",
            "Iter 82 - agent_1 lam: 1.200060, grad mean: 1.285511\n",
            "Iter 82 - agent_1 w_plus_prime_gamma: 0.500026, grad mean: 0.000000\n",
            "Iter 82 - agent_1 w_minus_prime_gamma: 0.689940, grad mean: 27.609859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -412.62017822265625:  42%|████▏     | 84/200 [07:16<09:52,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 83 - agent_0 alpha: 1.200060, grad mean: 17.617725\n",
            "Iter 83 - agent_0 lam: 1.500060, grad mean: 1.919481\n",
            "Iter 83 - agent_0 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 83 - agent_0 w_minus_prime_gamma: 0.689940, grad mean: 51.500988\n",
            "Iter 83 - agent_1 alpha: 1.200060, grad mean: 14.094324\n",
            "Iter 83 - agent_1 lam: 1.200060, grad mean: 1.919481\n",
            "Iter 83 - agent_1 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 83 - agent_1 w_minus_prime_gamma: 0.689940, grad mean: 41.201214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -410.1794128417969:  42%|████▎     | 85/200 [07:21<09:46,  5.10s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 84 - agent_0 alpha: 1.200060, grad mean: 23.389612\n",
            "Iter 84 - agent_0 lam: 1.500060, grad mean: 2.550033\n",
            "Iter 84 - agent_0 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 84 - agent_0 w_minus_prime_gamma: 0.689940, grad mean: 68.369652\n",
            "Iter 84 - agent_1 alpha: 1.200060, grad mean: 18.711891\n",
            "Iter 84 - agent_1 lam: 1.200060, grad mean: 2.550033\n",
            "Iter 84 - agent_1 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 84 - agent_1 w_minus_prime_gamma: 0.689940, grad mean: 54.696281\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -442.6257019042969:  43%|████▎     | 86/200 [07:26<09:46,  5.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 85 - agent_0 alpha: 1.200060, grad mean: 29.083750\n",
            "Iter 85 - agent_0 lam: 1.500060, grad mean: 3.171283\n",
            "Iter 85 - agent_0 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 85 - agent_0 w_minus_prime_gamma: 0.689940, grad mean: 84.977539\n",
            "Iter 85 - agent_1 alpha: 1.200060, grad mean: 23.267254\n",
            "Iter 85 - agent_1 lam: 1.200060, grad mean: 3.171283\n",
            "Iter 85 - agent_1 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 85 - agent_1 w_minus_prime_gamma: 0.689940, grad mean: 67.982704\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -470.2922058105469:  44%|████▎     | 87/200 [07:32<09:54,  5.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 86 - agent_0 alpha: 1.200060, grad mean: 34.698456\n",
            "Iter 86 - agent_0 lam: 1.500060, grad mean: 3.782798\n",
            "Iter 86 - agent_0 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 86 - agent_0 w_minus_prime_gamma: 0.689940, grad mean: 101.454643\n",
            "Iter 86 - agent_1 alpha: 1.200060, grad mean: 27.759056\n",
            "Iter 86 - agent_1 lam: 1.200060, grad mean: 3.782798\n",
            "Iter 86 - agent_1 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 86 - agent_1 w_minus_prime_gamma: 0.689940, grad mean: 81.164566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -551.0076904296875:  44%|████▍     | 88/200 [07:37<09:54,  5.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 87 - agent_0 alpha: 1.200060, grad mean: 40.317486\n",
            "Iter 87 - agent_0 lam: 1.500060, grad mean: 4.400458\n",
            "Iter 87 - agent_0 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 87 - agent_0 w_minus_prime_gamma: 0.689940, grad mean: 117.862549\n",
            "Iter 87 - agent_1 alpha: 1.200060, grad mean: 32.254333\n",
            "Iter 87 - agent_1 lam: 1.200060, grad mean: 4.400458\n",
            "Iter 87 - agent_1 w_plus_prime_gamma: 0.500026, grad mean: 0.000001\n",
            "Iter 87 - agent_1 w_minus_prime_gamma: 0.689940, grad mean: 94.291039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -417.42901611328125:  44%|████▍     | 89/200 [07:42<09:36,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 88 - agent_0 alpha: 1.200060, grad mean: 45.994343\n",
            "Iter 88 - agent_0 lam: 1.500060, grad mean: 5.017316\n",
            "Iter 88 - agent_0 w_plus_prime_gamma: 0.500026, grad mean: 0.000002\n",
            "Iter 88 - agent_0 w_minus_prime_gamma: 0.689940, grad mean: 134.465057\n",
            "Iter 88 - agent_1 alpha: 1.200060, grad mean: 36.795856\n",
            "Iter 88 - agent_1 lam: 1.200060, grad mean: 5.017316\n",
            "Iter 88 - agent_1 w_plus_prime_gamma: 0.500026, grad mean: 0.000002\n",
            "Iter 88 - agent_1 w_minus_prime_gamma: 0.689940, grad mean: 107.573250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -463.2416687011719:  45%|████▌     | 90/200 [07:47<09:39,  5.27s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 89 - agent_0 alpha: 1.200060, grad mean: 51.556026\n",
            "Iter 89 - agent_0 lam: 1.500060, grad mean: 5.620864\n",
            "Iter 89 - agent_0 w_plus_prime_gamma: 0.500026, grad mean: 0.000002\n",
            "Iter 89 - agent_0 w_minus_prime_gamma: 0.689940, grad mean: 150.733231\n",
            "Iter 89 - agent_1 alpha: 1.200060, grad mean: 41.245239\n",
            "Iter 89 - agent_1 lam: 1.200060, grad mean: 5.620864\n",
            "Iter 89 - agent_1 w_plus_prime_gamma: 0.500026, grad mean: 0.000002\n",
            "Iter 89 - agent_1 w_minus_prime_gamma: 0.689940, grad mean: 120.587868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -529.0089721679688:  46%|████▌     | 91/200 [07:52<09:24,  5.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 90: Adaptive loss = 0.0380854532122612, dynamic factor = 1.6260757446289062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -577.2557983398438:  46%|████▌     | 92/200 [07:58<09:21,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 91 - agent_0 alpha: 1.200070, grad mean: 5.452456\n",
            "Iter 91 - agent_0 lam: 1.500070, grad mean: 0.594885\n",
            "Iter 91 - agent_0 w_plus_prime_gamma: 0.500030, grad mean: 0.000000\n",
            "Iter 91 - agent_0 w_minus_prime_gamma: 0.689930, grad mean: 15.884701\n",
            "Iter 91 - agent_1 alpha: 1.200070, grad mean: 4.362017\n",
            "Iter 91 - agent_1 lam: 1.200070, grad mean: 0.594885\n",
            "Iter 91 - agent_1 w_plus_prime_gamma: 0.500030, grad mean: 0.000000\n",
            "Iter 91 - agent_1 w_minus_prime_gamma: 0.689930, grad mean: 12.707907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -517.7755126953125:  46%|████▋     | 93/200 [08:03<09:13,  5.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 92 - agent_0 alpha: 1.200070, grad mean: 10.734095\n",
            "Iter 92 - agent_0 lam: 1.500070, grad mean: 1.168421\n",
            "Iter 92 - agent_0 w_plus_prime_gamma: 0.500030, grad mean: 0.000000\n",
            "Iter 92 - agent_0 w_minus_prime_gamma: 0.689930, grad mean: 31.320860\n",
            "Iter 92 - agent_1 alpha: 1.200070, grad mean: 8.587378\n",
            "Iter 92 - agent_1 lam: 1.200070, grad mean: 1.168421\n",
            "Iter 92 - agent_1 w_plus_prime_gamma: 0.500030, grad mean: 0.000000\n",
            "Iter 92 - agent_1 w_minus_prime_gamma: 0.689930, grad mean: 25.056984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -550.9685668945312:  47%|████▋     | 94/200 [08:08<08:59,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 93 - agent_0 alpha: 1.200070, grad mean: 16.114656\n",
            "Iter 93 - agent_0 lam: 1.500070, grad mean: 1.756913\n",
            "Iter 93 - agent_0 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 93 - agent_0 w_minus_prime_gamma: 0.689930, grad mean: 46.977329\n",
            "Iter 93 - agent_1 alpha: 1.200070, grad mean: 12.891881\n",
            "Iter 93 - agent_1 lam: 1.200070, grad mean: 1.756913\n",
            "Iter 93 - agent_1 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 93 - agent_1 w_minus_prime_gamma: 0.689930, grad mean: 37.582314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -412.01104736328125:  48%|████▊     | 95/200 [08:13<09:07,  5.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 94 - agent_0 alpha: 1.200070, grad mean: 21.296528\n",
            "Iter 94 - agent_0 lam: 1.500070, grad mean: 2.316514\n",
            "Iter 94 - agent_0 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 94 - agent_0 w_minus_prime_gamma: 0.689930, grad mean: 62.096981\n",
            "Iter 94 - agent_1 alpha: 1.200070, grad mean: 17.037432\n",
            "Iter 94 - agent_1 lam: 1.200070, grad mean: 2.316514\n",
            "Iter 94 - agent_1 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 94 - agent_1 w_minus_prime_gamma: 0.689930, grad mean: 49.678177\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -515.1778564453125:  48%|████▊     | 96/200 [08:18<08:50,  5.11s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 95 - agent_0 alpha: 1.200070, grad mean: 26.480892\n",
            "Iter 95 - agent_0 lam: 1.500070, grad mean: 2.882269\n",
            "Iter 95 - agent_0 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 95 - agent_0 w_minus_prime_gamma: 0.689930, grad mean: 77.229347\n",
            "Iter 95 - agent_1 alpha: 1.200070, grad mean: 21.184969\n",
            "Iter 95 - agent_1 lam: 1.200070, grad mean: 2.882269\n",
            "Iter 95 - agent_1 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 95 - agent_1 w_minus_prime_gamma: 0.689930, grad mean: 61.784180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -566.5494384765625:  48%|████▊     | 97/200 [08:23<08:55,  5.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 96 - agent_0 alpha: 1.200070, grad mean: 31.468948\n",
            "Iter 96 - agent_0 lam: 1.500070, grad mean: 3.425303\n",
            "Iter 96 - agent_0 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 96 - agent_0 w_minus_prime_gamma: 0.689930, grad mean: 91.795120\n",
            "Iter 96 - agent_1 alpha: 1.200070, grad mean: 25.175470\n",
            "Iter 96 - agent_1 lam: 1.200070, grad mean: 3.425303\n",
            "Iter 96 - agent_1 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 96 - agent_1 w_minus_prime_gamma: 0.689930, grad mean: 73.436852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -368.44659423828125:  49%|████▉     | 98/200 [08:28<08:41,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 97 - agent_0 alpha: 1.200070, grad mean: 36.652679\n",
            "Iter 97 - agent_0 lam: 1.500070, grad mean: 3.984512\n",
            "Iter 97 - agent_0 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 97 - agent_0 w_minus_prime_gamma: 0.689930, grad mean: 106.930634\n",
            "Iter 97 - agent_1 alpha: 1.200070, grad mean: 29.322481\n",
            "Iter 97 - agent_1 lam: 1.200070, grad mean: 3.984512\n",
            "Iter 97 - agent_1 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 97 - agent_1 w_minus_prime_gamma: 0.689930, grad mean: 85.545349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -496.5257873535156:  50%|████▉     | 99/200 [08:33<08:35,  5.11s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 98 - agent_0 alpha: 1.200070, grad mean: 41.870789\n",
            "Iter 98 - agent_0 lam: 1.500070, grad mean: 4.553684\n",
            "Iter 98 - agent_0 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 98 - agent_0 w_minus_prime_gamma: 0.689930, grad mean: 122.164642\n",
            "Iter 98 - agent_1 alpha: 1.200070, grad mean: 33.497005\n",
            "Iter 98 - agent_1 lam: 1.200070, grad mean: 4.553684\n",
            "Iter 98 - agent_1 w_plus_prime_gamma: 0.500030, grad mean: 0.000001\n",
            "Iter 98 - agent_1 w_minus_prime_gamma: 0.689930, grad mean: 97.732651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -615.3317260742188:  50%|█████     | 100/200 [08:39<08:37,  5.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 99 - agent_0 alpha: 1.200070, grad mean: 46.900673\n",
            "Iter 99 - agent_0 lam: 1.500070, grad mean: 5.107388\n",
            "Iter 99 - agent_0 w_plus_prime_gamma: 0.500030, grad mean: 0.000002\n",
            "Iter 99 - agent_0 w_minus_prime_gamma: 0.689930, grad mean: 136.803436\n",
            "Iter 99 - agent_1 alpha: 1.200070, grad mean: 37.520939\n",
            "Iter 99 - agent_1 lam: 1.200070, grad mean: 5.107388\n",
            "Iter 99 - agent_1 w_plus_prime_gamma: 0.500030, grad mean: 0.000002\n",
            "Iter 99 - agent_1 w_minus_prime_gamma: 0.689930, grad mean: 109.443863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -555.2200317382812:  50%|█████     | 101/200 [08:44<08:31,  5.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100: Adaptive loss = 0.043386079370975494, dynamic factor = 1.7918128967285156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -451.2821960449219:  51%|█████     | 102/200 [08:49<08:38,  5.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 101 - agent_0 alpha: 1.200080, grad mean: 5.014759\n",
            "Iter 101 - agent_0 lam: 1.500080, grad mean: 0.541438\n",
            "Iter 101 - agent_0 w_plus_prime_gamma: 0.500035, grad mean: 0.000000\n",
            "Iter 101 - agent_0 w_minus_prime_gamma: 0.689920, grad mean: 14.585978\n",
            "Iter 101 - agent_1 alpha: 1.200080, grad mean: 4.011861\n",
            "Iter 101 - agent_1 lam: 1.200080, grad mean: 0.541438\n",
            "Iter 101 - agent_1 w_plus_prime_gamma: 0.500035, grad mean: 0.000000\n",
            "Iter 101 - agent_1 w_minus_prime_gamma: 0.689920, grad mean: 11.668935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -495.6954040527344:  52%|█████▏    | 103/200 [08:54<08:21,  5.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 102 - agent_0 alpha: 1.200080, grad mean: 10.025071\n",
            "Iter 102 - agent_0 lam: 1.500080, grad mean: 1.089030\n",
            "Iter 102 - agent_0 w_plus_prime_gamma: 0.500035, grad mean: 0.000000\n",
            "Iter 102 - agent_0 w_minus_prime_gamma: 0.689920, grad mean: 29.167982\n",
            "Iter 102 - agent_1 alpha: 1.200080, grad mean: 8.020163\n",
            "Iter 102 - agent_1 lam: 1.200080, grad mean: 1.089030\n",
            "Iter 102 - agent_1 w_plus_prime_gamma: 0.500035, grad mean: 0.000000\n",
            "Iter 102 - agent_1 w_minus_prime_gamma: 0.689920, grad mean: 23.334696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -489.30078125:  52%|█████▏    | 104/200 [09:00<08:22,  5.24s/it]     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 103 - agent_0 alpha: 1.200080, grad mean: 14.803215\n",
            "Iter 103 - agent_0 lam: 1.500080, grad mean: 1.609392\n",
            "Iter 103 - agent_0 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 103 - agent_0 w_minus_prime_gamma: 0.689920, grad mean: 43.102093\n",
            "Iter 103 - agent_1 alpha: 1.200080, grad mean: 11.842731\n",
            "Iter 103 - agent_1 lam: 1.200080, grad mean: 1.609392\n",
            "Iter 103 - agent_1 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 103 - agent_1 w_minus_prime_gamma: 0.689920, grad mean: 34.482117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -428.41192626953125:  52%|█████▎    | 105/200 [09:05<08:04,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 104 - agent_0 alpha: 1.200080, grad mean: 19.491467\n",
            "Iter 104 - agent_0 lam: 1.500080, grad mean: 2.120522\n",
            "Iter 104 - agent_0 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 104 - agent_0 w_minus_prime_gamma: 0.689920, grad mean: 56.766273\n",
            "Iter 104 - agent_1 alpha: 1.200080, grad mean: 15.593376\n",
            "Iter 104 - agent_1 lam: 1.200080, grad mean: 2.120522\n",
            "Iter 104 - agent_1 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 104 - agent_1 w_minus_prime_gamma: 0.689920, grad mean: 45.413582\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -477.3158264160156:  53%|█████▎    | 106/200 [09:10<08:02,  5.13s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 105 - agent_0 alpha: 1.200080, grad mean: 24.204271\n",
            "Iter 105 - agent_0 lam: 1.500080, grad mean: 2.629457\n",
            "Iter 105 - agent_0 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 105 - agent_0 w_minus_prime_gamma: 0.689920, grad mean: 70.490967\n",
            "Iter 105 - agent_1 alpha: 1.200080, grad mean: 19.363665\n",
            "Iter 105 - agent_1 lam: 1.200080, grad mean: 2.629457\n",
            "Iter 105 - agent_1 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 105 - agent_1 w_minus_prime_gamma: 0.689920, grad mean: 56.393467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -514.29345703125:  54%|█████▎    | 107/200 [09:15<07:59,  5.15s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 106 - agent_0 alpha: 1.200080, grad mean: 28.836636\n",
            "Iter 106 - agent_0 lam: 1.500080, grad mean: 3.133185\n",
            "Iter 106 - agent_0 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 106 - agent_0 w_minus_prime_gamma: 0.689920, grad mean: 83.998734\n",
            "Iter 106 - agent_1 alpha: 1.200080, grad mean: 23.069609\n",
            "Iter 106 - agent_1 lam: 1.200080, grad mean: 3.133185\n",
            "Iter 106 - agent_1 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 106 - agent_1 w_minus_prime_gamma: 0.689920, grad mean: 67.199814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -559.6727294921875:  54%|█████▍    | 108/200 [09:20<07:46,  5.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 107 - agent_0 alpha: 1.200080, grad mean: 33.659527\n",
            "Iter 107 - agent_0 lam: 1.500080, grad mean: 3.650673\n",
            "Iter 107 - agent_0 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 107 - agent_0 w_minus_prime_gamma: 0.689920, grad mean: 98.027061\n",
            "Iter 107 - agent_1 alpha: 1.200080, grad mean: 26.927994\n",
            "Iter 107 - agent_1 lam: 1.200080, grad mean: 3.650673\n",
            "Iter 107 - agent_1 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 107 - agent_1 w_minus_prime_gamma: 0.689920, grad mean: 78.422623\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -651.5548095703125:  55%|█████▍    | 109/200 [09:25<07:53,  5.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 108 - agent_0 alpha: 1.200080, grad mean: 38.371559\n",
            "Iter 108 - agent_0 lam: 1.500080, grad mean: 4.157896\n",
            "Iter 108 - agent_0 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 108 - agent_0 w_minus_prime_gamma: 0.689920, grad mean: 111.766136\n",
            "Iter 108 - agent_1 alpha: 1.200080, grad mean: 30.697687\n",
            "Iter 108 - agent_1 lam: 1.200080, grad mean: 4.157896\n",
            "Iter 108 - agent_1 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 108 - agent_1 w_minus_prime_gamma: 0.689920, grad mean: 89.414070\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -556.8292846679688:  55%|█████▌    | 110/200 [09:30<07:37,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 109 - agent_0 alpha: 1.200080, grad mean: 43.015743\n",
            "Iter 109 - agent_0 lam: 1.500080, grad mean: 4.655587\n",
            "Iter 109 - agent_0 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 109 - agent_0 w_minus_prime_gamma: 0.689920, grad mean: 125.289177\n",
            "Iter 109 - agent_1 alpha: 1.200080, grad mean: 34.413063\n",
            "Iter 109 - agent_1 lam: 1.200080, grad mean: 4.655587\n",
            "Iter 109 - agent_1 w_plus_prime_gamma: 0.500035, grad mean: 0.000001\n",
            "Iter 109 - agent_1 w_minus_prime_gamma: 0.689920, grad mean: 100.232658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -459.5697326660156:  56%|█████▌    | 111/200 [09:36<07:43,  5.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 110: Adaptive loss = 0.23823411762714386, dynamic factor = 7.705181121826172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -607.139404296875:  56%|█████▌    | 112/200 [09:41<07:30,  5.11s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 111 - agent_0 alpha: 1.200090, grad mean: 4.567508\n",
            "Iter 111 - agent_0 lam: 1.500090, grad mean: 0.492185\n",
            "Iter 111 - agent_0 w_plus_prime_gamma: 0.500039, grad mean: 0.000000\n",
            "Iter 111 - agent_0 w_minus_prime_gamma: 0.689910, grad mean: 13.255944\n",
            "Iter 111 - agent_1 alpha: 1.200090, grad mean: 3.654060\n",
            "Iter 111 - agent_1 lam: 1.200090, grad mean: 0.492185\n",
            "Iter 111 - agent_1 w_plus_prime_gamma: 0.500039, grad mean: 0.000000\n",
            "Iter 111 - agent_1 w_minus_prime_gamma: 0.689910, grad mean: 10.604916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -503.860107421875:  56%|█████▋    | 113/200 [09:45<07:16,  5.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 112 - agent_0 alpha: 1.200090, grad mean: 9.151545\n",
            "Iter 112 - agent_0 lam: 1.500090, grad mean: 0.987114\n",
            "Iter 112 - agent_0 w_plus_prime_gamma: 0.500039, grad mean: 0.000000\n",
            "Iter 112 - agent_0 w_minus_prime_gamma: 0.689910, grad mean: 26.594702\n",
            "Iter 112 - agent_1 alpha: 1.200090, grad mean: 7.321340\n",
            "Iter 112 - agent_1 lam: 1.200090, grad mean: 0.987114\n",
            "Iter 112 - agent_1 w_plus_prime_gamma: 0.500039, grad mean: 0.000000\n",
            "Iter 112 - agent_1 w_minus_prime_gamma: 0.689910, grad mean: 21.276089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -551.3898315429688:  57%|█████▋    | 114/200 [09:51<07:26,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 113 - agent_0 alpha: 1.200090, grad mean: 13.596696\n",
            "Iter 113 - agent_0 lam: 1.500090, grad mean: 1.464160\n",
            "Iter 113 - agent_0 w_plus_prime_gamma: 0.500039, grad mean: 0.000000\n",
            "Iter 113 - agent_0 w_minus_prime_gamma: 0.689910, grad mean: 39.505909\n",
            "Iter 113 - agent_1 alpha: 1.200090, grad mean: 10.877513\n",
            "Iter 113 - agent_1 lam: 1.200090, grad mean: 1.464160\n",
            "Iter 113 - agent_1 w_plus_prime_gamma: 0.500039, grad mean: 0.000000\n",
            "Iter 113 - agent_1 w_minus_prime_gamma: 0.689910, grad mean: 31.605207\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -553.9404296875:  57%|█████▊    | 115/200 [09:56<07:15,  5.12s/it]   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 114 - agent_0 alpha: 1.200090, grad mean: 18.057671\n",
            "Iter 114 - agent_0 lam: 1.500090, grad mean: 1.942760\n",
            "Iter 114 - agent_0 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 114 - agent_0 w_minus_prime_gamma: 0.689910, grad mean: 52.496548\n",
            "Iter 114 - agent_1 alpha: 1.200090, grad mean: 14.446347\n",
            "Iter 114 - agent_1 lam: 1.200090, grad mean: 1.942760\n",
            "Iter 114 - agent_1 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 114 - agent_1 w_minus_prime_gamma: 0.689910, grad mean: 41.997841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -574.9894409179688:  58%|█████▊    | 116/200 [10:01<07:17,  5.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 115 - agent_0 alpha: 1.200090, grad mean: 22.468330\n",
            "Iter 115 - agent_0 lam: 1.500090, grad mean: 2.418318\n",
            "Iter 115 - agent_0 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 115 - agent_0 w_minus_prime_gamma: 0.689910, grad mean: 65.361259\n",
            "Iter 115 - agent_1 alpha: 1.200090, grad mean: 17.974941\n",
            "Iter 115 - agent_1 lam: 1.200090, grad mean: 2.418318\n",
            "Iter 115 - agent_1 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 115 - agent_1 w_minus_prime_gamma: 0.689910, grad mean: 52.289764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -408.5033874511719:  58%|█████▊    | 117/200 [10:06<07:03,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 116 - agent_0 alpha: 1.200090, grad mean: 26.949821\n",
            "Iter 116 - agent_0 lam: 1.500090, grad mean: 2.900784\n",
            "Iter 116 - agent_0 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 116 - agent_0 w_minus_prime_gamma: 0.689910, grad mean: 78.379173\n",
            "Iter 116 - agent_1 alpha: 1.200090, grad mean: 21.560188\n",
            "Iter 116 - agent_1 lam: 1.200090, grad mean: 2.900784\n",
            "Iter 116 - agent_1 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 116 - agent_1 w_minus_prime_gamma: 0.689910, grad mean: 62.704231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -498.8067932128906:  59%|█████▉    | 118/200 [10:12<07:08,  5.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 117 - agent_0 alpha: 1.200090, grad mean: 31.261475\n",
            "Iter 117 - agent_0 lam: 1.500090, grad mean: 3.352828\n",
            "Iter 117 - agent_0 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 117 - agent_0 w_minus_prime_gamma: 0.689910, grad mean: 90.924721\n",
            "Iter 117 - agent_1 alpha: 1.200090, grad mean: 25.009575\n",
            "Iter 117 - agent_1 lam: 1.200090, grad mean: 3.352828\n",
            "Iter 117 - agent_1 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 117 - agent_1 w_minus_prime_gamma: 0.689910, grad mean: 72.740799\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -437.7371520996094:  60%|█████▉    | 119/200 [10:17<06:55,  5.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 118 - agent_0 alpha: 1.200090, grad mean: 35.707264\n",
            "Iter 118 - agent_0 lam: 1.500090, grad mean: 3.837846\n",
            "Iter 118 - agent_0 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 118 - agent_0 w_minus_prime_gamma: 0.689910, grad mean: 103.871941\n",
            "Iter 118 - agent_1 alpha: 1.200090, grad mean: 28.566256\n",
            "Iter 118 - agent_1 lam: 1.200090, grad mean: 3.837846\n",
            "Iter 118 - agent_1 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 118 - agent_1 w_minus_prime_gamma: 0.689910, grad mean: 83.098648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -507.44659423828125:  60%|██████    | 120/200 [10:21<06:42,  5.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 119 - agent_0 alpha: 1.200090, grad mean: 40.135235\n",
            "Iter 119 - agent_0 lam: 1.500090, grad mean: 4.310285\n",
            "Iter 119 - agent_0 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 119 - agent_0 w_minus_prime_gamma: 0.689910, grad mean: 116.746162\n",
            "Iter 119 - agent_1 alpha: 1.200090, grad mean: 32.108704\n",
            "Iter 119 - agent_1 lam: 1.200090, grad mean: 4.310285\n",
            "Iter 119 - agent_1 w_plus_prime_gamma: 0.500039, grad mean: 0.000001\n",
            "Iter 119 - agent_1 w_minus_prime_gamma: 0.689910, grad mean: 93.398163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -604.1290893554688:  60%|██████    | 121/200 [10:27<06:49,  5.19s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 120: Adaptive loss = 0.04146668687462807, dynamic factor = 1.3271121978759766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -525.4503173828125:  61%|██████    | 122/200 [10:32<06:38,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 121 - agent_0 alpha: 1.200100, grad mean: 4.415084\n",
            "Iter 121 - agent_0 lam: 1.500100, grad mean: 0.472417\n",
            "Iter 121 - agent_0 w_plus_prime_gamma: 0.500043, grad mean: 0.000000\n",
            "Iter 121 - agent_0 w_minus_prime_gamma: 0.689900, grad mean: 12.826773\n",
            "Iter 121 - agent_1 alpha: 1.200100, grad mean: 3.532125\n",
            "Iter 121 - agent_1 lam: 1.200100, grad mean: 0.472417\n",
            "Iter 121 - agent_1 w_plus_prime_gamma: 0.500043, grad mean: 0.000000\n",
            "Iter 121 - agent_1 w_minus_prime_gamma: 0.689900, grad mean: 10.261589\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -474.22509765625:  62%|██████▏   | 123/200 [10:37<06:40,  5.20s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 122 - agent_0 alpha: 1.200100, grad mean: 8.841757\n",
            "Iter 122 - agent_0 lam: 1.500100, grad mean: 0.946838\n",
            "Iter 122 - agent_0 w_plus_prime_gamma: 0.500043, grad mean: 0.000000\n",
            "Iter 122 - agent_0 w_minus_prime_gamma: 0.689900, grad mean: 25.679144\n",
            "Iter 122 - agent_1 alpha: 1.200100, grad mean: 7.073521\n",
            "Iter 122 - agent_1 lam: 1.200100, grad mean: 0.946838\n",
            "Iter 122 - agent_1 w_plus_prime_gamma: 0.500043, grad mean: 0.000000\n",
            "Iter 122 - agent_1 w_minus_prime_gamma: 0.689900, grad mean: 20.543646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -512.66259765625:  62%|██████▏   | 124/200 [10:42<06:26,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 123 - agent_0 alpha: 1.200100, grad mean: 13.308562\n",
            "Iter 123 - agent_0 lam: 1.500100, grad mean: 1.426705\n",
            "Iter 123 - agent_0 w_plus_prime_gamma: 0.500043, grad mean: 0.000000\n",
            "Iter 123 - agent_0 w_minus_prime_gamma: 0.689900, grad mean: 38.644207\n",
            "Iter 123 - agent_1 alpha: 1.200100, grad mean: 10.647032\n",
            "Iter 123 - agent_1 lam: 1.200100, grad mean: 1.426705\n",
            "Iter 123 - agent_1 w_plus_prime_gamma: 0.500043, grad mean: 0.000000\n",
            "Iter 123 - agent_1 w_minus_prime_gamma: 0.689900, grad mean: 30.915840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -440.2720642089844:  62%|██████▎   | 125/200 [10:47<06:27,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 124 - agent_0 alpha: 1.200100, grad mean: 17.780119\n",
            "Iter 124 - agent_0 lam: 1.500100, grad mean: 1.906802\n",
            "Iter 124 - agent_0 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 124 - agent_0 w_minus_prime_gamma: 0.689900, grad mean: 51.624695\n",
            "Iter 124 - agent_1 alpha: 1.200100, grad mean: 14.224334\n",
            "Iter 124 - agent_1 lam: 1.200100, grad mean: 1.906802\n",
            "Iter 124 - agent_1 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 124 - agent_1 w_minus_prime_gamma: 0.689900, grad mean: 41.300400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -449.84649658203125:  63%|██████▎   | 126/200 [10:53<06:21,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 125 - agent_0 alpha: 1.200100, grad mean: 22.281151\n",
            "Iter 125 - agent_0 lam: 1.500100, grad mean: 2.388165\n",
            "Iter 125 - agent_0 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 125 - agent_0 w_minus_prime_gamma: 0.689900, grad mean: 64.681519\n",
            "Iter 125 - agent_1 alpha: 1.200100, grad mean: 17.825222\n",
            "Iter 125 - agent_1 lam: 1.200100, grad mean: 2.388165\n",
            "Iter 125 - agent_1 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 125 - agent_1 w_minus_prime_gamma: 0.689900, grad mean: 51.746021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -525.5870361328125:  64%|██████▎   | 127/200 [10:58<06:13,  5.11s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 126 - agent_0 alpha: 1.200100, grad mean: 26.681110\n",
            "Iter 126 - agent_0 lam: 1.500100, grad mean: 2.857170\n",
            "Iter 126 - agent_0 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 126 - agent_0 w_minus_prime_gamma: 0.689900, grad mean: 77.469589\n",
            "Iter 126 - agent_1 alpha: 1.200100, grad mean: 21.345255\n",
            "Iter 126 - agent_1 lam: 1.200100, grad mean: 2.857170\n",
            "Iter 126 - agent_1 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 126 - agent_1 w_minus_prime_gamma: 0.689900, grad mean: 61.976620\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -592.33642578125:  64%|██████▍   | 128/200 [11:03<06:15,  5.21s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 127 - agent_0 alpha: 1.200100, grad mean: 31.084175\n",
            "Iter 127 - agent_0 lam: 1.500100, grad mean: 3.322639\n",
            "Iter 127 - agent_0 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 127 - agent_0 w_minus_prime_gamma: 0.689900, grad mean: 90.298172\n",
            "Iter 127 - agent_1 alpha: 1.200100, grad mean: 24.867739\n",
            "Iter 127 - agent_1 lam: 1.200100, grad mean: 3.322639\n",
            "Iter 127 - agent_1 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 127 - agent_1 w_minus_prime_gamma: 0.689900, grad mean: 72.239693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -608.3639526367188:  64%|██████▍   | 129/200 [11:08<06:02,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 128 - agent_0 alpha: 1.200100, grad mean: 35.534161\n",
            "Iter 128 - agent_0 lam: 1.500100, grad mean: 3.797384\n",
            "Iter 128 - agent_0 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 128 - agent_0 w_minus_prime_gamma: 0.689900, grad mean: 103.175346\n",
            "Iter 128 - agent_1 alpha: 1.200100, grad mean: 28.427757\n",
            "Iter 128 - agent_1 lam: 1.200100, grad mean: 3.797384\n",
            "Iter 128 - agent_1 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 128 - agent_1 w_minus_prime_gamma: 0.689900, grad mean: 82.541595\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -476.9266662597656:  65%|██████▌   | 130/200 [11:13<06:04,  5.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 129 - agent_0 alpha: 1.200100, grad mean: 40.080040\n",
            "Iter 129 - agent_0 lam: 1.500100, grad mean: 4.288538\n",
            "Iter 129 - agent_0 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 129 - agent_0 w_minus_prime_gamma: 0.689900, grad mean: 116.384331\n",
            "Iter 129 - agent_1 alpha: 1.200100, grad mean: 32.064541\n",
            "Iter 129 - agent_1 lam: 1.200100, grad mean: 4.288538\n",
            "Iter 129 - agent_1 w_plus_prime_gamma: 0.500043, grad mean: 0.000001\n",
            "Iter 129 - agent_1 w_minus_prime_gamma: 0.689900, grad mean: 93.108948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -619.8222045898438:  66%|██████▌   | 131/200 [11:18<05:53,  5.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 130: Adaptive loss = 0.12704962491989136, dynamic factor = 3.7384815216064453\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -521.5534057617188:  66%|██████▌   | 132/200 [11:24<05:55,  5.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 131 - agent_0 alpha: 1.200110, grad mean: 4.258932\n",
            "Iter 131 - agent_0 lam: 1.500110, grad mean: 0.453780\n",
            "Iter 131 - agent_0 w_plus_prime_gamma: 0.500048, grad mean: 0.000000\n",
            "Iter 131 - agent_0 w_minus_prime_gamma: 0.689890, grad mean: 12.407144\n",
            "Iter 131 - agent_1 alpha: 1.200110, grad mean: 3.407207\n",
            "Iter 131 - agent_1 lam: 1.200110, grad mean: 0.453780\n",
            "Iter 131 - agent_1 w_plus_prime_gamma: 0.500048, grad mean: 0.000000\n",
            "Iter 131 - agent_1 w_minus_prime_gamma: 0.689890, grad mean: 9.925899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -519.5029296875:  66%|██████▋   | 133/200 [11:29<05:47,  5.19s/it]   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 132 - agent_0 alpha: 1.200110, grad mean: 8.786864\n",
            "Iter 132 - agent_0 lam: 1.500110, grad mean: 0.934490\n",
            "Iter 132 - agent_0 w_plus_prime_gamma: 0.500048, grad mean: 0.000000\n",
            "Iter 132 - agent_0 w_minus_prime_gamma: 0.689890, grad mean: 25.533457\n",
            "Iter 132 - agent_1 alpha: 1.200110, grad mean: 7.029617\n",
            "Iter 132 - agent_1 lam: 1.200110, grad mean: 0.934490\n",
            "Iter 132 - agent_1 w_plus_prime_gamma: 0.500048, grad mean: 0.000000\n",
            "Iter 132 - agent_1 w_minus_prime_gamma: 0.689890, grad mean: 20.427145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -424.80010986328125:  67%|██████▋   | 134/200 [11:34<05:36,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 133 - agent_0 alpha: 1.200110, grad mean: 13.285052\n",
            "Iter 133 - agent_0 lam: 1.500110, grad mean: 1.408702\n",
            "Iter 133 - agent_0 w_plus_prime_gamma: 0.500048, grad mean: 0.000000\n",
            "Iter 133 - agent_0 w_minus_prime_gamma: 0.689890, grad mean: 38.597916\n",
            "Iter 133 - agent_1 alpha: 1.200110, grad mean: 10.628238\n",
            "Iter 133 - agent_1 lam: 1.200110, grad mean: 1.408702\n",
            "Iter 133 - agent_1 w_plus_prime_gamma: 0.500048, grad mean: 0.000000\n",
            "Iter 133 - agent_1 w_minus_prime_gamma: 0.689890, grad mean: 30.878920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -520.8834838867188:  68%|██████▊   | 135/200 [11:40<05:49,  5.38s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 134 - agent_0 alpha: 1.200110, grad mean: 17.637907\n",
            "Iter 134 - agent_0 lam: 1.500110, grad mean: 1.869017\n",
            "Iter 134 - agent_0 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 134 - agent_0 w_minus_prime_gamma: 0.689890, grad mean: 51.240765\n",
            "Iter 134 - agent_1 alpha: 1.200110, grad mean: 14.110581\n",
            "Iter 134 - agent_1 lam: 1.200110, grad mean: 1.869017\n",
            "Iter 134 - agent_1 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 134 - agent_1 w_minus_prime_gamma: 0.689890, grad mean: 40.993374\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -559.9139404296875:  68%|██████▊   | 136/200 [11:45<05:33,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 135 - agent_0 alpha: 1.200110, grad mean: 22.070604\n",
            "Iter 135 - agent_0 lam: 1.500110, grad mean: 2.343494\n",
            "Iter 135 - agent_0 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 135 - agent_0 w_minus_prime_gamma: 0.689890, grad mean: 64.100616\n",
            "Iter 135 - agent_1 alpha: 1.200110, grad mean: 17.656813\n",
            "Iter 135 - agent_1 lam: 1.200110, grad mean: 2.343494\n",
            "Iter 135 - agent_1 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 135 - agent_1 w_minus_prime_gamma: 0.689890, grad mean: 51.281471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -439.140625:  68%|██████▊   | 137/200 [11:50<05:33,  5.29s/it]       "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 136 - agent_0 alpha: 1.200110, grad mean: 26.552109\n",
            "Iter 136 - agent_0 lam: 1.500110, grad mean: 2.812627\n",
            "Iter 136 - agent_0 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 136 - agent_0 w_minus_prime_gamma: 0.689890, grad mean: 77.142311\n",
            "Iter 136 - agent_1 alpha: 1.200110, grad mean: 21.242071\n",
            "Iter 136 - agent_1 lam: 1.200110, grad mean: 2.812627\n",
            "Iter 136 - agent_1 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 136 - agent_1 w_minus_prime_gamma: 0.689890, grad mean: 61.714931\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -487.66192626953125:  69%|██████▉   | 138/200 [11:55<05:19,  5.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 137 - agent_0 alpha: 1.200110, grad mean: 31.089823\n",
            "Iter 137 - agent_0 lam: 1.500110, grad mean: 3.292558\n",
            "Iter 137 - agent_0 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 137 - agent_0 w_minus_prime_gamma: 0.689890, grad mean: 90.299347\n",
            "Iter 137 - agent_1 alpha: 1.200110, grad mean: 24.872301\n",
            "Iter 137 - agent_1 lam: 1.200110, grad mean: 3.292558\n",
            "Iter 137 - agent_1 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 137 - agent_1 w_minus_prime_gamma: 0.689890, grad mean: 72.240730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -498.6615295410156:  70%|██████▉   | 139/200 [12:00<05:17,  5.21s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 138 - agent_0 alpha: 1.200110, grad mean: 35.755672\n",
            "Iter 138 - agent_0 lam: 1.500110, grad mean: 3.786693\n",
            "Iter 138 - agent_0 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 138 - agent_0 w_minus_prime_gamma: 0.689890, grad mean: 103.824120\n",
            "Iter 138 - agent_1 alpha: 1.200110, grad mean: 28.605032\n",
            "Iter 138 - agent_1 lam: 1.200110, grad mean: 3.786693\n",
            "Iter 138 - agent_1 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 138 - agent_1 w_minus_prime_gamma: 0.689890, grad mean: 83.060753\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -442.1182556152344:  70%|███████   | 140/200 [12:05<05:08,  5.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 139 - agent_0 alpha: 1.200110, grad mean: 40.518536\n",
            "Iter 139 - agent_0 lam: 1.500110, grad mean: 4.291543\n",
            "Iter 139 - agent_0 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 139 - agent_0 w_minus_prime_gamma: 0.689890, grad mean: 117.604095\n",
            "Iter 139 - agent_1 alpha: 1.200110, grad mean: 32.415394\n",
            "Iter 139 - agent_1 lam: 1.200110, grad mean: 4.291543\n",
            "Iter 139 - agent_1 w_plus_prime_gamma: 0.500048, grad mean: 0.000001\n",
            "Iter 139 - agent_1 w_minus_prime_gamma: 0.689890, grad mean: 94.084892\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -508.14874267578125:  70%|███████   | 141/200 [12:10<04:58,  5.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 140: Adaptive loss = 0.1669943928718567, dynamic factor = 5.703136444091797\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -604.1310424804688:  71%|███████   | 142/200 [12:16<05:01,  5.20s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 141 - agent_0 alpha: 1.200120, grad mean: 4.426630\n",
            "Iter 141 - agent_0 lam: 1.500120, grad mean: 0.463192\n",
            "Iter 141 - agent_0 w_plus_prime_gamma: 0.500052, grad mean: 0.000000\n",
            "Iter 141 - agent_0 w_minus_prime_gamma: 0.689880, grad mean: 12.882374\n",
            "Iter 141 - agent_1 alpha: 1.200120, grad mean: 3.541376\n",
            "Iter 141 - agent_1 lam: 1.200120, grad mean: 0.463192\n",
            "Iter 141 - agent_1 w_plus_prime_gamma: 0.500052, grad mean: 0.000000\n",
            "Iter 141 - agent_1 w_minus_prime_gamma: 0.689880, grad mean: 10.306103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -474.8993225097656:  72%|███████▏  | 143/200 [12:21<04:51,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 142 - agent_0 alpha: 1.200120, grad mean: 8.952003\n",
            "Iter 142 - agent_0 lam: 1.500120, grad mean: 0.945057\n",
            "Iter 142 - agent_0 w_plus_prime_gamma: 0.500052, grad mean: 0.000000\n",
            "Iter 142 - agent_0 w_minus_prime_gamma: 0.689880, grad mean: 26.002697\n",
            "Iter 142 - agent_1 alpha: 1.200120, grad mean: 7.161745\n",
            "Iter 142 - agent_1 lam: 1.200120, grad mean: 0.945057\n",
            "Iter 142 - agent_1 w_plus_prime_gamma: 0.500052, grad mean: 0.000000\n",
            "Iter 142 - agent_1 w_minus_prime_gamma: 0.689880, grad mean: 20.802568\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -459.97100830078125:  72%|███████▏  | 144/200 [12:26<04:55,  5.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 143 - agent_0 alpha: 1.200120, grad mean: 13.559962\n",
            "Iter 143 - agent_0 lam: 1.500120, grad mean: 1.429672\n",
            "Iter 143 - agent_0 w_plus_prime_gamma: 0.500052, grad mean: 0.000000\n",
            "Iter 143 - agent_0 w_minus_prime_gamma: 0.689880, grad mean: 39.386593\n",
            "Iter 143 - agent_1 alpha: 1.200120, grad mean: 10.848180\n",
            "Iter 143 - agent_1 lam: 1.200120, grad mean: 1.429672\n",
            "Iter 143 - agent_1 w_plus_prime_gamma: 0.500052, grad mean: 0.000000\n",
            "Iter 143 - agent_1 w_minus_prime_gamma: 0.689880, grad mean: 31.509882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -481.703369140625:  72%|███████▎  | 145/200 [12:31<04:44,  5.18s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 144 - agent_0 alpha: 1.200120, grad mean: 18.067547\n",
            "Iter 144 - agent_0 lam: 1.500120, grad mean: 1.905220\n",
            "Iter 144 - agent_0 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 144 - agent_0 w_minus_prime_gamma: 0.689880, grad mean: 52.498856\n",
            "Iter 144 - agent_1 alpha: 1.200120, grad mean: 14.454306\n",
            "Iter 144 - agent_1 lam: 1.200120, grad mean: 1.905220\n",
            "Iter 144 - agent_1 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 144 - agent_1 w_minus_prime_gamma: 0.689880, grad mean: 41.999863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -581.2250366210938:  73%|███████▎  | 146/200 [12:36<04:40,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 145 - agent_0 alpha: 1.200120, grad mean: 22.682859\n",
            "Iter 145 - agent_0 lam: 1.500120, grad mean: 2.395630\n",
            "Iter 145 - agent_0 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 145 - agent_0 w_minus_prime_gamma: 0.689880, grad mean: 65.891479\n",
            "Iter 145 - agent_1 alpha: 1.200120, grad mean: 18.146626\n",
            "Iter 145 - agent_1 lam: 1.200120, grad mean: 2.395630\n",
            "Iter 145 - agent_1 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 145 - agent_1 w_minus_prime_gamma: 0.689880, grad mean: 52.714184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -565.1862182617188:  74%|███████▎  | 147/200 [12:41<04:33,  5.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 146 - agent_0 alpha: 1.200120, grad mean: 27.287687\n",
            "Iter 146 - agent_0 lam: 1.500120, grad mean: 2.882557\n",
            "Iter 146 - agent_0 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 146 - agent_0 w_minus_prime_gamma: 0.689880, grad mean: 79.223137\n",
            "Iter 146 - agent_1 alpha: 1.200120, grad mean: 21.830563\n",
            "Iter 146 - agent_1 lam: 1.200120, grad mean: 2.882557\n",
            "Iter 146 - agent_1 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 146 - agent_1 w_minus_prime_gamma: 0.689880, grad mean: 63.379749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -531.4401245117188:  74%|███████▍  | 148/200 [12:46<04:24,  5.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 147 - agent_0 alpha: 1.200120, grad mean: 31.948215\n",
            "Iter 147 - agent_0 lam: 1.500120, grad mean: 3.372846\n",
            "Iter 147 - agent_0 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 147 - agent_0 w_minus_prime_gamma: 0.689880, grad mean: 92.724251\n",
            "Iter 147 - agent_1 alpha: 1.200120, grad mean: 25.559065\n",
            "Iter 147 - agent_1 lam: 1.200120, grad mean: 3.372846\n",
            "Iter 147 - agent_1 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 147 - agent_1 w_minus_prime_gamma: 0.689880, grad mean: 74.180847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -662.7482299804688:  74%|███████▍  | 149/200 [12:52<04:27,  5.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 148 - agent_0 alpha: 1.200120, grad mean: 36.588623\n",
            "Iter 148 - agent_0 lam: 1.500120, grad mean: 3.866857\n",
            "Iter 148 - agent_0 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 148 - agent_0 w_minus_prime_gamma: 0.689880, grad mean: 106.134026\n",
            "Iter 148 - agent_1 alpha: 1.200120, grad mean: 29.271484\n",
            "Iter 148 - agent_1 lam: 1.200120, grad mean: 3.866857\n",
            "Iter 148 - agent_1 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 148 - agent_1 w_minus_prime_gamma: 0.689880, grad mean: 84.908920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -505.67559814453125:  75%|███████▌  | 150/200 [12:57<04:16,  5.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 149 - agent_0 alpha: 1.200120, grad mean: 40.969959\n",
            "Iter 149 - agent_0 lam: 1.500120, grad mean: 4.326152\n",
            "Iter 149 - agent_0 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 149 - agent_0 w_minus_prime_gamma: 0.689880, grad mean: 118.823738\n",
            "Iter 149 - agent_1 alpha: 1.200120, grad mean: 32.776627\n",
            "Iter 149 - agent_1 lam: 1.200120, grad mean: 4.326152\n",
            "Iter 149 - agent_1 w_plus_prime_gamma: 0.500052, grad mean: 0.000001\n",
            "Iter 149 - agent_1 w_minus_prime_gamma: 0.689880, grad mean: 95.060898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -515.1175537109375:  76%|███████▌  | 151/200 [13:02<04:16,  5.24s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 150: Adaptive loss = 0.07033080607652664, dynamic factor = 2.5347747802734375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -427.93212890625:  76%|███████▌  | 152/200 [13:07<04:06,  5.13s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 151 - agent_0 alpha: 1.200130, grad mean: 4.612248\n",
            "Iter 151 - agent_0 lam: 1.500130, grad mean: 0.489155\n",
            "Iter 151 - agent_0 w_plus_prime_gamma: 0.500056, grad mean: 0.000000\n",
            "Iter 151 - agent_0 w_minus_prime_gamma: 0.689870, grad mean: 13.363112\n",
            "Iter 151 - agent_1 alpha: 1.200130, grad mean: 3.689878\n",
            "Iter 151 - agent_1 lam: 1.200130, grad mean: 0.489155\n",
            "Iter 151 - agent_1 w_plus_prime_gamma: 0.500056, grad mean: 0.000000\n",
            "Iter 151 - agent_1 w_minus_prime_gamma: 0.689870, grad mean: 10.690722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -595.9863891601562:  76%|███████▋  | 153/200 [13:12<04:01,  5.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 152 - agent_0 alpha: 1.200130, grad mean: 9.499837\n",
            "Iter 152 - agent_0 lam: 1.500130, grad mean: 1.007895\n",
            "Iter 152 - agent_0 w_plus_prime_gamma: 0.500056, grad mean: 0.000000\n",
            "Iter 152 - agent_0 w_minus_prime_gamma: 0.689870, grad mean: 27.516235\n",
            "Iter 152 - agent_1 alpha: 1.200130, grad mean: 7.600038\n",
            "Iter 152 - agent_1 lam: 1.200130, grad mean: 1.007895\n",
            "Iter 152 - agent_1 w_plus_prime_gamma: 0.500056, grad mean: 0.000000\n",
            "Iter 152 - agent_1 w_minus_prime_gamma: 0.689870, grad mean: 22.013468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -513.3623046875:  77%|███████▋  | 154/200 [13:17<03:54,  5.10s/it]   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 153 - agent_0 alpha: 1.200130, grad mean: 14.306067\n",
            "Iter 153 - agent_0 lam: 1.500130, grad mean: 1.514440\n",
            "Iter 153 - agent_0 w_plus_prime_gamma: 0.500056, grad mean: 0.000000\n",
            "Iter 153 - agent_0 w_minus_prime_gamma: 0.689870, grad mean: 41.441391\n",
            "Iter 153 - agent_1 alpha: 1.200130, grad mean: 11.445104\n",
            "Iter 153 - agent_1 lam: 1.200130, grad mean: 1.514440\n",
            "Iter 153 - agent_1 w_plus_prime_gamma: 0.500056, grad mean: 0.000000\n",
            "Iter 153 - agent_1 w_minus_prime_gamma: 0.689870, grad mean: 33.153835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -584.2470092773438:  78%|███████▊  | 155/200 [13:22<03:47,  5.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 154 - agent_0 alpha: 1.200130, grad mean: 18.910446\n",
            "Iter 154 - agent_0 lam: 1.500130, grad mean: 2.003269\n",
            "Iter 154 - agent_0 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 154 - agent_0 w_minus_prime_gamma: 0.689870, grad mean: 54.789925\n",
            "Iter 154 - agent_1 alpha: 1.200130, grad mean: 15.128691\n",
            "Iter 154 - agent_1 lam: 1.200130, grad mean: 2.003269\n",
            "Iter 154 - agent_1 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 154 - agent_1 w_minus_prime_gamma: 0.689870, grad mean: 43.832893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -497.4313659667969:  78%|███████▊  | 156/200 [13:28<03:49,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 155 - agent_0 alpha: 1.200130, grad mean: 23.384966\n",
            "Iter 155 - agent_0 lam: 1.500130, grad mean: 2.470166\n",
            "Iter 155 - agent_0 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 155 - agent_0 w_minus_prime_gamma: 0.689870, grad mean: 67.758636\n",
            "Iter 155 - agent_1 alpha: 1.200130, grad mean: 18.708380\n",
            "Iter 155 - agent_1 lam: 1.200130, grad mean: 2.470166\n",
            "Iter 155 - agent_1 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 155 - agent_1 w_minus_prime_gamma: 0.689870, grad mean: 54.208057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -528.68017578125:  78%|███████▊  | 157/200 [13:33<03:39,  5.11s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 156 - agent_0 alpha: 1.200130, grad mean: 28.045948\n",
            "Iter 156 - agent_0 lam: 1.500130, grad mean: 2.961101\n",
            "Iter 156 - agent_0 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 156 - agent_0 w_minus_prime_gamma: 0.689870, grad mean: 81.246140\n",
            "Iter 156 - agent_1 alpha: 1.200130, grad mean: 22.437256\n",
            "Iter 156 - agent_1 lam: 1.200130, grad mean: 2.961101\n",
            "Iter 156 - agent_1 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 156 - agent_1 w_minus_prime_gamma: 0.689870, grad mean: 64.998260\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -459.58935546875:  79%|███████▉  | 158/200 [13:38<03:41,  5.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 157 - agent_0 alpha: 1.200130, grad mean: 32.565563\n",
            "Iter 157 - agent_0 lam: 1.500130, grad mean: 3.431120\n",
            "Iter 157 - agent_0 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 157 - agent_0 w_minus_prime_gamma: 0.689870, grad mean: 94.390221\n",
            "Iter 157 - agent_1 alpha: 1.200130, grad mean: 26.053040\n",
            "Iter 157 - agent_1 lam: 1.200130, grad mean: 3.431120\n",
            "Iter 157 - agent_1 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 157 - agent_1 w_minus_prime_gamma: 0.689870, grad mean: 75.513779\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -434.8185729980469:  80%|███████▉  | 159/200 [13:43<03:30,  5.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 158 - agent_0 alpha: 1.200130, grad mean: 37.155483\n",
            "Iter 158 - agent_0 lam: 1.500130, grad mean: 3.910578\n",
            "Iter 158 - agent_0 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 158 - agent_0 w_minus_prime_gamma: 0.689870, grad mean: 107.682236\n",
            "Iter 158 - agent_1 alpha: 1.200130, grad mean: 29.725061\n",
            "Iter 158 - agent_1 lam: 1.200130, grad mean: 3.910578\n",
            "Iter 158 - agent_1 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 158 - agent_1 w_minus_prime_gamma: 0.689870, grad mean: 86.147659\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -472.2134704589844:  80%|████████  | 160/200 [13:48<03:24,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 159 - agent_0 alpha: 1.200130, grad mean: 41.676685\n",
            "Iter 159 - agent_0 lam: 1.500130, grad mean: 4.386768\n",
            "Iter 159 - agent_0 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 159 - agent_0 w_minus_prime_gamma: 0.689870, grad mean: 120.795586\n",
            "Iter 159 - agent_1 alpha: 1.200130, grad mean: 33.342060\n",
            "Iter 159 - agent_1 lam: 1.200130, grad mean: 4.386768\n",
            "Iter 159 - agent_1 w_plus_prime_gamma: 0.500056, grad mean: 0.000001\n",
            "Iter 159 - agent_1 w_minus_prime_gamma: 0.689870, grad mean: 96.638542\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -584.2101440429688:  80%|████████  | 161/200 [13:54<03:23,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 160: Adaptive loss = 0.07460132241249084, dynamic factor = 2.546720504760742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -545.2113647460938:  81%|████████  | 162/200 [13:59<03:15,  5.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 161 - agent_0 alpha: 1.200140, grad mean: 4.662809\n",
            "Iter 161 - agent_0 lam: 1.500140, grad mean: 0.485307\n",
            "Iter 161 - agent_0 w_plus_prime_gamma: 0.500061, grad mean: 0.000000\n",
            "Iter 161 - agent_0 w_minus_prime_gamma: 0.689860, grad mean: 13.490897\n",
            "Iter 161 - agent_1 alpha: 1.200140, grad mean: 3.730330\n",
            "Iter 161 - agent_1 lam: 1.200140, grad mean: 0.485307\n",
            "Iter 161 - agent_1 w_plus_prime_gamma: 0.500061, grad mean: 0.000000\n",
            "Iter 161 - agent_1 w_minus_prime_gamma: 0.689860, grad mean: 10.792974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -591.9594116210938:  82%|████████▏ | 163/200 [14:04<03:14,  5.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 162 - agent_0 alpha: 1.200140, grad mean: 9.334726\n",
            "Iter 162 - agent_0 lam: 1.500140, grad mean: 0.978889\n",
            "Iter 162 - agent_0 w_plus_prime_gamma: 0.500061, grad mean: 0.000000\n",
            "Iter 162 - agent_0 w_minus_prime_gamma: 0.689860, grad mean: 27.048679\n",
            "Iter 162 - agent_1 alpha: 1.200140, grad mean: 7.467953\n",
            "Iter 162 - agent_1 lam: 1.200140, grad mean: 0.978889\n",
            "Iter 162 - agent_1 w_plus_prime_gamma: 0.500061, grad mean: 0.000000\n",
            "Iter 162 - agent_1 w_minus_prime_gamma: 0.689860, grad mean: 21.639454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -473.2980651855469:  82%|████████▏ | 164/200 [14:09<03:05,  5.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 163 - agent_0 alpha: 1.200140, grad mean: 14.183357\n",
            "Iter 163 - agent_0 lam: 1.500140, grad mean: 1.482138\n",
            "Iter 163 - agent_0 w_plus_prime_gamma: 0.500061, grad mean: 0.000000\n",
            "Iter 163 - agent_0 w_minus_prime_gamma: 0.689860, grad mean: 41.060356\n",
            "Iter 163 - agent_1 alpha: 1.200140, grad mean: 11.346957\n",
            "Iter 163 - agent_1 lam: 1.200140, grad mean: 1.482138\n",
            "Iter 163 - agent_1 w_plus_prime_gamma: 0.500061, grad mean: 0.000000\n",
            "Iter 163 - agent_1 w_minus_prime_gamma: 0.689860, grad mean: 32.849049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -586.7151489257812:  82%|████████▎ | 165/200 [14:15<03:03,  5.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 164 - agent_0 alpha: 1.200140, grad mean: 18.977003\n",
            "Iter 164 - agent_0 lam: 1.500140, grad mean: 1.988054\n",
            "Iter 164 - agent_0 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 164 - agent_0 w_minus_prime_gamma: 0.689860, grad mean: 54.968452\n",
            "Iter 164 - agent_1 alpha: 1.200140, grad mean: 15.181955\n",
            "Iter 164 - agent_1 lam: 1.200140, grad mean: 1.988054\n",
            "Iter 164 - agent_1 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 164 - agent_1 w_minus_prime_gamma: 0.689860, grad mean: 43.975788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -533.2486572265625:  83%|████████▎ | 166/200 [14:20<02:54,  5.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 165 - agent_0 alpha: 1.200140, grad mean: 23.503159\n",
            "Iter 165 - agent_0 lam: 1.500140, grad mean: 2.462926\n",
            "Iter 165 - agent_0 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 165 - agent_0 w_minus_prime_gamma: 0.689860, grad mean: 68.087448\n",
            "Iter 165 - agent_1 alpha: 1.200140, grad mean: 18.802963\n",
            "Iter 165 - agent_1 lam: 1.200140, grad mean: 2.462926\n",
            "Iter 165 - agent_1 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 165 - agent_1 w_minus_prime_gamma: 0.689860, grad mean: 54.471230\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -579.10400390625:  84%|████████▎ | 167/200 [14:25<02:49,  5.14s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 166 - agent_0 alpha: 1.200140, grad mean: 28.260538\n",
            "Iter 166 - agent_0 lam: 1.500140, grad mean: 2.955229\n",
            "Iter 166 - agent_0 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 166 - agent_0 w_minus_prime_gamma: 0.689860, grad mean: 81.887787\n",
            "Iter 166 - agent_1 alpha: 1.200140, grad mean: 22.608955\n",
            "Iter 166 - agent_1 lam: 1.200140, grad mean: 2.955229\n",
            "Iter 166 - agent_1 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 166 - agent_1 w_minus_prime_gamma: 0.689860, grad mean: 65.511726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -428.22979736328125:  84%|████████▍ | 168/200 [14:30<02:45,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 167 - agent_0 alpha: 1.200140, grad mean: 33.112988\n",
            "Iter 167 - agent_0 lam: 1.500140, grad mean: 3.465037\n",
            "Iter 167 - agent_0 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 167 - agent_0 w_minus_prime_gamma: 0.689860, grad mean: 95.911713\n",
            "Iter 167 - agent_1 alpha: 1.200140, grad mean: 26.491011\n",
            "Iter 167 - agent_1 lam: 1.200140, grad mean: 3.465037\n",
            "Iter 167 - agent_1 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 167 - agent_1 w_minus_prime_gamma: 0.689860, grad mean: 76.731155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -469.1403503417969:  84%|████████▍ | 169/200 [14:35<02:37,  5.08s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 168 - agent_0 alpha: 1.200140, grad mean: 37.913219\n",
            "Iter 168 - agent_0 lam: 1.500140, grad mean: 3.966436\n",
            "Iter 168 - agent_0 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 168 - agent_0 w_minus_prime_gamma: 0.689860, grad mean: 109.842781\n",
            "Iter 168 - agent_1 alpha: 1.200140, grad mean: 30.331291\n",
            "Iter 168 - agent_1 lam: 1.200140, grad mean: 3.966436\n",
            "Iter 168 - agent_1 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 168 - agent_1 w_minus_prime_gamma: 0.689860, grad mean: 87.876282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -473.48516845703125:  85%|████████▌ | 170/200 [14:40<02:36,  5.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 169 - agent_0 alpha: 1.200140, grad mean: 42.884781\n",
            "Iter 169 - agent_0 lam: 1.500140, grad mean: 4.486765\n",
            "Iter 169 - agent_0 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 169 - agent_0 w_minus_prime_gamma: 0.689860, grad mean: 124.252701\n",
            "Iter 169 - agent_1 alpha: 1.200140, grad mean: 34.308620\n",
            "Iter 169 - agent_1 lam: 1.200140, grad mean: 4.486765\n",
            "Iter 169 - agent_1 w_plus_prime_gamma: 0.500061, grad mean: 0.000001\n",
            "Iter 169 - agent_1 w_minus_prime_gamma: 0.689860, grad mean: 99.404388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -520.0921630859375:  86%|████████▌ | 171/200 [14:45<02:27,  5.08s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 170: Adaptive loss = 0.05201183259487152, dynamic factor = 1.7322635650634766\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -484.0438537597656:  86%|████████▌ | 172/200 [14:51<02:25,  5.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 171 - agent_0 alpha: 1.200150, grad mean: 4.860141\n",
            "Iter 171 - agent_0 lam: 1.500150, grad mean: 0.510288\n",
            "Iter 171 - agent_0 w_plus_prime_gamma: 0.500065, grad mean: 0.000000\n",
            "Iter 171 - agent_0 w_minus_prime_gamma: 0.689850, grad mean: 14.047259\n",
            "Iter 171 - agent_1 alpha: 1.200150, grad mean: 3.888212\n",
            "Iter 171 - agent_1 lam: 1.200150, grad mean: 0.510288\n",
            "Iter 171 - agent_1 w_plus_prime_gamma: 0.500065, grad mean: 0.000000\n",
            "Iter 171 - agent_1 w_minus_prime_gamma: 0.689850, grad mean: 11.238094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -575.9532470703125:  86%|████████▋ | 173/200 [14:56<02:19,  5.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 172 - agent_0 alpha: 1.200150, grad mean: 9.822975\n",
            "Iter 172 - agent_0 lam: 1.500150, grad mean: 1.027166\n",
            "Iter 172 - agent_0 w_plus_prime_gamma: 0.500065, grad mean: 0.000000\n",
            "Iter 172 - agent_0 w_minus_prime_gamma: 0.689850, grad mean: 28.393089\n",
            "Iter 172 - agent_1 alpha: 1.200150, grad mean: 7.858579\n",
            "Iter 172 - agent_1 lam: 1.200150, grad mean: 1.027166\n",
            "Iter 172 - agent_1 w_plus_prime_gamma: 0.500065, grad mean: 0.000000\n",
            "Iter 172 - agent_1 w_minus_prime_gamma: 0.689850, grad mean: 22.715057\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -529.2379760742188:  87%|████████▋ | 174/200 [15:01<02:12,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 173 - agent_0 alpha: 1.200150, grad mean: 14.604820\n",
            "Iter 173 - agent_0 lam: 1.500150, grad mean: 1.529573\n",
            "Iter 173 - agent_0 w_plus_prime_gamma: 0.500065, grad mean: 0.000000\n",
            "Iter 173 - agent_0 w_minus_prime_gamma: 0.689850, grad mean: 42.263489\n",
            "Iter 173 - agent_1 alpha: 1.200150, grad mean: 11.684149\n",
            "Iter 173 - agent_1 lam: 1.200150, grad mean: 1.529573\n",
            "Iter 173 - agent_1 w_plus_prime_gamma: 0.500065, grad mean: 0.000000\n",
            "Iter 173 - agent_1 w_minus_prime_gamma: 0.689850, grad mean: 33.811649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -473.07916259765625:  88%|████████▊ | 175/200 [15:06<02:09,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 174 - agent_0 alpha: 1.200150, grad mean: 19.511570\n",
            "Iter 174 - agent_0 lam: 1.500150, grad mean: 2.038790\n",
            "Iter 174 - agent_0 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 174 - agent_0 w_minus_prime_gamma: 0.689850, grad mean: 56.461239\n",
            "Iter 174 - agent_1 alpha: 1.200150, grad mean: 15.609647\n",
            "Iter 174 - agent_1 lam: 1.200150, grad mean: 2.038790\n",
            "Iter 174 - agent_1 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 174 - agent_1 w_minus_prime_gamma: 0.689850, grad mean: 45.170139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -419.3282775878906:  88%|████████▊ | 176/200 [15:11<02:02,  5.11s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 175 - agent_0 alpha: 1.200150, grad mean: 24.430813\n",
            "Iter 175 - agent_0 lam: 1.500150, grad mean: 2.556159\n",
            "Iter 175 - agent_0 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 175 - agent_0 w_minus_prime_gamma: 0.689850, grad mean: 70.727463\n",
            "Iter 175 - agent_1 alpha: 1.200150, grad mean: 19.545139\n",
            "Iter 175 - agent_1 lam: 1.200150, grad mean: 2.556159\n",
            "Iter 175 - agent_1 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 175 - agent_1 w_minus_prime_gamma: 0.689850, grad mean: 56.583412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -500.2815856933594:  88%|████████▊ | 177/200 [15:16<02:00,  5.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 176 - agent_0 alpha: 1.200150, grad mean: 29.269194\n",
            "Iter 176 - agent_0 lam: 1.500150, grad mean: 3.063108\n",
            "Iter 176 - agent_0 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 176 - agent_0 w_minus_prime_gamma: 0.689850, grad mean: 84.741623\n",
            "Iter 176 - agent_1 alpha: 1.200150, grad mean: 23.415943\n",
            "Iter 176 - agent_1 lam: 1.200150, grad mean: 3.063108\n",
            "Iter 176 - agent_1 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 176 - agent_1 w_minus_prime_gamma: 0.689850, grad mean: 67.795021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -583.6416625976562:  89%|████████▉ | 178/200 [15:21<01:53,  5.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 177 - agent_0 alpha: 1.200150, grad mean: 34.222134\n",
            "Iter 177 - agent_0 lam: 1.500150, grad mean: 3.579473\n",
            "Iter 177 - agent_0 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 177 - agent_0 w_minus_prime_gamma: 0.689850, grad mean: 99.067825\n",
            "Iter 177 - agent_1 alpha: 1.200150, grad mean: 27.378403\n",
            "Iter 177 - agent_1 lam: 1.200150, grad mean: 3.579473\n",
            "Iter 177 - agent_1 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 177 - agent_1 w_minus_prime_gamma: 0.689850, grad mean: 79.256348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -529.5021362304688:  90%|████████▉ | 179/200 [15:27<01:51,  5.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 178 - agent_0 alpha: 1.200150, grad mean: 39.294552\n",
            "Iter 178 - agent_0 lam: 1.500150, grad mean: 4.108223\n",
            "Iter 178 - agent_0 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 178 - agent_0 w_minus_prime_gamma: 0.689850, grad mean: 113.715286\n",
            "Iter 178 - agent_1 alpha: 1.200150, grad mean: 31.436447\n",
            "Iter 178 - agent_1 lam: 1.200150, grad mean: 4.108223\n",
            "Iter 178 - agent_1 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 178 - agent_1 w_minus_prime_gamma: 0.689850, grad mean: 90.974640\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -530.2305297851562:  90%|█████████ | 180/200 [15:32<01:43,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 179 - agent_0 alpha: 1.200150, grad mean: 44.423061\n",
            "Iter 179 - agent_0 lam: 1.500150, grad mean: 4.650014\n",
            "Iter 179 - agent_0 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 179 - agent_0 w_minus_prime_gamma: 0.689850, grad mean: 128.569977\n",
            "Iter 179 - agent_1 alpha: 1.200150, grad mean: 35.539341\n",
            "Iter 179 - agent_1 lam: 1.200150, grad mean: 4.650014\n",
            "Iter 179 - agent_1 w_plus_prime_gamma: 0.500065, grad mean: 0.000001\n",
            "Iter 179 - agent_1 w_minus_prime_gamma: 0.689850, grad mean: 102.858650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -544.7283325195312:  90%|█████████ | 181/200 [15:37<01:37,  5.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 180: Adaptive loss = 0.09268376976251602, dynamic factor = 2.9028778076171875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -508.6464538574219:  91%|█████████ | 182/200 [15:43<01:34,  5.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 181 - agent_0 alpha: 1.200160, grad mean: 5.223752\n",
            "Iter 181 - agent_0 lam: 1.500160, grad mean: 0.544756\n",
            "Iter 181 - agent_0 w_plus_prime_gamma: 0.500069, grad mean: 0.000000\n",
            "Iter 181 - agent_0 w_minus_prime_gamma: 0.689840, grad mean: 15.094688\n",
            "Iter 181 - agent_1 alpha: 1.200160, grad mean: 4.179114\n",
            "Iter 181 - agent_1 lam: 1.200160, grad mean: 0.544756\n",
            "Iter 181 - agent_1 w_plus_prime_gamma: 0.500069, grad mean: 0.000000\n",
            "Iter 181 - agent_1 w_minus_prime_gamma: 0.689840, grad mean: 12.076073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -481.99072265625:  92%|█████████▏| 183/200 [15:47<01:27,  5.15s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 182 - agent_0 alpha: 1.200160, grad mean: 10.460606\n",
            "Iter 182 - agent_0 lam: 1.500160, grad mean: 1.089153\n",
            "Iter 182 - agent_0 w_plus_prime_gamma: 0.500069, grad mean: 0.000000\n",
            "Iter 182 - agent_0 w_minus_prime_gamma: 0.689840, grad mean: 30.203600\n",
            "Iter 182 - agent_1 alpha: 1.200160, grad mean: 8.368713\n",
            "Iter 182 - agent_1 lam: 1.200160, grad mean: 1.089153\n",
            "Iter 182 - agent_1 w_plus_prime_gamma: 0.500069, grad mean: 0.000000\n",
            "Iter 182 - agent_1 w_minus_prime_gamma: 0.689840, grad mean: 24.163523\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -480.677001953125:  92%|█████████▏| 184/200 [15:54<01:28,  5.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 183 - agent_0 alpha: 1.200160, grad mean: 15.751181\n",
            "Iter 183 - agent_0 lam: 1.500160, grad mean: 1.645402\n",
            "Iter 183 - agent_0 w_plus_prime_gamma: 0.500069, grad mean: 0.000000\n",
            "Iter 183 - agent_0 w_minus_prime_gamma: 0.689840, grad mean: 45.476147\n",
            "Iter 183 - agent_1 alpha: 1.200160, grad mean: 12.601281\n",
            "Iter 183 - agent_1 lam: 1.200160, grad mean: 1.645402\n",
            "Iter 183 - agent_1 w_plus_prime_gamma: 0.500069, grad mean: 0.000000\n",
            "Iter 183 - agent_1 w_minus_prime_gamma: 0.689840, grad mean: 36.381920\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -500.1422424316406:  92%|█████████▎| 185/200 [15:59<01:19,  5.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 184 - agent_0 alpha: 1.200160, grad mean: 21.094662\n",
            "Iter 184 - agent_0 lam: 1.500160, grad mean: 2.206791\n",
            "Iter 184 - agent_0 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 184 - agent_0 w_minus_prime_gamma: 0.689840, grad mean: 60.952263\n",
            "Iter 184 - agent_1 alpha: 1.200160, grad mean: 16.876181\n",
            "Iter 184 - agent_1 lam: 1.200160, grad mean: 2.206791\n",
            "Iter 184 - agent_1 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 184 - agent_1 w_minus_prime_gamma: 0.689840, grad mean: 48.763123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -492.052978515625:  93%|█████████▎| 186/200 [16:04<01:14,  5.36s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 185 - agent_0 alpha: 1.200160, grad mean: 26.358618\n",
            "Iter 185 - agent_0 lam: 1.500160, grad mean: 2.760092\n",
            "Iter 185 - agent_0 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 185 - agent_0 w_minus_prime_gamma: 0.689840, grad mean: 76.198441\n",
            "Iter 185 - agent_1 alpha: 1.200160, grad mean: 21.087458\n",
            "Iter 185 - agent_1 lam: 1.200160, grad mean: 2.760092\n",
            "Iter 185 - agent_1 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 185 - agent_1 w_minus_prime_gamma: 0.689840, grad mean: 60.960388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -482.31768798828125:  94%|█████████▎| 187/200 [16:09<01:07,  5.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 186 - agent_0 alpha: 1.200160, grad mean: 31.794243\n",
            "Iter 186 - agent_0 lam: 1.500160, grad mean: 3.326290\n",
            "Iter 186 - agent_0 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 186 - agent_0 w_minus_prime_gamma: 0.689840, grad mean: 91.905182\n",
            "Iter 186 - agent_1 alpha: 1.200160, grad mean: 25.436073\n",
            "Iter 186 - agent_1 lam: 1.200160, grad mean: 3.326290\n",
            "Iter 186 - agent_1 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 186 - agent_1 w_minus_prime_gamma: 0.689840, grad mean: 73.526077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -504.7378845214844:  94%|█████████▍| 188/200 [16:14<01:01,  5.12s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 187 - agent_0 alpha: 1.200160, grad mean: 37.183586\n",
            "Iter 187 - agent_0 lam: 1.500160, grad mean: 3.895224\n",
            "Iter 187 - agent_0 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 187 - agent_0 w_minus_prime_gamma: 0.689840, grad mean: 107.428215\n",
            "Iter 187 - agent_1 alpha: 1.200160, grad mean: 29.747639\n",
            "Iter 187 - agent_1 lam: 1.200160, grad mean: 3.895224\n",
            "Iter 187 - agent_1 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 187 - agent_1 w_minus_prime_gamma: 0.689840, grad mean: 85.944893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -521.0038452148438:  94%|█████████▍| 189/200 [16:19<00:57,  5.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 188 - agent_0 alpha: 1.200160, grad mean: 42.759827\n",
            "Iter 188 - agent_0 lam: 1.500160, grad mean: 4.471226\n",
            "Iter 188 - agent_0 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 188 - agent_0 w_minus_prime_gamma: 0.689840, grad mean: 123.510208\n",
            "Iter 188 - agent_1 alpha: 1.200160, grad mean: 34.208759\n",
            "Iter 188 - agent_1 lam: 1.200160, grad mean: 4.471226\n",
            "Iter 188 - agent_1 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 188 - agent_1 w_minus_prime_gamma: 0.689840, grad mean: 98.810783\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -469.7966003417969:  95%|█████████▌| 190/200 [16:24<00:51,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 189 - agent_0 alpha: 1.200160, grad mean: 48.219395\n",
            "Iter 189 - agent_0 lam: 1.500160, grad mean: 5.045531\n",
            "Iter 189 - agent_0 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 189 - agent_0 w_minus_prime_gamma: 0.689840, grad mean: 139.298706\n",
            "Iter 189 - agent_1 alpha: 1.200160, grad mean: 38.576550\n",
            "Iter 189 - agent_1 lam: 1.200160, grad mean: 5.045531\n",
            "Iter 189 - agent_1 w_plus_prime_gamma: 0.500069, grad mean: 0.000001\n",
            "Iter 189 - agent_1 w_minus_prime_gamma: 0.689840, grad mean: 111.441925\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -487.8529357910156:  96%|█████████▌| 191/200 [16:30<00:47,  5.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 190: Adaptive loss = 0.14292359352111816, dynamic factor = 5.144065856933594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -554.0320434570312:  96%|█████████▌| 192/200 [16:35<00:41,  5.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 191 - agent_0 alpha: 1.200170, grad mean: 5.834091\n",
            "Iter 191 - agent_0 lam: 1.500170, grad mean: 0.611726\n",
            "Iter 191 - agent_0 w_plus_prime_gamma: 0.500073, grad mean: 0.000000\n",
            "Iter 191 - agent_0 w_minus_prime_gamma: 0.689830, grad mean: 16.793400\n",
            "Iter 191 - agent_1 alpha: 1.200170, grad mean: 4.667402\n",
            "Iter 191 - agent_1 lam: 1.200170, grad mean: 0.611726\n",
            "Iter 191 - agent_1 w_plus_prime_gamma: 0.500073, grad mean: 0.000000\n",
            "Iter 191 - agent_1 w_minus_prime_gamma: 0.689830, grad mean: 13.435104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -409.8778381347656:  96%|█████████▋| 193/200 [16:40<00:36,  5.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 192 - agent_0 alpha: 1.200170, grad mean: 11.742702\n",
            "Iter 192 - agent_0 lam: 1.500170, grad mean: 1.237358\n",
            "Iter 192 - agent_0 w_plus_prime_gamma: 0.500073, grad mean: 0.000000\n",
            "Iter 192 - agent_0 w_minus_prime_gamma: 0.689830, grad mean: 33.780964\n",
            "Iter 192 - agent_1 alpha: 1.200170, grad mean: 9.394423\n",
            "Iter 192 - agent_1 lam: 1.200170, grad mean: 1.237358\n",
            "Iter 192 - agent_1 w_plus_prime_gamma: 0.500073, grad mean: 0.000000\n",
            "Iter 192 - agent_1 w_minus_prime_gamma: 0.689830, grad mean: 27.025545\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -535.8864135742188:  97%|█████████▋| 194/200 [16:45<00:31,  5.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 193 - agent_0 alpha: 1.200170, grad mean: 17.642719\n",
            "Iter 193 - agent_0 lam: 1.500170, grad mean: 1.854670\n",
            "Iter 193 - agent_0 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 193 - agent_0 w_minus_prime_gamma: 0.689830, grad mean: 50.842499\n",
            "Iter 193 - agent_1 alpha: 1.200170, grad mean: 14.114574\n",
            "Iter 193 - agent_1 lam: 1.200170, grad mean: 1.854670\n",
            "Iter 193 - agent_1 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 193 - agent_1 w_minus_prime_gamma: 0.689830, grad mean: 40.675179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -473.03399658203125:  98%|█████████▊| 195/200 [16:50<00:25,  5.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 194 - agent_0 alpha: 1.200170, grad mean: 23.613501\n",
            "Iter 194 - agent_0 lam: 1.500170, grad mean: 2.486857\n",
            "Iter 194 - agent_0 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 194 - agent_0 w_minus_prime_gamma: 0.689830, grad mean: 68.060318\n",
            "Iter 194 - agent_1 alpha: 1.200170, grad mean: 18.891333\n",
            "Iter 194 - agent_1 lam: 1.200170, grad mean: 2.486857\n",
            "Iter 194 - agent_1 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 194 - agent_1 w_minus_prime_gamma: 0.689830, grad mean: 54.449833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -603.077392578125:  98%|█████████▊| 196/200 [16:56<00:20,  5.23s/it]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 195 - agent_0 alpha: 1.200170, grad mean: 29.720272\n",
            "Iter 195 - agent_0 lam: 1.500170, grad mean: 3.128843\n",
            "Iter 195 - agent_0 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 195 - agent_0 w_minus_prime_gamma: 0.689830, grad mean: 85.630531\n",
            "Iter 195 - agent_1 alpha: 1.200170, grad mean: 23.776888\n",
            "Iter 195 - agent_1 lam: 1.200170, grad mean: 3.128843\n",
            "Iter 195 - agent_1 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 195 - agent_1 w_minus_prime_gamma: 0.689830, grad mean: 68.506409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -559.131103515625:  98%|█████████▊| 197/200 [17:00<00:15,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 196 - agent_0 alpha: 1.200170, grad mean: 35.668655\n",
            "Iter 196 - agent_0 lam: 1.500170, grad mean: 3.757627\n",
            "Iter 196 - agent_0 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 196 - agent_0 w_minus_prime_gamma: 0.689830, grad mean: 102.808113\n",
            "Iter 196 - agent_1 alpha: 1.200170, grad mean: 28.535706\n",
            "Iter 196 - agent_1 lam: 1.200170, grad mean: 3.757627\n",
            "Iter 196 - agent_1 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 196 - agent_1 w_minus_prime_gamma: 0.689830, grad mean: 82.248863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -407.782470703125:  99%|█████████▉| 198/200 [17:06<00:10,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 197 - agent_0 alpha: 1.200170, grad mean: 41.635876\n",
            "Iter 197 - agent_0 lam: 1.500170, grad mean: 4.390976\n",
            "Iter 197 - agent_0 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 197 - agent_0 w_minus_prime_gamma: 0.689830, grad mean: 120.023552\n",
            "Iter 197 - agent_1 alpha: 1.200170, grad mean: 33.309605\n",
            "Iter 197 - agent_1 lam: 1.200170, grad mean: 4.390976\n",
            "Iter 197 - agent_1 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 197 - agent_1 w_minus_prime_gamma: 0.689830, grad mean: 96.021652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -475.30743408203125: 100%|█████████▉| 199/200 [17:11<00:05,  5.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 198 - agent_0 alpha: 1.200170, grad mean: 47.754375\n",
            "Iter 198 - agent_0 lam: 1.500170, grad mean: 5.035998\n",
            "Iter 198 - agent_0 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 198 - agent_0 w_minus_prime_gamma: 0.689830, grad mean: 137.602829\n",
            "Iter 198 - agent_1 alpha: 1.200170, grad mean: 38.204521\n",
            "Iter 198 - agent_1 lam: 1.200170, grad mean: 5.035998\n",
            "Iter 198 - agent_1 w_plus_prime_gamma: 0.500073, grad mean: 0.000001\n",
            "Iter 198 - agent_1 w_minus_prime_gamma: 0.689830, grad mean: 110.085487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\repisode_reward_mean_agents = -432.3363342285156: 100%|██████████| 200/200 [17:16<00:00,  5.22s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 199 - agent_0 alpha: 1.200170, grad mean: 53.845490\n",
            "Iter 199 - agent_0 lam: 1.500170, grad mean: 5.675220\n",
            "Iter 199 - agent_0 w_plus_prime_gamma: 0.500073, grad mean: 0.000002\n",
            "Iter 199 - agent_0 w_minus_prime_gamma: 0.689830, grad mean: 155.156265\n",
            "Iter 199 - agent_1 alpha: 1.200170, grad mean: 43.077526\n",
            "Iter 199 - agent_1 lam: 1.200170, grad mean: 5.675220\n",
            "Iter 199 - agent_1 w_plus_prime_gamma: 0.500073, grad mean: 0.000002\n",
            "Iter 199 - agent_1 w_minus_prime_gamma: 0.689830, grad mean: 124.128647\n"
          ]
        }
      ],
      "source": [
        "pbar = tqdm(\n",
        "    total=n_iters,\n",
        "    desc=\", \".join(\n",
        "        [f\"episode_reward_mean_{group} = 0\" for group in env.group_map.keys()]\n",
        "    ),\n",
        ")\n",
        "episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
        "train_group_map = copy.deepcopy(env.group_map)\n",
        "\n",
        "\n",
        "adaptive_update_frequency = 10  # update adaptive parameters every 10 iterations\n",
        "freeze_adaptive_until = 20  # don't update adaptive parameters until after 20 iterations\n",
        "scale_factor = 1e-3  # scaling for adaptive loss\n",
        "reg_lambda = 1e-3  # regularization coefficient\n",
        "\n",
        "# (Assume initial_params is defined right after adaptive_params creation)\n",
        "# For example:\n",
        "initial_params = {\n",
        "    agent_id: {name: param.clone().detach() for name, param in module.get_params().items()}\n",
        "    for agent_id, module in adaptive_params.items()\n",
        "}\n",
        "\n",
        "previous_base_loss = None\n",
        "\n",
        "for iteration, batch in enumerate(collector):\n",
        "    current_frames = batch.numel()\n",
        "    batch = process_batch(batch)\n",
        "\n",
        "    for group in train_group_map.keys():\n",
        "        group_batch = batch.exclude(\n",
        "            *[\n",
        "                key\n",
        "                for _group in env.group_map.keys()\n",
        "                if _group != group\n",
        "                for key in [_group, (\"next\", _group)]\n",
        "            ]\n",
        "        )\n",
        "        group_batch = group_batch.reshape(-1)\n",
        "        replay_buffers[group].extend(group_batch)\n",
        "\n",
        "        for _ in range(n_optimiser_steps):\n",
        "            subdata = replay_buffers[group].sample()\n",
        "            loss_vals = losses[group](subdata)\n",
        "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
        "                loss_value = loss_vals[loss_name]\n",
        "                optimiser = optimisers[group][loss_name]\n",
        "                loss_value.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(optimiser.param_groups[0][\"params\"], max_grad_norm)\n",
        "                optimiser.step()\n",
        "                optimiser.zero_grad()\n",
        "            target_updaters[group].step()\n",
        "            exploration_policies[group][-1].step(current_frames)\n",
        "\n",
        "    # Decoupled update for adaptive parameters every adaptive_update_frequency iterations,\n",
        "    # but only after freeze_adaptive_until iterations.\n",
        "    if iteration > freeze_adaptive_until and iteration % adaptive_update_frequency == 0:\n",
        "        # Sample a batch from the cooperative group \"agents\"\n",
        "        subdata = replay_buffers[\"agents\"].sample()\n",
        "\n",
        "        # Compute the standard target Q-value:\n",
        "        target_value = losses[\"agents\"]._value_estimator.value_estimate(\n",
        "            subdata, target_params=losses[\"agents\"]._cached_target_params\n",
        "        ).squeeze(-1)\n",
        "\n",
        "        # Compute the CPT-transformed target:\n",
        "        target_value_CPT = C_transform_cross_dynamic(target_value, adaptive_params)\n",
        "\n",
        "        # Compute the base adaptive loss as mean squared error:\n",
        "        base_loss = torch.mean((target_value_CPT - target_value) ** 2)\n",
        "\n",
        "        # Compute a dynamic scaling factor based on the change in base loss\n",
        "        if previous_base_loss is None:\n",
        "            dynamic_factor = 1.0\n",
        "        else:\n",
        "            # Increase the factor if the loss has changed substantially\n",
        "            dynamic_factor = 1.0 + torch.abs(base_loss - previous_base_loss)\n",
        "            # If you prefer to have a plain Python float, you can call .item() here\n",
        "            # dynamic_factor = 1.0 + torch.abs(base_loss - previous_base_loss).item()\n",
        "\n",
        "        # Update the previous_base_loss for the next adaptive update (detach it to avoid gradient tracking)\n",
        "        previous_base_loss = base_loss.detach()\n",
        "\n",
        "        # Compute the L2 regularization loss for keeping adaptive parameters near their initial values:\n",
        "        reg_loss = 0.0\n",
        "        for agent_id, module in adaptive_params.items():\n",
        "            params = module.get_params()\n",
        "            for name, param in params.items():\n",
        "                reg_loss += torch.mean((param - initial_params[agent_id][name]) ** 2)\n",
        "\n",
        "        # Combine the base adaptive loss (scaled dynamically) with the regularization term\n",
        "        adaptive_loss = dynamic_factor * scale_factor * base_loss + reg_lambda * reg_loss\n",
        "\n",
        "        # Print out the loss and the dynamic factor (dynamic_factor is a float or a tensor—if it's a float, no .item() is needed)\n",
        "        print(f\"Iteration {iteration}: Adaptive loss = {adaptive_loss.item()}, dynamic factor = {dynamic_factor}\")\n",
        "\n",
        "        # Backward pass and update adaptive parameters:\n",
        "        adaptive_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(adaptive_params.parameters(), max_norm=1.0)\n",
        "        optimizer_behavioral.step()\n",
        "        optimizer_behavioral.zero_grad()\n",
        "\n",
        "    # Optionally, monitor adaptive parameters here\n",
        "    for agent_id, module in adaptive_params.items():\n",
        "        params = module.get_params()\n",
        "        for name, param in params.items():\n",
        "            if param.grad is not None:\n",
        "                print(f\"Iter {iteration} - {agent_id} {name}: {param.item():.6f}, grad mean: {param.grad.abs().mean().item():.6f}\")\n",
        "\n",
        "    # Logging of episode rewards, etc.\n",
        "    for group in env.group_map.keys():\n",
        "        episode_reward_mean = (\n",
        "            batch.get((\"next\", group, \"episode_reward\"))[\n",
        "                batch.get((\"next\", group, \"done\"))\n",
        "            ]\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        episode_reward_mean_map[group].append(episode_reward_mean)\n",
        "\n",
        "    pbar.set_description(\n",
        "        \", \".join(\n",
        "            [\n",
        "                f\"episode_reward_mean_{group} = {episode_reward_mean_map[group][-1]}\"\n",
        "                for group in env.group_map.keys()\n",
        "            ]\n",
        "        ),\n",
        "        refresh=False,\n",
        "    )\n",
        "    pbar.update()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdxfvvGb2uRb"
      },
      "source": [
        "This is our \"test\" to make sure our agents are trainng, we see after the agent stops training the adversaries rewards are increasing and then while it is trainng their rewards both go to 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_voZiw3WnTw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "ca874291-ae18-4a62-9e07-8364bd11a86b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['agents'])\n"
          ]
        }
      ],
      "source": [
        "fig, axs = plt.subplots(1, 1)\n",
        "for i, group in enumerate(env.group_map.keys()):\n",
        "    axs.plot(episode_reward_mean_map[group], label=f\"Episode reward mean {group}\")\n",
        "    axs.set_ylabel(\"Reward\")\n",
        "    axs.legend()\n",
        "axs.set_xlabel(\"Training iterations\")\n",
        "plt.show()\n",
        "print(env.group_map.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBIAaI_SXoSd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "outputId": "f8bcdaf0-5a79-4b01-d63c-cfa19f66e46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating rendering env\n",
            "Rendering rollout...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "Error occurred while running `from pyglet.gl import *`, HINT: make sure you have OpenGL installed. On Ubuntu, you can run 'apt-get install python3-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work:'xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vmas/simulator/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     from pyglet.gl import (\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mGL_BLEND\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/gl/gl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mctypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlink_GL\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_link_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mc_ptrdiff_t\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib_glx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlink_GL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_GLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_GLX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/gl/lib_glx.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mgl_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mglu_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GLU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyglet/lib.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, *names, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Library \"%s\" not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Library \"GLU\" not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-a1fda816bdad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mset_exploration_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExplorationType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDETERMINISTIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rendering rollout...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0menv_with_render\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magents_exploration_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving the video...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0menv_with_render\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/common.py\u001b[0m in \u001b[0;36mrollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, break_when_all_done, return_contiguous, tensordict, set_truncated, out, trust_policy)\u001b[0m\n\u001b[1;32m   2598\u001b[0m             policy = _make_compatible_policy(\n\u001b[1;32m   2599\u001b[0m                 \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2601\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m                 \u001b[0mfast_wrap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/common.py\u001b[0m in \u001b[0;36mobservation_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \"\"\"\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0mobservation_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"full_observation_spec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mobservation_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mobservation_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComposite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/transforms/transforms.py\u001b[0m in \u001b[0;36moutput_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0moutput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlock_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0moutput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_output_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0moutput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_specs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/transforms/transforms.py\u001b[0m in \u001b[0;36mtransform_output_spec\u001b[0;34m(self, output_spec)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform_output_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorSpec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensorSpec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0moutput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_output_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/envs/transforms/transforms.py\u001b[0m in \u001b[0;36mtransform_output_spec\u001b[0;34m(self, output_spec)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0moutput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         output_spec[\"full_observation_spec\"] = self.transform_observation_spec(\n\u001b[0m\u001b[1;32m    382\u001b[0m             \u001b[0moutput_spec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"full_observation_spec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/record/recorder.py\u001b[0m in \u001b[0;36mtransform_observation_spec\u001b[0;34m(self, observation_spec)\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mtd_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNonTensorData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchrl/record/recorder.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_tensordict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensordict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vmas/simulator/environment/environment.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, env_index, agent_index_focus, visualize_when_rgb, plot_position_function, plot_position_function_precision, plot_position_function_range, plot_position_function_cmap_range, plot_position_function_cmap_alpha, plot_position_function_cmap_name)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"headless\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheadless\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_rendering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscenario\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer_zoom\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vmas/simulator/environment/environment.py\u001b[0m in \u001b[0;36m_init_rendering\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_rendering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mvmas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         self.viewer = rendering.Viewer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vmas/simulator/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     59\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;34m\"Error occurred while running `from pyglet.gl import *`, HINT: make sure you have OpenGL installed. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m\"On Ubuntu, you can run 'apt-get install python3-opengl'. If you're running on a server, you may need a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Error occurred while running `from pyglet.gl import *`, HINT: make sure you have OpenGL installed. On Ubuntu, you can run 'apt-get install python3-opengl'. If you're running on a server, you may need a virtual frame buffer; something like this should work:'xvfb-run -s \"-screen 0 1400x900x24\" python <your_script.py>'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "if use_vmas and not is_sphinx:\n",
        "    # Replace tmpdir with any desired path where the video should be saved\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        video_logger = CSVLogger(\"vmas_logs\", tmpdir, video_format=\"mp4\")\n",
        "        print(\"Creating rendering env\")\n",
        "        env_with_render = TransformedEnv(env.base_env, env.transform.clone())\n",
        "        env_with_render = env_with_render.append_transform(\n",
        "            PixelRenderTransform(\n",
        "                out_keys=[\"pixels\"],\n",
        "                # the np.ndarray has a negative stride and needs to be copied before being cast to a tensor\n",
        "                preproc=lambda x: x.copy(),\n",
        "                as_non_tensor=True,\n",
        "                # asking for array rather than on-screen rendering\n",
        "                mode=\"rgb_array\",\n",
        "            )\n",
        "        )\n",
        "        env_with_render = env_with_render.append_transform(\n",
        "            VideoRecorder(logger=video_logger, tag=\"vmas_rendered\")\n",
        "        )\n",
        "        with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
        "            print(\"Rendering rollout...\")\n",
        "            env_with_render.rollout(100, policy=agents_exploration_policy)\n",
        "        print(\"Saving the video...\")\n",
        "        env_with_render.transform.dump()\n",
        "        print(\"Saved! Saved directory tree:\")\n",
        "        video_logger.print_log_dir()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define a permanent directory path (e.g., \"local_videos\")\n",
        "local_dir = \"local_videos\"\n",
        "os.makedirs(local_dir, exist_ok=True)\n",
        "\n",
        "# Use the permanent directory instead of a temporary one\n",
        "video_logger = CSVLogger(\"vmas_logs\", local_dir, video_format=\"mp4\")\n",
        "print(\"Creating rendering env\")\n",
        "env_with_render = TransformedEnv(env.base_env, env.transform.clone())\n",
        "env_with_render = env_with_render.append_transform(\n",
        "    PixelRenderTransform(\n",
        "        out_keys=[\"pixels\"],\n",
        "        # the np.ndarray has a negative stride and needs to be copied before being cast to a tensor\n",
        "        preproc=lambda x: x.copy(),\n",
        "        as_non_tensor=True,\n",
        "        # asking for array rather than on-screen rendering\n",
        "        mode=\"rgb_array\",\n",
        "    )\n",
        ")\n",
        "env_with_render = env_with_render.append_transform(\n",
        "    VideoRecorder(logger=video_logger, tag=\"vmas_rendered\")\n",
        ")\n",
        "with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
        "    print(\"Rendering rollout...\")\n",
        "    env_with_render.rollout(100, policy=agents_exploration_policy)\n",
        "print(\"Saving the video...\")\n",
        "env_with_render.transform.dump()\n",
        "print(\"Saved! Saved directory tree:\")\n",
        "video_logger.print_log_dir()"
      ],
      "metadata": {
        "id": "ebVlbORDdSYM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}