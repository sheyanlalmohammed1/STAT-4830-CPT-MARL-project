{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchrl==0.6.0 in /opt/conda/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: torch>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from torchrl==0.6.0) (2.6.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchrl==0.6.0) (1.24.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from torchrl==0.6.0) (23.2)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.11/site-packages (from torchrl==0.6.0) (3.0.0)\n",
      "Requirement already satisfied: tensordict>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from torchrl==0.6.0) (0.7.2)\n",
      "Requirement already satisfied: orjson in /opt/conda/lib/python3.11/site-packages (from tensordict>=0.6.0->torchrl==0.6.0) (3.10.16)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (4.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.5.0->torchrl==0.6.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.5.0->torchrl==0.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.5.0->torchrl==0.6.0) (2.1.3)\n",
      "Requirement already satisfied: vmas in /opt/conda/lib/python3.11/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from vmas) (1.24.4)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (from vmas) (2.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.27 in /opt/conda/lib/python3.11/site-packages (from vmas) (1.5.27)\n",
      "Requirement already satisfied: gym in /opt/conda/lib/python3.11/site-packages (from vmas) (0.26.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from vmas) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gym->vmas) (3.0.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /opt/conda/lib/python3.11/site-packages (from gym->vmas) (0.0.8)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (4.13.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch->vmas) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch->vmas) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch->vmas) (2.1.3)\n",
      "Requirement already satisfied: pettingzoo==1.24.3 in /opt/conda/lib/python3.11/site-packages (from pettingzoo[mpe]==1.24.3) (1.24.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (1.24.4)\n",
      "Requirement already satisfied: gymnasium>=0.28.0 in /opt/conda/lib/python3.11/site-packages (from pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (1.1.1)\n",
      "Requirement already satisfied: pygame==2.3.0 in /opt/conda/lib/python3.11/site-packages (from pettingzoo[mpe]==1.24.3) (2.3.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium>=0.28.0->pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.11/site-packages (from gymnasium>=0.28.0->pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (4.13.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from gymnasium>=0.28.0->pettingzoo==1.24.3->pettingzoo[mpe]==1.24.3) (0.0.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchrl==0.6.0\n",
    "!pip3 install vmas\n",
    "!pip3 install pettingzoo[mpe]==1.24.3\n",
    "!pip3 install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensordict import TensorDictBase, is_tensor_collection\n",
    "\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch import multiprocessing\n",
    "\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
    "\n",
    "from torchrl.envs import (\n",
    "    check_env_specs,\n",
    "    ExplorationType,\n",
    "    PettingZooEnv,\n",
    "    RewardSum,\n",
    "    set_exploration_type,\n",
    "    TransformedEnv,\n",
    "    VmasEnv,\n",
    ")\n",
    "\n",
    "from torchrl.modules import (\n",
    "    AdditiveGaussianModule,\n",
    "    MultiAgentMLP,\n",
    "    ProbabilisticActor,\n",
    "    TanhDelta,\n",
    ")\n",
    "\n",
    "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
    "\n",
    "from torchrl.record import CSVLogger, PixelRenderTransform, VideoRecorder\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    is_sphinx = __sphinx_build__\n",
    "except NameError:\n",
    "    is_sphinx = False\n",
    "\n",
    "\n",
    "try:\n",
    "    from torch.compiler import is_compiling\n",
    "except ImportError:\n",
    "    from torch._dynamo import is_compiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Devices\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "# Sampling\n",
    "frames_per_batch = 1000  # Number of team frames collected per sampling iteration\n",
    "n_iters = 500  # Number of sampling and training iterations\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "\n",
    "# Replay buffer\n",
    "memory_size = 1000000  # The replay buffer of each group can store this many frames\n",
    "\n",
    "# Training\n",
    "n_optimiser_steps = 100  # Number of optimization steps per training iteration\n",
    "train_batch_size = 128  # Number of frames trained in each optimiser step\n",
    "lr = 1e-4  # Learning rate\n",
    "max_grad_norm = 1 # Maximum norm for the gradients\n",
    "\n",
    "# DDPG\n",
    "gamma = 0.99  # Discount factor\n",
    "polyak_tau = 0.01  # Tau for the soft-update of the target network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 100  # Environment steps before done\n",
    "\n",
    "n_agents = 2\n",
    "n_landmarks = 1\n",
    "\n",
    "use_vmas = True  # Set this to True for a great performance speedup\n",
    "\n",
    "if not use_vmas:\n",
    "  base_env = PettingZooEnv(\n",
    "      task=\"simple_spread_v3\",\n",
    "      parallel=True,\n",
    "      seed=seed,\n",
    "      continuous_actions=True,\n",
    "      N = n_landmarks\n",
    "  )\n",
    "else:\n",
    "    num_vmas_envs = (\n",
    "        frames_per_batch // max_steps\n",
    "    )\n",
    "    base_env = VmasEnv(\n",
    "        scenario=\"simple_spread\",\n",
    "        num_envs=num_vmas_envs,\n",
    "        continuous_actions=True,\n",
    "        max_steps=max_steps,\n",
    "        local_ratio=0.5,\n",
    "        device=device,\n",
    "        seed=seed,\n",
    "        n_agents = n_agents\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_map: {'agents': ['agent_0', 'agent_1']}\n"
     ]
    }
   ],
   "source": [
    "print(f\"group_map: {base_env.group_map}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: Composite(\n",
      "    agents: Composite(\n",
      "        action: BoundedContinuous(\n",
      "            shape=torch.Size([10, 2, 2]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2, 2]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2, 2]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([10, 2])),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([10]))\n",
      "reward_spec: Composite(\n",
      "    agents: Composite(\n",
      "        reward: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 2, 1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2, 1]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2, 1]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([10, 2])),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([10]))\n",
      "done_spec: Composite(\n",
      "    done: Categorical(\n",
      "        shape=torch.Size([10, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    terminated: Categorical(\n",
      "        shape=torch.Size([10, 1]),\n",
      "        space=CategoricalBox(n=2),\n",
      "        device=cuda:0,\n",
      "        dtype=torch.bool,\n",
      "        domain=discrete),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([10]))\n",
      "observation_spec: Composite(\n",
      "    agents: Composite(\n",
      "        observation: UnboundedContinuous(\n",
      "            shape=torch.Size([10, 2, 10]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([10, 2, 10]), device=cuda:0, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([10, 2, 10]), device=cuda:0, dtype=torch.float32, contiguous=True)),\n",
      "            device=cuda:0,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        device=cuda:0,\n",
      "        shape=torch.Size([10, 2])),\n",
      "    device=cuda:0,\n",
      "    shape=torch.Size([10]))\n"
     ]
    }
   ],
   "source": [
    "print(\"action_spec:\", base_env.full_action_spec)\n",
    "print(\"reward_spec:\", base_env.full_reward_spec)\n",
    "print(\"done_spec:\", base_env.full_done_spec)\n",
    "print(\"observation_spec:\", base_env.observation_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_keys: [('agents', 'action')]\n",
      "reward_keys: [('agents', 'reward')]\n",
      "done_keys: ['done', 'terminated']\n"
     ]
    }
   ],
   "source": [
    "print(\"action_keys:\", base_env.action_keys)\n",
    "print(\"reward_keys:\", base_env.reward_keys)\n",
    "print(\"done_keys:\", base_env.done_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    base_env,\n",
    "    RewardSum(\n",
    "        in_keys=base_env.reward_keys,\n",
    "        reset_keys=[\"_reset\"] * len(base_env.group_map.keys()),\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 00:45:45,228 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout of 5 steps: TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 5, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 5, 2, 10]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([10, 5, 2]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                agents: TensorDict(\n",
      "                    fields={\n",
      "                        episode_reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        observation: Tensor(shape=torch.Size([10, 5, 2, 10]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                        reward: Tensor(shape=torch.Size([10, 5, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "                    batch_size=torch.Size([10, 5, 2]),\n",
      "                    device=cuda:0,\n",
      "                    is_shared=True),\n",
      "                done: Tensor(shape=torch.Size([10, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "                terminated: Tensor(shape=torch.Size([10, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "            batch_size=torch.Size([10, 5]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 5, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10, 5]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n",
      "Shape of the rollout TensorDict: torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "n_rollout_steps = 5\n",
    "rollout = env.rollout(n_rollout_steps)\n",
    "print(f\"rollout of {n_rollout_steps} steps:\", rollout)\n",
    "print(\"Shape of the rollout TensorDict:\", rollout.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_modules = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_policy = False  # Can change this based on the group\n",
    "\n",
    "    policy_net = MultiAgentMLP(\n",
    "        n_agent_inputs=env.observation_spec[group, \"observation\"].shape[\n",
    "            -1\n",
    "        ],  # n_obs_per_agent\n",
    "        n_agent_outputs=env.full_action_spec[group, \"action\"].shape[\n",
    "            -1\n",
    "        ],  # n_actions_per_agents\n",
    "        n_agents=len(agents),  # Number of agents in the group\n",
    "        centralised=False,  # the policies are decentralised (i.e., each agent will act from its local observation)\n",
    "        share_params=share_parameters_policy,\n",
    "        device=device,\n",
    "        depth=2,\n",
    "        num_cells=256,\n",
    "        activation_class=torch.nn.Tanh,\n",
    "    )\n",
    "\n",
    "    # Wrap the neural network in a :class:`~tensordict.nn.TensorDictModule`.\n",
    "    # This is simply a module that will read the ``in_keys`` from a tensordict, feed them to the\n",
    "    # neural networks, and write the\n",
    "    # outputs in-place at the ``out_keys``.\n",
    "\n",
    "    policy_module = TensorDictModule(\n",
    "        policy_net,\n",
    "        in_keys=[(group, \"observation\")],\n",
    "        out_keys=[(group, \"param\")],\n",
    "    )  # We just name the input and output that the network will read and write to the input tensordict\n",
    "    policy_modules[group] = policy_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    policy = ProbabilisticActor(\n",
    "        module=policy_modules[group],\n",
    "        spec=env.full_action_spec[group, \"action\"],\n",
    "        in_keys=[(group, \"param\")],\n",
    "        out_keys=[(group, \"action\")],\n",
    "        distribution_class=TanhDelta,\n",
    "        distribution_kwargs={\n",
    "            \"low\": env.full_action_spec[group, \"action\"].space.low,\n",
    "            \"high\": env.full_action_spec[group, \"action\"].space.high,\n",
    "        },\n",
    "        return_log_prob=False,\n",
    "    )\n",
    "    policies[group] = policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_policies = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    exploration_policy = TensorDictSequential(\n",
    "        policies[group],\n",
    "        AdditiveGaussianModule(\n",
    "            spec=policies[group].spec,\n",
    "            annealing_num_steps=total_frames\n",
    "            // 2,  # Number of frames after which sigma is sigma_end\n",
    "            action_key=(group, \"action\"),\n",
    "            sigma_init=0.4,  # Initial value of the sigma\n",
    "            sigma_end=0.1,  # Final value of the sigma\n",
    "        ),\n",
    "    )\n",
    "    exploration_policies[group] = exploration_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "critics = {}\n",
    "for group, agents in env.group_map.items():\n",
    "    share_parameters_critic = True  # Can change for each group\n",
    "    MADDPG = True  # IDDPG if False, can change for each group\n",
    "\n",
    "    # This module applies the lambda function: reading the action and observation entries for the group\n",
    "    # and concatenating them in a new ``(group, \"obs_action\")`` entry\n",
    "    cat_module = TensorDictModule(\n",
    "        lambda obs, action: torch.cat([obs, action], dim=-1),\n",
    "        in_keys=[(group, \"observation\"), (group, \"action\")],\n",
    "        out_keys=[(group, \"obs_action\")],\n",
    "    )\n",
    "\n",
    "    critic_module = TensorDictModule(\n",
    "        module=MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[group, \"observation\"].shape[-1]\n",
    "            + env.full_action_spec[group, \"action\"].shape[-1],\n",
    "            n_agent_outputs=1,  # 1 value per agent\n",
    "            n_agents=len(agents),\n",
    "            centralised=MADDPG,\n",
    "            share_params=share_parameters_critic,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "        ),\n",
    "        in_keys=[(group, \"obs_action\")],  # Read ``(group, \"obs_action\")``\n",
    "        out_keys=[\n",
    "            (group, \"state_action_value\")\n",
    "        ],  # Write ``(group, \"state_action_value\")``\n",
    "    )\n",
    "\n",
    "    critics[group] = TensorDictSequential(\n",
    "        cat_module, critic_module\n",
    "    )  # Run them in sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running value and policy for group 'agents': TensorDict(\n",
      "    fields={\n",
      "        agents: TensorDict(\n",
      "            fields={\n",
      "                action: Tensor(shape=torch.Size([10, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                episode_reward: Tensor(shape=torch.Size([10, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                obs_action: Tensor(shape=torch.Size([10, 2, 12]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                observation: Tensor(shape=torch.Size([10, 2, 10]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                param: Tensor(shape=torch.Size([10, 2, 2]), device=cuda:0, dtype=torch.float32, is_shared=True),\n",
      "                state_action_value: Tensor(shape=torch.Size([10, 2, 1]), device=cuda:0, dtype=torch.float32, is_shared=True)},\n",
      "            batch_size=torch.Size([10, 2]),\n",
      "            device=cuda:0,\n",
      "            is_shared=True),\n",
      "        done: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n",
      "        terminated: Tensor(shape=torch.Size([10, 1]), device=cuda:0, dtype=torch.bool, is_shared=True)},\n",
      "    batch_size=torch.Size([10]),\n",
      "    device=cuda:0,\n",
      "    is_shared=True)\n"
     ]
    }
   ],
   "source": [
    "reset_td = env.reset()\n",
    "for group, _agents in env.group_map.items():\n",
    "    print(\n",
    "        f\"Running value and policy for group '{group}':\",\n",
    "        critics[group](policies[group](reset_td)),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put exploration policies from each group in a sequence\n",
    "agents_exploration_policy = TensorDictSequential(*exploration_policies.values())\n",
    "\n",
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    agents_exploration_policy,\n",
    "    device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard in off policy algos for efficient data collections\n",
    "replay_buffers = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        storage=LazyMemmapStorage(memory_size, device=\"cpu\"),\n",
    "        sampler=RandomSampler(),\n",
    "        batch_size=train_batch_size,\n",
    "    )\n",
    "    replay_buffer.append_transform(lambda batch: batch.to(device))\n",
    "    replay_buffers[group] = replay_buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_plus_prime_const = 0.2\n",
    "w_minus_prime_const = 0.8\n",
    "\n",
    "def u_plus(x):\n",
    "    alpha = 0.7\n",
    "    return torch.pow(x, alpha)\n",
    "\n",
    "def u_minus(x):\n",
    "    alpha = 0.95\n",
    "    lam = 2.5\n",
    "    return lam * torch.pow(-x, alpha)\n",
    "\n",
    "def w_plus_prime(p):\n",
    "    eta = 0.61\n",
    "    return eta * torch.pow(p, eta - 1)\n",
    "\n",
    "def w_minus_prime(p):\n",
    "    eta = 0.69\n",
    "    return eta * torch.pow(p, eta - 1)\n",
    "\n",
    "def compute_phi_linear(R):\n",
    "    \"\"\"\n",
    "    Compute linearized CPT sensitivity:\n",
    "    φ(R) ≈ w'_+(p*) * u^+(R) for R>=0, and -w'_-(p*) * u^-(R) for R<0.\n",
    "    \"\"\"\n",
    "    R = R.view(-1)\n",
    "    v = torch.where(R >= 0, u_plus(R), -u_minus(R))\n",
    "    phi = torch.where(R >= 0, w_plus_prime_const * v, -w_minus_prime_const * v)\n",
    "    return phi.mean()\n",
    "\n",
    "def C_transform(x):\n",
    "    \"\"\"\n",
    "    Simple CPT transformation on one-step return:\n",
    "    C(x) ≈ w'_+(p*) * u^+(x) if x >= 0, else -w'_-(p*) * u^-(x).\n",
    "    \"\"\"\n",
    "    return torch.where(x >= 0, w_plus_prime_const * u_plus(x), -w_minus_prime_const * u_minus(x))\n",
    "\n",
    "def w_approx_aux(L,x):\n",
    "    n = (len(L)+1)//3\n",
    "    breaking_points = L[2*n:]\n",
    "    for i, elt in enumerate(breaking_points):\n",
    "        if x < elt:\n",
    "            return L[2*i]*x + L[2*i+1]\n",
    "    i = n-1\n",
    "    return L[2*i]*x + L[2*i+1]\n",
    "def w_approx(L):\n",
    "    return (lambda x:w_approx_aux(L,x))\n",
    "\n",
    "def compute_cpt_integral(trajectory_utilities, w, batch_size):\n",
    "    \"\"\"\n",
    "    Compute the CPT integral using piecewise integration.\n",
    "    \n",
    "    Args:\n",
    "        trajectory_utilities: a list or 1D tensor of utility values (episode returns).\n",
    "        w: a tuple (function_type, parameters) that defines the weighting function.\n",
    "           (We assume that a function w_approx(parameters) is available.)\n",
    "        batch_size: the number of trajectories in the batch.\n",
    "    \n",
    "    Returns:\n",
    "        The estimated CPT value (a float).\n",
    "    \"\"\"\n",
    "    # Convert to a list of floats if necessary.\n",
    "    if torch.is_tensor(trajectory_utilities):\n",
    "        utilities = trajectory_utilities.tolist()\n",
    "    else:\n",
    "        utilities = [float(x) for x in trajectory_utilities]\n",
    "    \n",
    "    # Determine segmentation points (include 0 plus unique outcomes).\n",
    "    segments = sorted(list(set(utilities + [0])))\n",
    "    \n",
    "    # Create segments for gains (non-negative) and losses (non-positive)\n",
    "    positive_segments = sorted([x for x in segments if x >= 0])\n",
    "    negative_segments = sorted([-x for x in segments if x <= 0])\n",
    "    \n",
    "    res = 0\n",
    "    if len(segments) == 0:\n",
    "        return 0\n",
    "    parameters = w[1]  # extract the parameter list from the tuple\n",
    "    \n",
    "    # Integral over gains\n",
    "    for i in range(len(positive_segments) - 1):\n",
    "        count = sum(1 for x in utilities if x > positive_segments[i])\n",
    "        weight = w_approx(parameters)(count / batch_size)\n",
    "        res += weight * (positive_segments[i+1] - positive_segments[i])\n",
    "    \n",
    "    # Integral over losses\n",
    "    for i in range(len(negative_segments) - 1):\n",
    "        count = sum(1 for x in utilities if -x > negative_segments[i])\n",
    "        weight = 1 - w_approx(parameters)(1 - count / batch_size)\n",
    "        res -= weight * (negative_segments[i+1] - negative_segments[i])\n",
    "    \n",
    "    return torch.tensor(res, dtype=torch.float32, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from tensordict import TensorDict, TensorDictBase, TensorDictParams\n",
    "from tensordict.nn import dispatch, TensorDictModule\n",
    "\n",
    "from tensordict.utils import NestedKey, unravel_key\n",
    "from torchrl.modules.tensordict_module.actors import ActorCriticWrapper\n",
    "from torchrl.objectives.common import LossModule\n",
    "from torchrl.objectives.utils import (\n",
    "    _cache_values,\n",
    "    _GAMMA_LMBDA_DEPREC_ERROR,\n",
    "    _reduce,\n",
    "    default_value_kwargs,\n",
    "    distance_loss,\n",
    "    ValueEstimators,\n",
    ")\n",
    "from torchrl.objectives.value import TD0Estimator, TD1Estimator, TDLambdaEstimator\n",
    "\n",
    "\n",
    "class CPTDDPGLoss(LossModule):\n",
    "    \"\"\"The DDPG Loss class.\n",
    "\n",
    "    Args:\n",
    "        actor_network (TensorDictModule): a policy operator.\n",
    "        value_network (TensorDictModule): a Q value operator.\n",
    "        loss_function (str): loss function for the value discrepancy. Can be one of \"l1\", \"l2\" or \"smooth_l1\".\n",
    "        delay_actor (bool, optional): whether to separate the target actor networks from the actor networks used for\n",
    "            data collection. Default is ``False``.\n",
    "        delay_value (bool, optional): whether to separate the target value networks from the value networks used for\n",
    "            data collection. Default is ``True``.\n",
    "        separate_losses (bool, optional): if ``True``, shared parameters between\n",
    "            policy and critic will only be trained on the policy loss.\n",
    "            Defaults to ``False``, i.e., gradients are propagated to shared\n",
    "            parameters for both policy and critic losses.\n",
    "        reduction (str, optional): Specifies the reduction to apply to the output:\n",
    "            ``\"none\"`` | ``\"mean\"`` | ``\"sum\"``. ``\"none\"``: no reduction will be applied,\n",
    "            ``\"mean\"``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``\"sum\"``: the output will be summed. Default: ``\"mean\"``.\n",
    "\n",
    "    Examples:\n",
    "        >>> import torch\n",
    "        >>> from torch import nn\n",
    "        >>> from torchrl.data import Bounded\n",
    "        >>> from torchrl.modules.tensordict_module.actors import Actor, ValueOperator\n",
    "        >>> from torchrl.objectives.ddpg import DDPGLoss\n",
    "        >>> from tensordict import TensorDict\n",
    "        >>> n_act, n_obs = 4, 3\n",
    "        >>> spec = Bounded(-torch.ones(n_act), torch.ones(n_act), (n_act,))\n",
    "        >>> actor = Actor(spec=spec, module=nn.Linear(n_obs, n_act))\n",
    "        >>> class ValueClass(nn.Module):\n",
    "        ...     def __init__(self):\n",
    "        ...         super().__init__()\n",
    "        ...         self.linear = nn.Linear(n_obs + n_act, 1)\n",
    "        ...     def forward(self, obs, act):\n",
    "        ...         return self.linear(torch.cat([obs, act], -1))\n",
    "        >>> module = ValueClass()\n",
    "        >>> value = ValueOperator(\n",
    "        ...     module=module,\n",
    "        ...     in_keys=[\"observation\", \"action\"])\n",
    "        >>> loss = DDPGLoss(actor, value)\n",
    "        >>> batch = [2, ]\n",
    "        >>> data = TensorDict({\n",
    "        ...        \"observation\": torch.randn(*batch, n_obs),\n",
    "        ...        \"action\": spec.rand(batch),\n",
    "        ...        (\"next\", \"done\"): torch.zeros(*batch, 1, dtype=torch.bool),\n",
    "        ...        (\"next\", \"terminated\"): torch.zeros(*batch, 1, dtype=torch.bool),\n",
    "        ...        (\"next\", \"reward\"): torch.randn(*batch, 1),\n",
    "        ...        (\"next\", \"observation\"): torch.randn(*batch, n_obs),\n",
    "        ...    }, batch)\n",
    "        >>> loss(data)\n",
    "        TensorDict(\n",
    "            fields={\n",
    "                loss_actor: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "                loss_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "                pred_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "                pred_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "                target_value: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
    "                target_value_max: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
    "            batch_size=torch.Size([]),\n",
    "            device=None,\n",
    "            is_shared=False)\n",
    "\n",
    "    This class is compatible with non-tensordict based modules too and can be\n",
    "    used without recurring to any tensordict-related primitive. In this case,\n",
    "    the expected keyword arguments are:\n",
    "    ``[\"next_reward\", \"next_done\", \"next_terminated\"]`` + in_keys of the actor_network and value_network.\n",
    "    The return value is a tuple of tensors in the following order:\n",
    "    ``[\"loss_actor\", \"loss_value\", \"pred_value\", \"target_value\", \"pred_value_max\", \"target_value_max\"]``\n",
    "\n",
    "    Examples:\n",
    "        >>> import torch\n",
    "        >>> from torch import nn\n",
    "        >>> from torchrl.data import Bounded\n",
    "        >>> from torchrl.modules.tensordict_module.actors import Actor, ValueOperator\n",
    "        >>> from torchrl.objectives.ddpg import DDPGLoss\n",
    "        >>> _ = torch.manual_seed(42)\n",
    "        >>> n_act, n_obs = 4, 3\n",
    "        >>> spec = Bounded(-torch.ones(n_act), torch.ones(n_act), (n_act,))\n",
    "        >>> actor = Actor(spec=spec, module=nn.Linear(n_obs, n_act))\n",
    "        >>> class ValueClass(nn.Module):\n",
    "        ...     def __init__(self):\n",
    "        ...         super().__init__()\n",
    "        ...         self.linear = nn.Linear(n_obs + n_act, 1)\n",
    "        ...     def forward(self, obs, act):\n",
    "        ...         return self.linear(torch.cat([obs, act], -1))\n",
    "        >>> module = ValueClass()\n",
    "        >>> value = ValueOperator(\n",
    "        ...     module=module,\n",
    "        ...     in_keys=[\"observation\", \"action\"])\n",
    "        >>> loss = DDPGLoss(actor, value)\n",
    "        >>> loss_actor, loss_value, pred_value, target_value, pred_value_max, target_value_max = loss(\n",
    "        ...     observation=torch.randn(n_obs),\n",
    "        ...     action=spec.rand(),\n",
    "        ...     next_done=torch.zeros(1, dtype=torch.bool),\n",
    "        ...     next_terminated=torch.zeros(1, dtype=torch.bool),\n",
    "        ...     next_observation=torch.randn(n_obs),\n",
    "        ...     next_reward=torch.randn(1))\n",
    "        >>> loss_actor.backward()\n",
    "\n",
    "    The output keys can also be filtered using the :meth:`DDPGLoss.select_out_keys`\n",
    "    method.\n",
    "\n",
    "    Examples:\n",
    "        >>> loss.select_out_keys('loss_actor', 'loss_value')\n",
    "        >>> loss_actor, loss_value = loss(\n",
    "        ...     observation=torch.randn(n_obs),\n",
    "        ...     action=spec.rand(),\n",
    "        ...     next_done=torch.zeros(1, dtype=torch.bool),\n",
    "        ...     next_terminated=torch.zeros(1, dtype=torch.bool),\n",
    "        ...     next_observation=torch.randn(n_obs),\n",
    "        ...     next_reward=torch.randn(1))\n",
    "        >>> loss_actor.backward()\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @dataclass\n",
    "    class _AcceptedKeys:\n",
    "        \"\"\"Maintains default values for all configurable tensordict keys.\n",
    "\n",
    "        This class defines which tensordict keys can be set using '.set_keys(key_name=key_value)' and their\n",
    "        default values.\n",
    "\n",
    "        Attributes:\n",
    "            state_action_value (NestedKey): The input tensordict key where the\n",
    "                state action value is expected. Will be used for the underlying\n",
    "                value estimator as value key. Defaults to ``\"state_action_value\"``.\n",
    "            priority (NestedKey): The input tensordict key where the target\n",
    "                priority is written to. Defaults to ``\"td_error\"``.\n",
    "            reward (NestedKey): The input tensordict key where the reward is expected.\n",
    "                Will be used for the underlying value estimator. Defaults to ``\"reward\"``.\n",
    "            done (NestedKey): The key in the input TensorDict that indicates\n",
    "                whether a trajectory is done. Will be used for the underlying value estimator.\n",
    "                Defaults to ``\"done\"``.\n",
    "            terminated (NestedKey): The key in the input TensorDict that indicates\n",
    "                whether a trajectory is terminated. Will be used for the underlying value estimator.\n",
    "                Defaults to ``\"terminated\"``.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        state_action_value: NestedKey = \"state_action_value\"\n",
    "        priority: NestedKey = \"td_error\"\n",
    "        reward: NestedKey = \"reward\"\n",
    "        done: NestedKey = \"done\"\n",
    "        terminated: NestedKey = \"terminated\"\n",
    "\n",
    "    tensor_keys: _AcceptedKeys\n",
    "    default_keys = _AcceptedKeys\n",
    "    default_value_estimator: ValueEstimators = ValueEstimators.TD0\n",
    "    out_keys = [\n",
    "        \"loss_actor\",\n",
    "        \"loss_value\",\n",
    "        \"pred_value\",\n",
    "        \"target_value\",\n",
    "        \"pred_value_max\",\n",
    "        \"target_value_max\",\n",
    "    ]\n",
    "\n",
    "    actor_network: TensorDictModule\n",
    "    value_network: actor_network\n",
    "    actor_network_params: TensorDictParams\n",
    "    value_network_params: TensorDictParams\n",
    "    target_actor_network_params: TensorDictParams\n",
    "    target_value_network_params: TensorDictParams\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        actor_network: TensorDictModule,\n",
    "        value_network: TensorDictModule,\n",
    "        *,\n",
    "        loss_function: str = \"l2\",\n",
    "        delay_actor: bool = False,\n",
    "        delay_value: bool = True,\n",
    "        gamma: float = None,\n",
    "        separate_losses: bool = False,\n",
    "        reduction: str = None,\n",
    "        w: tuple = None\n",
    "    ) -> None:\n",
    "        self.w = w\n",
    "        self._in_keys = None\n",
    "        if reduction is None:\n",
    "            reduction = \"mean\"\n",
    "        super().__init__()\n",
    "        self.delay_actor = delay_actor\n",
    "        self.delay_value = delay_value\n",
    "\n",
    "        actor_critic = ActorCriticWrapper(actor_network, value_network)\n",
    "        params = TensorDict.from_module(actor_critic)\n",
    "        params_meta = params.apply(\n",
    "            self._make_meta_params, device=torch.device(\"meta\"), filter_empty=False\n",
    "        )\n",
    "        with params_meta.to_module(actor_critic):\n",
    "            self.__dict__[\"actor_critic\"] = deepcopy(actor_critic)\n",
    "\n",
    "        self.convert_to_functional(\n",
    "            actor_network,\n",
    "            \"actor_network\",\n",
    "            create_target_params=self.delay_actor,\n",
    "        )\n",
    "        if separate_losses:\n",
    "            # we want to make sure there are no duplicates in the params: the\n",
    "            # params of critic must be refs to actor if they're shared\n",
    "            policy_params = list(actor_network.parameters())\n",
    "        else:\n",
    "            policy_params = None\n",
    "        self.convert_to_functional(\n",
    "            value_network,\n",
    "            \"value_network\",\n",
    "            create_target_params=self.delay_value,\n",
    "            compare_against=policy_params,\n",
    "        )\n",
    "        self.actor_critic.module[0] = self.actor_network\n",
    "        self.actor_critic.module[1] = self.value_network\n",
    "\n",
    "        self.actor_in_keys = actor_network.in_keys\n",
    "        self.value_exclusive_keys = set(self.value_network.in_keys) - (\n",
    "            set(self.actor_in_keys) | set(self.actor_network.out_keys)\n",
    "        )\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.reduction = reduction\n",
    "        if gamma is not None:\n",
    "            raise TypeError(_GAMMA_LMBDA_DEPREC_ERROR)\n",
    "\n",
    "    def _forward_value_estimator_keys(self, **kwargs) -> None:\n",
    "        if self._value_estimator is not None:\n",
    "            self._value_estimator.set_keys(\n",
    "                value=self._tensor_keys.state_action_value,\n",
    "                reward=self._tensor_keys.reward,\n",
    "                done=self._tensor_keys.done,\n",
    "                terminated=self._tensor_keys.terminated,\n",
    "            )\n",
    "        self._set_in_keys()\n",
    "\n",
    "    def _set_in_keys(self):\n",
    "        in_keys = {\n",
    "            unravel_key((\"next\", self.tensor_keys.reward)),\n",
    "            unravel_key((\"next\", self.tensor_keys.done)),\n",
    "            unravel_key((\"next\", self.tensor_keys.terminated)),\n",
    "            *self.actor_in_keys,\n",
    "            *[unravel_key((\"next\", key)) for key in self.actor_in_keys],\n",
    "            *self.value_network.in_keys,\n",
    "            *[unravel_key((\"next\", key)) for key in self.value_network.in_keys],\n",
    "        }\n",
    "        self._in_keys = sorted(in_keys, key=str)\n",
    "\n",
    "    @property\n",
    "    def in_keys(self):\n",
    "        if self._in_keys is None:\n",
    "            self._set_in_keys()\n",
    "        return self._in_keys\n",
    "\n",
    "    @in_keys.setter\n",
    "    def in_keys(self, values):\n",
    "        self._in_keys = values\n",
    "\n",
    "\n",
    "    def _clear_weakrefs(self, *tds):\n",
    "        if is_compiling():\n",
    "            # Waiting for weakrefs reconstruct to be supported by compile\n",
    "            for td in tds:\n",
    "                if isinstance(td, str):\n",
    "                    td = getattr(self, td, None)\n",
    "                if not is_tensor_collection(td):\n",
    "                    continue\n",
    "                td.clear_refs_for_compile_()\n",
    "\n",
    "    @dispatch\n",
    "    def forward(self, tensordict: TensorDictBase) -> TensorDict:\n",
    "        \"\"\"Computes the DDPG losses given a tensordict sampled from the replay buffer.\n",
    "\n",
    "        This function will also write a \"td_error\" key that can be used by prioritized replay buffers to assign\n",
    "            a priority to items in the tensordict.\n",
    "\n",
    "        Args:\n",
    "            tensordict (TensorDictBase): a tensordict with keys [\"done\", \"terminated\", \"reward\"] and the in_keys of the actor\n",
    "                and value networks.\n",
    "\n",
    "        Returns:\n",
    "            a tuple of 2 tensors containing the DDPG loss.\n",
    "\n",
    "        \"\"\"\n",
    "        loss_value, metadata = self.loss_value(tensordict)\n",
    "        loss_actor, metadata_actor = self.loss_actor(tensordict)\n",
    "        metadata.update(metadata_actor)\n",
    "        td_out = TensorDict(\n",
    "            source={\"loss_actor\": loss_actor, \"loss_value\": loss_value, **metadata},\n",
    "            batch_size=[],\n",
    "        )\n",
    "        self._clear_weakrefs(\n",
    "            tensordict,\n",
    "            td_out,\n",
    "            \"value_network_params\",\n",
    "            \"target_value_network_params\",\n",
    "            \"target_actor_network_params\",\n",
    "            \"actor_network_params\",\n",
    "        )\n",
    "        return td_out\n",
    "\n",
    "    def loss_actor(self, tensordict: TensorDictBase) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"Compute the CPT-modified actor loss.\"\"\"\n",
    "\n",
    "        td_copy = tensordict.select(\n",
    "            *self.actor_in_keys, *self.value_exclusive_keys, strict=False\n",
    "        ).detach()\n",
    "\n",
    "        with self.actor_network_params.to_module(self.actor_network):\n",
    "            td_copy = self.actor_network(td_copy)\n",
    "\n",
    "        with self._cached_detached_value_params.to_module(self.value_network):\n",
    "            td_copy = self.value_network(td_copy)\n",
    "\n",
    "        # Get action chosen by policy: μ(o)\n",
    "        actions = td_copy.get((self.actor_network.in_keys[0][0], \"action\"))\n",
    "\n",
    "        # Compute Q-value (state-action value) for current policy\n",
    "        Q_values = td_copy.get(self.tensor_keys.state_action_value).squeeze(-1)\n",
    "\n",
    "        # Compute Policy Gradient: ∇_a Q(s, a) at a = μ(o)\n",
    "        grad_Q = torch.autograd.grad(Q_values.sum(), actions, retain_graph=True)[0]  # (batch_size, action_dim)\n",
    "\n",
    "        # Compute CPT-derived adjustment (phi transformation)\n",
    "        group = self.actor_network.in_keys[0][0]  # Extracts the group key (e.g., \"adversary\")\n",
    "        returns = tensordict.get((group, \"episode_reward\")).view(-1)  # Sum of rewards over episode\n",
    "        # phi_factor = compute_cpt_integral(returns, self.w, returns.numel())\n",
    "        # # print(\n",
    "        # #     f\"DEBUG phi_factor → type: {type(phi_factor)}, \"\n",
    "        # #     f\"value: {phi_factor}, \"\n",
    "        # #     f\"is_tensor: {isinstance(phi_factor, torch.Tensor)}\"\n",
    "        # # )\n",
    "        # phi_factor = phi_factor.abs()\n",
    "        phi_factor = compute_cpt_integral(returns, self.w, returns.numel())\n",
    "        phi_factor = phi_factor.abs()\n",
    "        policy_gradient = actions * grad_Q  # Chain rule term: ∇_θ μ(o) ∇_a Q(s, a)\n",
    "        loss_actor = -phi_factor * policy_gradient.mean()  # CPT-adjusted loss\n",
    "\n",
    "        return _reduce(loss_actor, self.reduction), {}\n",
    "\n",
    "    def loss_value(self, tensordict: TensorDictBase) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"Compute the CPT-modified critic loss.\"\"\"\n",
    "        td_copy = tensordict.select(*self.value_network.in_keys, strict=False).detach()\n",
    "        with self.value_network_params.to_module(self.value_network):\n",
    "            self.value_network(td_copy)\n",
    "        pred_val = td_copy.get(self.tensor_keys.state_action_value).squeeze(-1)\n",
    "\n",
    "        # Standard target value calculation\n",
    "        target_value = self.value_estimator.value_estimate(\n",
    "            tensordict, target_params=self._cached_target_params\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Apply CPT transformation\n",
    "        target_value_CPT = C_transform(target_value)\n",
    "\n",
    "        # Compute CPT critic loss\n",
    "        loss_value = distance_loss(pred_val, target_value_CPT, loss_function=self.loss_function)\n",
    "\n",
    "        # Update tensor dictionary with CPT-transformed target\n",
    "        tensordict.set(\"target_value_CPT\", target_value_CPT, inplace=True)\n",
    "\n",
    "        # Compute TD error for prioritized replay buffer\n",
    "        td_error = (pred_val - target_value_CPT).pow(2).detach()\n",
    "        tensordict.set(self.tensor_keys.priority, td_error, inplace=True)\n",
    "\n",
    "        metadata = {\n",
    "            \"td_error\": td_error,\n",
    "            \"pred_value\": pred_val,\n",
    "            \"target_value_CPT\": target_value_CPT,\n",
    "        }\n",
    "        return _reduce(loss_value, self.reduction), metadata\n",
    "\n",
    "    def make_value_estimator(self, value_type: ValueEstimators = None, **hyperparams):\n",
    "        if value_type is None:\n",
    "            value_type = self.default_value_estimator\n",
    "        self.value_type = value_type\n",
    "        hp = dict(default_value_kwargs(value_type))\n",
    "        if hasattr(self, \"gamma\"):\n",
    "            hp[\"gamma\"] = self.gamma\n",
    "        hp.update(hyperparams)\n",
    "        if value_type == ValueEstimators.TD1:\n",
    "            self._value_estimator = TD1Estimator(value_network=self.actor_critic, **hp)\n",
    "        elif value_type == ValueEstimators.TD0:\n",
    "            self._value_estimator = TD0Estimator(value_network=self.actor_critic, **hp)\n",
    "        elif value_type == ValueEstimators.GAE:\n",
    "            raise NotImplementedError(\n",
    "                f\"Value type {value_type} it not implemented for loss {type(self)}.\"\n",
    "            )\n",
    "        elif value_type == ValueEstimators.TDLambda:\n",
    "            self._value_estimator = TDLambdaEstimator(\n",
    "                value_network=self.actor_critic, **hp\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown value type {value_type}\")\n",
    "\n",
    "        tensor_keys = {\n",
    "            \"value\": self.tensor_keys.state_action_value,\n",
    "            \"reward\": self.tensor_keys.reward,\n",
    "            \"done\": self.tensor_keys.done,\n",
    "            \"terminated\": self.tensor_keys.terminated,\n",
    "        }\n",
    "        self._value_estimator.set_keys(**tensor_keys)\n",
    "\n",
    "    @property\n",
    "    @_cache_values\n",
    "    def _cached_target_params(self):\n",
    "        target_params = TensorDict(\n",
    "            {\n",
    "                \"module\": {\n",
    "                    \"0\": self.target_actor_network_params,\n",
    "                    \"1\": self.target_value_network_params,\n",
    "                }\n",
    "            },\n",
    "            batch_size=self.target_actor_network_params.batch_size,\n",
    "            device=self.target_actor_network_params.device,\n",
    "        )\n",
    "        return target_params\n",
    "\n",
    "    @property\n",
    "    @_cache_values\n",
    "    def _cached_detached_value_params(self):\n",
    "        return self.value_network_params.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "for group, _agents in env.group_map.items():\n",
    "    loss_module = CPTDDPGLoss(\n",
    "        actor_network=policies[group],  # Use the non-explorative policies\n",
    "        value_network=critics[group],\n",
    "        delay_value=True,  # Whether to use a target network for the value\n",
    "        loss_function=\"l2\",\n",
    "        w=(0, [ 1.94671279,  0.,  0.69678392,  0.11132902,  2.70977837,\n",
    "       -1.70977837,  0.08906833,  0.90467621])\n",
    "    )\n",
    "    loss_module.set_keys(\n",
    "        state_action_value=(group, \"state_action_value\"),\n",
    "        reward=(group, \"reward\"),\n",
    "        done=(group, \"done\"),\n",
    "        terminated=(group, \"terminated\"),\n",
    "    )\n",
    "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
    "\n",
    "    losses[group] = loss_module\n",
    "\n",
    "target_updaters = {\n",
    "    group: SoftUpdate(loss, tau=polyak_tau) for group, loss in losses.items()\n",
    "}\n",
    "\n",
    "optimisers = {\n",
    "    group: {\n",
    "        \"loss_actor\": torch.optim.Adam(\n",
    "            loss.actor_network_params.flatten_keys().values(), lr=lr\n",
    "        ),\n",
    "        \"loss_value\": torch.optim.Adam(\n",
    "            loss.value_network_params.flatten_keys().values(), lr=lr\n",
    "        ),\n",
    "    }\n",
    "    for group, loss in losses.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch: TensorDictBase) -> TensorDictBase:\n",
    "    \"\"\"\n",
    "    If the `(group, \"terminated\")` and `(group, \"done\")` keys are not present, create them by expanding\n",
    "    `\"terminated\"` and `\"done\"`.\n",
    "    This is needed to present them with the same shape as the reward to the loss.\n",
    "    \"\"\"\n",
    "    for group in env.group_map.keys():\n",
    "        keys = list(batch.keys(True, True))\n",
    "        group_shape = batch.get_item_shape(group)\n",
    "        nested_done_key = (\"next\", group, \"done\")\n",
    "        nested_terminated_key = (\"next\", group, \"terminated\")\n",
    "        if nested_done_key not in keys:\n",
    "            batch.set(\n",
    "                nested_done_key,\n",
    "                batch.get((\"next\", \"done\")).unsqueeze(-1).expand((*group_shape, 1)),\n",
    "            )\n",
    "        if nested_terminated_key not in keys:\n",
    "            batch.set(\n",
    "                nested_terminated_key,\n",
    "                batch.get((\"next\", \"terminated\"))\n",
    "                .unsqueeze(-1)\n",
    "                .expand((*group_shape, 1)),\n",
    "            )\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "episode_reward_mean_agents = -382.0019226074219:   4%|▍         | 21/500 [00:39<15:01,  1.88s/it]\n",
      "\n",
      "episode_reward_mean_agents = -363.1314392089844:   0%|          | 1/500 [00:01<11:34,  1.39s/it]\u001b[A\n",
      "episode_reward_mean_agents = -674.0586547851562:   0%|          | 2/500 [00:02<11:33,  1.39s/it]\u001b[A\n",
      "episode_reward_mean_agents = -615.23388671875:   1%|          | 3/500 [00:04<11:33,  1.40s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -639.3682250976562:   1%|          | 4/500 [00:05<12:07,  1.47s/it]\u001b[A\n",
      "episode_reward_mean_agents = -928.3449096679688:   1%|          | 5/500 [00:07<11:52,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -875.8380737304688:   1%|          | 6/500 [00:08<11:45,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -687.8705444335938:   1%|▏         | 7/500 [00:09<11:37,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -796.4594116210938:   2%|▏         | 8/500 [00:11<11:34,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -899.0021362304688:   2%|▏         | 9/500 [00:12<11:32,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -552.8582153320312:   2%|▏         | 10/500 [00:14<11:32,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -510.4913024902344:   2%|▏         | 11/500 [00:15<11:29,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -302.21917724609375:   2%|▏         | 12/500 [00:16<11:28,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -752.63427734375:   3%|▎         | 13/500 [00:18<11:26,  1.41s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -756.8345336914062:   3%|▎         | 14/500 [00:19<11:25,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -674.1351928710938:   3%|▎         | 15/500 [00:21<11:24,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -599.1852416992188:   3%|▎         | 16/500 [00:22<11:23,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -615.5262451171875:   3%|▎         | 17/500 [00:24<11:20,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -439.5451354980469:   4%|▎         | 18/500 [00:25<11:18,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -515.219970703125:   4%|▍         | 19/500 [00:26<11:16,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -668.8800048828125:   4%|▍         | 20/500 [00:28<11:14,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -768.5877075195312:   4%|▍         | 21/500 [00:29<11:12,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -685.15966796875:   4%|▍         | 22/500 [00:31<11:13,  1.41s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -829.4929809570312:   5%|▍         | 23/500 [00:32<11:10,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -698.1800537109375:   5%|▍         | 24/500 [00:33<11:10,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -647.2589721679688:   5%|▌         | 25/500 [00:35<11:07,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -636.4677734375:   5%|▌         | 26/500 [00:36<11:06,  1.41s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -614.3170776367188:   5%|▌         | 27/500 [00:38<11:04,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -694.6366577148438:   6%|▌         | 28/500 [00:39<11:03,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -640.5853881835938:   6%|▌         | 29/500 [00:40<11:03,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -609.3602905273438:   6%|▌         | 30/500 [00:42<11:04,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -563.0497436523438:   6%|▌         | 31/500 [00:43<11:03,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -657.2059936523438:   6%|▋         | 32/500 [00:45<11:02,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -673.8414916992188:   7%|▋         | 33/500 [00:46<11:00,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -707.7975463867188:   7%|▋         | 34/500 [00:48<10:58,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -693.3226928710938:   7%|▋         | 35/500 [00:49<10:57,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -714.3289184570312:   7%|▋         | 36/500 [00:50<10:56,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -628.63330078125:   7%|▋         | 37/500 [00:52<10:53,  1.41s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -585.3609619140625:   8%|▊         | 38/500 [00:53<10:49,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -251.5739288330078:   8%|▊         | 39/500 [00:55<10:47,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -325.5594177246094:   8%|▊         | 40/500 [00:56<10:46,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -380.07476806640625:   8%|▊         | 41/500 [00:57<10:43,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -406.2815856933594:   8%|▊         | 42/500 [00:59<10:43,  1.40s/it] \u001b[A\n",
      "episode_reward_mean_agents = -378.8387756347656:   9%|▊         | 43/500 [01:00<10:40,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -330.5510559082031:   9%|▉         | 44/500 [01:02<10:41,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -307.0458679199219:   9%|▉         | 45/500 [01:03<10:39,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -408.4517517089844:   9%|▉         | 46/500 [01:04<10:39,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -623.1549072265625:   9%|▉         | 47/500 [01:06<10:39,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -724.2107543945312:  10%|▉         | 48/500 [01:07<10:38,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -640.9275512695312:  10%|▉         | 49/500 [01:09<10:35,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -692.0451049804688:  10%|█         | 50/500 [01:10<10:33,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -627.0203247070312:  10%|█         | 51/500 [01:11<10:31,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -803.2608642578125:  10%|█         | 52/500 [01:13<10:30,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -548.3767700195312:  11%|█         | 53/500 [01:14<10:29,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -509.8880920410156:  11%|█         | 54/500 [01:16<10:28,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -496.06201171875:  11%|█         | 55/500 [01:17<10:25,  1.41s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -581.8577270507812:  11%|█         | 56/500 [01:18<10:22,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -581.0435791015625:  11%|█▏        | 57/500 [01:20<10:22,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -497.4676818847656:  12%|█▏        | 58/500 [01:21<10:21,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -516.740234375:  12%|█▏        | 59/500 [01:23<10:19,  1.41s/it]    \u001b[A\n",
      "episode_reward_mean_agents = -610.921875:  12%|█▏        | 60/500 [01:24<10:18,  1.40s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -690.2784423828125:  12%|█▏        | 61/500 [01:25<10:16,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -657.4072265625:  12%|█▏        | 62/500 [01:27<10:15,  1.40s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -583.1359252929688:  13%|█▎        | 63/500 [01:28<10:13,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -604.6491088867188:  13%|█▎        | 64/500 [01:30<10:11,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -740.1377563476562:  13%|█▎        | 65/500 [01:31<10:09,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -643.0206298828125:  13%|█▎        | 66/500 [01:33<10:30,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -660.3203735351562:  13%|█▎        | 67/500 [01:34<10:21,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -652.0642700195312:  14%|█▎        | 68/500 [01:35<10:17,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -490.95513916015625:  14%|█▍        | 69/500 [01:37<10:12,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -362.0499572753906:  14%|█▍        | 70/500 [01:38<10:07,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -319.945068359375:  14%|█▍        | 71/500 [01:40<10:03,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -362.4544677734375:  14%|█▍        | 72/500 [01:41<10:02,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -338.2677001953125:  15%|█▍        | 73/500 [01:42<10:00,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -297.5948791503906:  15%|█▍        | 74/500 [01:44<09:58,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -297.8341979980469:  15%|█▌        | 75/500 [01:45<09:55,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -263.8027038574219:  15%|█▌        | 76/500 [01:47<09:53,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -251.2448272705078:  15%|█▌        | 77/500 [01:48<09:51,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -306.4337463378906:  16%|█▌        | 78/500 [01:49<09:51,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -371.16436767578125:  16%|█▌        | 79/500 [01:51<09:49,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -376.4817810058594:  16%|█▌        | 80/500 [01:52<09:48,  1.40s/it] \u001b[A\n",
      "episode_reward_mean_agents = -337.0750427246094:  16%|█▌        | 81/500 [01:54<09:46,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -423.40631103515625:  16%|█▋        | 82/500 [01:55<09:44,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -353.102294921875:  17%|█▋        | 83/500 [01:56<09:42,  1.40s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -436.1288146972656:  17%|█▋        | 84/500 [01:58<09:41,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -431.421875:  17%|█▋        | 85/500 [01:59<09:39,  1.40s/it]       \u001b[A\n",
      "episode_reward_mean_agents = -325.0615539550781:  17%|█▋        | 86/500 [02:01<09:37,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -346.84820556640625:  17%|█▋        | 87/500 [02:02<09:37,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -276.8975830078125:  18%|█▊        | 88/500 [02:03<09:35,  1.40s/it] \u001b[A\n",
      "episode_reward_mean_agents = -265.35235595703125:  18%|█▊        | 89/500 [02:05<09:33,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -276.70111083984375:  18%|█▊        | 90/500 [02:06<09:32,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -269.1007080078125:  18%|█▊        | 91/500 [02:08<09:34,  1.40s/it] \u001b[A\n",
      "episode_reward_mean_agents = -226.1287841796875:  18%|█▊        | 92/500 [02:09<09:34,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -249.534912109375:  19%|█▊        | 93/500 [02:10<09:34,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -283.080078125:  19%|█▉        | 94/500 [02:12<09:31,  1.41s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -217.5324249267578:  19%|█▉        | 95/500 [02:13<09:28,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -236.23081970214844:  19%|█▉        | 96/500 [02:15<09:26,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -225.6719512939453:  19%|█▉        | 97/500 [02:16<09:24,  1.40s/it] \u001b[A\n",
      "episode_reward_mean_agents = -234.0694122314453:  20%|█▉        | 98/500 [02:17<09:23,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -235.49356079101562:  20%|█▉        | 99/500 [02:19<09:22,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -233.8702392578125:  20%|██        | 100/500 [02:20<09:20,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -238.98484802246094:  20%|██        | 101/500 [02:22<09:18,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -184.53378295898438:  20%|██        | 102/500 [02:23<09:17,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -210.178466796875:  21%|██        | 103/500 [02:24<09:16,  1.40s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -253.97227478027344:  21%|██        | 104/500 [02:26<09:17,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -230.1151580810547:  21%|██        | 105/500 [02:27<09:16,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -237.4310302734375:  21%|██        | 106/500 [02:29<09:17,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -237.842041015625:  21%|██▏       | 107/500 [02:30<09:16,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -280.7709045410156:  22%|██▏       | 108/500 [02:32<09:16,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -264.43511962890625:  22%|██▏       | 109/500 [02:33<09:17,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -281.7458801269531:  22%|██▏       | 110/500 [02:34<09:16,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -236.5369415283203:  22%|██▏       | 111/500 [02:36<09:12,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -294.77685546875:  22%|██▏       | 112/500 [02:37<09:10,  1.42s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -312.16802978515625:  23%|██▎       | 113/500 [02:39<09:10,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -286.1820373535156:  23%|██▎       | 114/500 [02:40<09:11,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -309.1930236816406:  23%|██▎       | 115/500 [02:42<09:11,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -320.0402526855469:  23%|██▎       | 116/500 [02:43<09:11,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -270.27178955078125:  23%|██▎       | 117/500 [02:44<09:09,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -336.222900390625:  24%|██▎       | 118/500 [02:46<09:09,  1.44s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -329.9755554199219:  24%|██▍       | 119/500 [02:47<09:06,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -273.6084899902344:  24%|██▍       | 120/500 [02:49<09:05,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -304.6615905761719:  24%|██▍       | 121/500 [02:50<09:03,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -318.00274658203125:  24%|██▍       | 122/500 [02:52<08:59,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -285.7718200683594:  25%|██▍       | 123/500 [02:53<08:54,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -252.0906219482422:  25%|██▍       | 124/500 [02:54<08:52,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -264.0481872558594:  25%|██▌       | 125/500 [02:56<08:49,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -271.263427734375:  25%|██▌       | 126/500 [02:57<08:48,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -258.7279357910156:  25%|██▌       | 127/500 [02:59<08:47,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -256.5343322753906:  26%|██▌       | 128/500 [03:00<08:47,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -256.1104431152344:  26%|██▌       | 129/500 [03:02<09:05,  1.47s/it]\u001b[A\n",
      "episode_reward_mean_agents = -256.3121032714844:  26%|██▌       | 130/500 [03:03<08:58,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -257.318603515625:  26%|██▌       | 131/500 [03:05<08:52,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -235.84902954101562:  26%|██▋       | 132/500 [03:06<08:47,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -270.7106628417969:  27%|██▋       | 133/500 [03:07<08:42,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -263.0828552246094:  27%|██▋       | 134/500 [03:09<08:38,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -281.2971496582031:  27%|██▋       | 135/500 [03:10<08:36,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -294.1187744140625:  27%|██▋       | 136/500 [03:12<08:33,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -255.80947875976562:  27%|██▋       | 137/500 [03:13<08:31,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -269.5205993652344:  28%|██▊       | 138/500 [03:14<08:28,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -290.5938720703125:  28%|██▊       | 139/500 [03:16<08:28,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -304.2187805175781:  28%|██▊       | 140/500 [03:17<08:34,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -296.267333984375:  28%|██▊       | 141/500 [03:19<08:33,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -280.1617126464844:  28%|██▊       | 142/500 [03:20<08:33,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -299.505126953125:  29%|██▊       | 143/500 [03:22<08:28,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -287.6842346191406:  29%|██▉       | 144/500 [03:23<08:23,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -344.244140625:  29%|██▉       | 145/500 [03:24<08:19,  1.41s/it]    \u001b[A\n",
      "episode_reward_mean_agents = -403.0528869628906:  29%|██▉       | 146/500 [03:26<08:16,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -387.58551025390625:  29%|██▉       | 147/500 [03:27<08:14,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -314.7077331542969:  30%|██▉       | 148/500 [03:28<08:13,  1.40s/it] \u001b[A\n",
      "episode_reward_mean_agents = -371.1988830566406:  30%|██▉       | 149/500 [03:30<08:11,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -363.5653991699219:  30%|███       | 150/500 [03:31<08:09,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -320.4029846191406:  30%|███       | 151/500 [03:33<08:08,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -304.5364685058594:  30%|███       | 152/500 [03:34<08:07,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -310.1700134277344:  31%|███       | 153/500 [03:35<08:05,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -340.92059326171875:  31%|███       | 154/500 [03:37<08:07,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -261.71710205078125:  31%|███       | 155/500 [03:38<08:09,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -310.111572265625:  31%|███       | 156/500 [03:40<08:09,  1.42s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -296.8946533203125:  31%|███▏      | 157/500 [03:41<08:09,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -265.0751647949219:  32%|███▏      | 158/500 [03:43<08:10,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -302.0903625488281:  32%|███▏      | 159/500 [03:44<08:10,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -269.6194763183594:  32%|███▏      | 160/500 [03:46<08:08,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -246.213134765625:  32%|███▏      | 161/500 [03:47<08:08,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -188.64781188964844:  32%|███▏      | 162/500 [03:48<08:06,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -216.16519165039062:  33%|███▎      | 163/500 [03:50<08:03,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -226.20068359375:  33%|███▎      | 164/500 [03:51<08:02,  1.44s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -204.8251190185547:  33%|███▎      | 165/500 [03:53<08:00,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -217.28271484375:  33%|███▎      | 166/500 [03:54<07:55,  1.42s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -210.6943817138672:  33%|███▎      | 167/500 [03:56<07:51,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -165.3431854248047:  34%|███▎      | 168/500 [03:57<07:48,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -210.84060668945312:  34%|███▍      | 169/500 [03:58<07:44,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -213.65489196777344:  34%|███▍      | 170/500 [04:00<07:42,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -237.8736114501953:  34%|███▍      | 171/500 [04:01<07:41,  1.40s/it] \u001b[A\n",
      "episode_reward_mean_agents = -178.34153747558594:  34%|███▍      | 172/500 [04:03<07:39,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -192.7257843017578:  35%|███▍      | 173/500 [04:04<07:38,  1.40s/it] \u001b[A\n",
      "episode_reward_mean_agents = -178.3828887939453:  35%|███▍      | 174/500 [04:05<07:36,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -174.75961303710938:  35%|███▌      | 175/500 [04:07<07:38,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -190.84652709960938:  35%|███▌      | 176/500 [04:08<07:39,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -233.8845672607422:  35%|███▌      | 177/500 [04:10<07:36,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -266.1352233886719:  36%|███▌      | 178/500 [04:11<07:32,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -240.95066833496094:  36%|███▌      | 179/500 [04:12<07:33,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -227.04173278808594:  36%|███▌      | 180/500 [04:14<07:34,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -215.2070770263672:  36%|███▌      | 181/500 [04:15<07:37,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -202.3286895751953:  36%|███▋      | 182/500 [04:17<07:34,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -203.87298583984375:  37%|███▋      | 183/500 [04:18<07:32,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -214.15000915527344:  37%|███▋      | 184/500 [04:20<07:28,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -264.4610595703125:  37%|███▋      | 185/500 [04:21<07:25,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -202.6200408935547:  37%|███▋      | 186/500 [04:22<07:22,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -285.7594909667969:  37%|███▋      | 187/500 [04:24<07:21,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -253.89016723632812:  38%|███▊      | 188/500 [04:25<07:18,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -213.5262908935547:  38%|███▊      | 189/500 [04:27<07:17,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -207.9173126220703:  38%|███▊      | 190/500 [04:28<07:19,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -241.739013671875:  38%|███▊      | 191/500 [04:29<07:19,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -234.85935974121094:  38%|███▊      | 192/500 [04:31<07:34,  1.48s/it]\u001b[A\n",
      "episode_reward_mean_agents = -244.554443359375:  39%|███▊      | 193/500 [04:33<07:30,  1.47s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -193.38455200195312:  39%|███▉      | 194/500 [04:34<07:26,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -229.80235290527344:  39%|███▉      | 195/500 [04:35<07:23,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -276.8580627441406:  39%|███▉      | 196/500 [04:37<07:19,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -233.289794921875:  39%|███▉      | 197/500 [04:38<07:16,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -256.5577697753906:  40%|███▉      | 198/500 [04:40<07:15,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -261.3799743652344:  40%|███▉      | 199/500 [04:41<07:14,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -270.0332946777344:  40%|████      | 200/500 [04:43<07:12,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -320.8587951660156:  40%|████      | 201/500 [04:44<07:10,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -240.5651092529297:  40%|████      | 202/500 [04:45<07:08,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -238.30029296875:  41%|████      | 203/500 [04:47<07:06,  1.44s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -284.66290283203125:  41%|████      | 204/500 [04:48<07:05,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -272.224365234375:  41%|████      | 205/500 [04:50<07:03,  1.44s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -299.9310607910156:  41%|████      | 206/500 [04:51<07:04,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -248.6597137451172:  41%|████▏     | 207/500 [04:53<07:02,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -295.3432922363281:  42%|████▏     | 208/500 [04:54<07:00,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -273.4830017089844:  42%|████▏     | 209/500 [04:56<06:59,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -264.7764587402344:  42%|████▏     | 210/500 [04:57<06:57,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -298.5461120605469:  42%|████▏     | 211/500 [04:58<06:55,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -279.5682678222656:  42%|████▏     | 212/500 [05:00<06:53,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -322.3954772949219:  43%|████▎     | 213/500 [05:01<06:52,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -325.1775817871094:  43%|████▎     | 214/500 [05:03<06:55,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -255.3806610107422:  43%|████▎     | 215/500 [05:04<06:53,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -307.7547912597656:  43%|████▎     | 216/500 [05:06<06:50,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -304.560546875:  43%|████▎     | 217/500 [05:07<06:48,  1.44s/it]    \u001b[A\n",
      "episode_reward_mean_agents = -274.3682861328125:  44%|████▎     | 218/500 [05:09<06:47,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -305.7644958496094:  44%|████▍     | 219/500 [05:10<06:45,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -360.3846435546875:  44%|████▍     | 220/500 [05:11<06:43,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -318.6377868652344:  44%|████▍     | 221/500 [05:13<06:41,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -360.763916015625:  44%|████▍     | 222/500 [05:14<06:40,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -327.6827087402344:  45%|████▍     | 223/500 [05:16<06:39,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -314.8164978027344:  45%|████▍     | 224/500 [05:17<06:38,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -373.3750915527344:  45%|████▌     | 225/500 [05:19<06:37,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -319.74749755859375:  45%|████▌     | 226/500 [05:20<06:35,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -294.86907958984375:  45%|████▌     | 227/500 [05:21<06:33,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -367.37646484375:  46%|████▌     | 228/500 [05:23<06:30,  1.44s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -322.9079284667969:  46%|████▌     | 229/500 [05:24<06:28,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -316.4820861816406:  46%|████▌     | 230/500 [05:26<06:29,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -325.08721923828125:  46%|████▌     | 231/500 [05:27<06:26,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -348.9981994628906:  46%|████▋     | 232/500 [05:29<06:25,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -354.4302978515625:  47%|████▋     | 233/500 [05:30<06:22,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -346.90997314453125:  47%|████▋     | 234/500 [05:32<06:21,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -394.3282165527344:  47%|████▋     | 235/500 [05:33<06:20,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -338.6043395996094:  47%|████▋     | 236/500 [05:34<06:18,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -349.5876770019531:  47%|████▋     | 237/500 [05:36<06:16,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -356.96734619140625:  48%|████▊     | 238/500 [05:37<06:14,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -385.65240478515625:  48%|████▊     | 239/500 [05:39<06:13,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -388.9162902832031:  48%|████▊     | 240/500 [05:40<06:12,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -323.33740234375:  48%|████▊     | 241/500 [05:42<06:11,  1.43s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -343.3388977050781:  48%|████▊     | 242/500 [05:43<06:09,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -354.8632507324219:  49%|████▊     | 243/500 [05:44<06:08,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -336.58685302734375:  49%|████▉     | 244/500 [05:46<06:06,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -334.04547119140625:  49%|████▉     | 245/500 [05:47<06:05,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -378.05230712890625:  49%|████▉     | 246/500 [05:49<06:04,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -348.2213134765625:  49%|████▉     | 247/500 [05:50<06:02,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -352.0901794433594:  50%|████▉     | 248/500 [05:52<06:01,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -334.8959655761719:  50%|████▉     | 249/500 [05:53<05:59,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -323.4853820800781:  50%|█████     | 250/500 [05:54<05:58,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -353.5452575683594:  50%|█████     | 251/500 [05:56<05:56,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -336.7255554199219:  50%|█████     | 252/500 [05:57<05:55,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -304.7521667480469:  51%|█████     | 253/500 [05:59<05:54,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -340.9352722167969:  51%|█████     | 254/500 [06:00<06:05,  1.49s/it]\u001b[A\n",
      "episode_reward_mean_agents = -278.8729553222656:  51%|█████     | 255/500 [06:02<06:00,  1.47s/it]\u001b[A\n",
      "episode_reward_mean_agents = -377.0314025878906:  51%|█████     | 256/500 [06:03<05:56,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -369.03375244140625:  51%|█████▏    | 257/500 [06:05<05:52,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -336.14007568359375:  52%|█████▏    | 258/500 [06:06<05:51,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -367.90576171875:  52%|█████▏    | 259/500 [06:08<05:49,  1.45s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -368.9735412597656:  52%|█████▏    | 260/500 [06:09<05:47,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -357.1623229980469:  52%|█████▏    | 261/500 [06:10<05:46,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -328.5522155761719:  52%|█████▏    | 262/500 [06:12<05:44,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -301.2111511230469:  53%|█████▎    | 263/500 [06:13<05:41,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -347.9908447265625:  53%|█████▎    | 264/500 [06:15<05:41,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -332.9448547363281:  53%|█████▎    | 265/500 [06:16<05:38,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -368.08074951171875:  53%|█████▎    | 266/500 [06:18<05:37,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -369.8809509277344:  53%|█████▎    | 267/500 [06:19<05:35,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -325.6413269042969:  54%|█████▎    | 268/500 [06:21<05:34,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -349.0414733886719:  54%|█████▍    | 269/500 [06:22<05:33,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -367.19610595703125:  54%|█████▍    | 270/500 [06:23<05:32,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -339.3651428222656:  54%|█████▍    | 271/500 [06:25<05:31,  1.45s/it] \u001b[A\n",
      "episode_reward_mean_agents = -323.7730407714844:  54%|█████▍    | 272/500 [06:26<05:29,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -340.9737243652344:  55%|█████▍    | 273/500 [06:28<05:27,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -334.39825439453125:  55%|█████▍    | 274/500 [06:29<05:26,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -324.0022888183594:  55%|█████▌    | 275/500 [06:31<05:25,  1.45s/it] \u001b[A\n",
      "episode_reward_mean_agents = -343.6036071777344:  55%|█████▌    | 276/500 [06:32<05:24,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -325.15423583984375:  55%|█████▌    | 277/500 [06:34<05:24,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -311.8338928222656:  56%|█████▌    | 278/500 [06:35<05:23,  1.46s/it] \u001b[A\n",
      "episode_reward_mean_agents = -294.0000305175781:  56%|█████▌    | 279/500 [06:37<05:23,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -279.0731506347656:  56%|█████▌    | 280/500 [06:38<05:19,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -326.0983581542969:  56%|█████▌    | 281/500 [06:39<05:17,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -325.5756530761719:  56%|█████▋    | 282/500 [06:41<05:14,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -328.9324645996094:  57%|█████▋    | 283/500 [06:42<05:12,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -251.17333984375:  57%|█████▋    | 284/500 [06:44<05:11,  1.44s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -310.9068908691406:  57%|█████▋    | 285/500 [06:45<05:09,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -307.9364318847656:  57%|█████▋    | 286/500 [06:47<05:09,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -291.6790466308594:  57%|█████▋    | 287/500 [06:48<05:07,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -340.7684020996094:  58%|█████▊    | 288/500 [06:49<05:06,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -267.7832336425781:  58%|█████▊    | 289/500 [06:51<05:02,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -326.68505859375:  58%|█████▊    | 290/500 [06:52<05:01,  1.44s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -326.4645690917969:  58%|█████▊    | 291/500 [06:54<05:01,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -294.85302734375:  58%|█████▊    | 292/500 [06:55<04:59,  1.44s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -346.047607421875:  59%|█████▊    | 293/500 [06:57<04:58,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -293.05999755859375:  59%|█████▉    | 294/500 [06:58<04:57,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -362.9304504394531:  59%|█████▉    | 295/500 [07:00<04:55,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -359.5557861328125:  59%|█████▉    | 296/500 [07:01<04:54,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -290.9006042480469:  59%|█████▉    | 297/500 [07:02<04:53,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -263.459228515625:  60%|█████▉    | 298/500 [07:04<04:51,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -235.0846710205078:  60%|█████▉    | 299/500 [07:05<04:48,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -280.0382995605469:  60%|██████    | 300/500 [07:07<04:47,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -335.10723876953125:  60%|██████    | 301/500 [07:08<04:44,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -320.3967590332031:  60%|██████    | 302/500 [07:10<04:43,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -255.7889862060547:  61%|██████    | 303/500 [07:11<04:41,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -285.84814453125:  61%|██████    | 304/500 [07:12<04:39,  1.42s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -287.00836181640625:  61%|██████    | 305/500 [07:14<04:38,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -314.0896911621094:  61%|██████    | 306/500 [07:15<04:38,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -315.0303649902344:  61%|██████▏   | 307/500 [07:17<04:37,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -291.55816650390625:  62%|██████▏   | 308/500 [07:18<04:36,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -289.3661804199219:  62%|██████▏   | 309/500 [07:20<04:34,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -299.63836669921875:  62%|██████▏   | 310/500 [07:21<04:33,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -291.8350830078125:  62%|██████▏   | 311/500 [07:23<04:31,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -341.01422119140625:  62%|██████▏   | 312/500 [07:24<04:30,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -291.77099609375:  63%|██████▎   | 313/500 [07:25<04:29,  1.44s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -305.9452209472656:  63%|██████▎   | 314/500 [07:27<04:28,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -328.7619934082031:  63%|██████▎   | 315/500 [07:28<04:28,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -325.1611328125:  63%|██████▎   | 316/500 [07:30<04:37,  1.51s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -317.68829345703125:  63%|██████▎   | 317/500 [07:31<04:33,  1.49s/it]\u001b[A\n",
      "episode_reward_mean_agents = -319.3818359375:  64%|██████▎   | 318/500 [07:33<04:30,  1.48s/it]    \u001b[A\n",
      "episode_reward_mean_agents = -288.19671630859375:  64%|██████▍   | 319/500 [07:34<04:27,  1.48s/it]\u001b[A\n",
      "episode_reward_mean_agents = -300.74609375:  64%|██████▍   | 320/500 [07:36<04:25,  1.47s/it]      \u001b[A\n",
      "episode_reward_mean_agents = -316.85040283203125:  64%|██████▍   | 321/500 [07:37<04:21,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -305.0741882324219:  64%|██████▍   | 322/500 [07:39<04:19,  1.46s/it] \u001b[A\n",
      "episode_reward_mean_agents = -289.039794921875:  65%|██████▍   | 323/500 [07:40<04:16,  1.45s/it] \u001b[A\n",
      "episode_reward_mean_agents = -308.4837341308594:  65%|██████▍   | 324/500 [07:42<04:14,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -320.0934143066406:  65%|██████▌   | 325/500 [07:43<04:13,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -264.685546875:  65%|██████▌   | 326/500 [07:44<04:13,  1.45s/it]    \u001b[A\n",
      "episode_reward_mean_agents = -286.7991638183594:  65%|██████▌   | 327/500 [07:46<04:11,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -244.427734375:  66%|██████▌   | 328/500 [07:47<04:08,  1.45s/it]    \u001b[A\n",
      "episode_reward_mean_agents = -278.5505676269531:  66%|██████▌   | 329/500 [07:49<04:07,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -277.64892578125:  66%|██████▌   | 330/500 [07:50<04:06,  1.45s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -251.0939178466797:  66%|██████▌   | 331/500 [07:52<04:04,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -297.3832702636719:  66%|██████▋   | 332/500 [07:53<04:02,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -253.8689727783203:  67%|██████▋   | 333/500 [07:55<04:02,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -229.3300323486328:  67%|██████▋   | 334/500 [07:56<04:00,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -291.3896179199219:  67%|██████▋   | 335/500 [07:57<03:57,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -265.2641296386719:  67%|██████▋   | 336/500 [07:59<03:54,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -292.4850769042969:  67%|██████▋   | 337/500 [08:00<03:52,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -290.1100769042969:  68%|██████▊   | 338/500 [08:02<03:52,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -230.2646942138672:  68%|██████▊   | 339/500 [08:03<03:49,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -287.7369079589844:  68%|██████▊   | 340/500 [08:05<03:47,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -287.2314758300781:  68%|██████▊   | 341/500 [08:06<03:46,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -323.9881896972656:  68%|██████▊   | 342/500 [08:07<03:46,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -265.9018249511719:  69%|██████▊   | 343/500 [08:09<03:45,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -292.6191101074219:  69%|██████▉   | 344/500 [08:10<03:43,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -278.9384460449219:  69%|██████▉   | 345/500 [08:12<03:42,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -251.38284301757812:  69%|██████▉   | 346/500 [08:13<03:40,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -273.50738525390625:  69%|██████▉   | 347/500 [08:15<03:38,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -260.093994140625:  70%|██████▉   | 348/500 [08:16<03:36,  1.42s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -311.3583679199219:  70%|██████▉   | 349/500 [08:17<03:35,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -263.3501281738281:  70%|███████   | 350/500 [08:19<03:33,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -277.8661804199219:  70%|███████   | 351/500 [08:20<03:32,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -320.2516784667969:  70%|███████   | 352/500 [08:22<03:32,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -265.4953308105469:  71%|███████   | 353/500 [08:23<03:31,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -299.8494567871094:  71%|███████   | 354/500 [08:25<03:30,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -244.4656219482422:  71%|███████   | 355/500 [08:26<03:27,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -244.8386688232422:  71%|███████   | 356/500 [08:28<03:28,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -241.96495056152344:  71%|███████▏  | 357/500 [08:29<03:28,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -268.14404296875:  72%|███████▏  | 358/500 [08:31<03:28,  1.47s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -293.62115478515625:  72%|███████▏  | 359/500 [08:32<03:28,  1.48s/it]\u001b[A\n",
      "episode_reward_mean_agents = -273.3957824707031:  72%|███████▏  | 360/500 [08:33<03:26,  1.47s/it] \u001b[A\n",
      "episode_reward_mean_agents = -232.60166931152344:  72%|███████▏  | 361/500 [08:35<03:24,  1.47s/it]\u001b[A\n",
      "episode_reward_mean_agents = -234.5908660888672:  72%|███████▏  | 362/500 [08:36<03:22,  1.47s/it] \u001b[A\n",
      "episode_reward_mean_agents = -248.2090606689453:  73%|███████▎  | 363/500 [08:38<03:21,  1.47s/it]\u001b[A\n",
      "episode_reward_mean_agents = -289.9598083496094:  73%|███████▎  | 364/500 [08:39<03:19,  1.47s/it]\u001b[A\n",
      "episode_reward_mean_agents = -239.6142120361328:  73%|███████▎  | 365/500 [08:41<03:17,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -272.6718444824219:  73%|███████▎  | 366/500 [08:42<03:16,  1.47s/it]\u001b[A\n",
      "episode_reward_mean_agents = -285.1918640136719:  73%|███████▎  | 367/500 [08:44<03:15,  1.47s/it]\u001b[A\n",
      "episode_reward_mean_agents = -294.4325256347656:  74%|███████▎  | 368/500 [08:45<03:12,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -271.65777587890625:  74%|███████▍  | 369/500 [08:47<03:09,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -265.0638122558594:  74%|███████▍  | 370/500 [08:48<03:08,  1.45s/it] \u001b[A\n",
      "episode_reward_mean_agents = -308.1670227050781:  74%|███████▍  | 371/500 [08:49<03:06,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -264.7391662597656:  74%|███████▍  | 372/500 [08:51<03:03,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -279.8519592285156:  75%|███████▍  | 373/500 [08:52<03:01,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -290.031494140625:  75%|███████▍  | 374/500 [08:54<02:59,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -284.04931640625:  75%|███████▌  | 375/500 [08:55<02:58,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -309.0556335449219:  75%|███████▌  | 376/500 [08:57<02:56,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -249.7964630126953:  75%|███████▌  | 377/500 [08:58<02:54,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -259.0162048339844:  76%|███████▌  | 378/500 [09:00<02:59,  1.47s/it]\u001b[A\n",
      "episode_reward_mean_agents = -275.7701416015625:  76%|███████▌  | 379/500 [09:01<02:55,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -278.2387390136719:  76%|███████▌  | 380/500 [09:02<02:53,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -243.7213897705078:  76%|███████▌  | 381/500 [09:04<02:50,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -289.2288513183594:  76%|███████▋  | 382/500 [09:05<02:48,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -270.0741271972656:  77%|███████▋  | 383/500 [09:07<02:47,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -257.12744140625:  77%|███████▋  | 384/500 [09:08<02:45,  1.43s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -262.1085510253906:  77%|███████▋  | 385/500 [09:10<02:44,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -296.5205993652344:  77%|███████▋  | 386/500 [09:11<02:43,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -295.2675476074219:  77%|███████▋  | 387/500 [09:12<02:42,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -220.4169464111328:  78%|███████▊  | 388/500 [09:14<02:40,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -280.230712890625:  78%|███████▊  | 389/500 [09:15<02:38,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -246.65872192382812:  78%|███████▊  | 390/500 [09:17<02:36,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -264.5804748535156:  78%|███████▊  | 391/500 [09:18<02:34,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -245.47744750976562:  78%|███████▊  | 392/500 [09:20<02:33,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -220.1096954345703:  79%|███████▊  | 393/500 [09:21<02:31,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -244.15493774414062:  79%|███████▉  | 394/500 [09:22<02:29,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -278.2629089355469:  79%|███████▉  | 395/500 [09:24<02:29,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -267.2572021484375:  79%|███████▉  | 396/500 [09:25<02:28,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -314.6554870605469:  79%|███████▉  | 397/500 [09:27<02:27,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -260.7794494628906:  80%|███████▉  | 398/500 [09:28<02:25,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -312.38360595703125:  80%|███████▉  | 399/500 [09:30<02:25,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -221.4324188232422:  80%|████████  | 400/500 [09:31<02:23,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -251.488525390625:  80%|████████  | 401/500 [09:32<02:21,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -253.88291931152344:  80%|████████  | 402/500 [09:34<02:20,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -239.20103454589844:  81%|████████  | 403/500 [09:35<02:18,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -273.5043029785156:  81%|████████  | 404/500 [09:37<02:16,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -227.6568145751953:  81%|████████  | 405/500 [09:38<02:14,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -225.0057373046875:  81%|████████  | 406/500 [09:39<02:12,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -255.30166625976562:  81%|████████▏ | 407/500 [09:41<02:10,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -268.33819580078125:  82%|████████▏ | 408/500 [09:42<02:09,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -260.4294128417969:  82%|████████▏ | 409/500 [09:44<02:07,  1.40s/it] \u001b[A\n",
      "episode_reward_mean_agents = -234.3670654296875:  82%|████████▏ | 410/500 [09:45<02:06,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -290.1518249511719:  82%|████████▏ | 411/500 [09:46<02:05,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -256.7238464355469:  82%|████████▏ | 412/500 [09:48<02:03,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -260.23028564453125:  83%|████████▎ | 413/500 [09:49<02:02,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -277.8146057128906:  83%|████████▎ | 414/500 [09:51<02:01,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -244.3211669921875:  83%|████████▎ | 415/500 [09:52<01:59,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -237.4394073486328:  83%|████████▎ | 416/500 [09:53<01:57,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -251.34776306152344:  83%|████████▎ | 417/500 [09:55<01:56,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -248.59434509277344:  84%|████████▎ | 418/500 [09:56<01:54,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -235.59683227539062:  84%|████████▍ | 419/500 [09:58<01:53,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -263.9831237792969:  84%|████████▍ | 420/500 [09:59<01:53,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -250.57456970214844:  84%|████████▍ | 421/500 [10:01<01:52,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -244.57579040527344:  84%|████████▍ | 422/500 [10:02<01:49,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -234.68772888183594:  85%|████████▍ | 423/500 [10:03<01:48,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -235.9731903076172:  85%|████████▍ | 424/500 [10:05<01:47,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -203.0589599609375:  85%|████████▌ | 425/500 [10:06<01:45,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -269.8863220214844:  85%|████████▌ | 426/500 [10:08<01:43,  1.40s/it]\u001b[A\n",
      "episode_reward_mean_agents = -257.5058288574219:  85%|████████▌ | 427/500 [10:09<01:43,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -264.8564147949219:  86%|████████▌ | 428/500 [10:10<01:42,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -248.3592071533203:  86%|████████▌ | 429/500 [10:12<01:42,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -255.6201171875:  86%|████████▌ | 430/500 [10:13<01:40,  1.44s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -199.9915771484375:  86%|████████▌ | 431/500 [10:15<01:39,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -227.38442993164062:  86%|████████▋ | 432/500 [10:16<01:37,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -278.5964050292969:  87%|████████▋ | 433/500 [10:18<01:37,  1.45s/it] \u001b[A\n",
      "episode_reward_mean_agents = -201.46563720703125:  87%|████████▋ | 434/500 [10:19<01:35,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -247.4633026123047:  87%|████████▋ | 435/500 [10:21<01:34,  1.46s/it] \u001b[A\n",
      "episode_reward_mean_agents = -201.1824188232422:  87%|████████▋ | 436/500 [10:22<01:33,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -252.74156188964844:  87%|████████▋ | 437/500 [10:24<01:31,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -223.5969696044922:  88%|████████▊ | 438/500 [10:25<01:29,  1.45s/it] \u001b[A\n",
      "episode_reward_mean_agents = -217.35325622558594:  88%|████████▊ | 439/500 [10:26<01:27,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -264.62152099609375:  88%|████████▊ | 440/500 [10:28<01:29,  1.49s/it]\u001b[A\n",
      "episode_reward_mean_agents = -205.516357421875:  88%|████████▊ | 441/500 [10:29<01:26,  1.46s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -250.43643188476562:  88%|████████▊ | 442/500 [10:31<01:24,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -261.30633544921875:  89%|████████▊ | 443/500 [10:32<01:22,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -262.79083251953125:  89%|████████▉ | 444/500 [10:34<01:20,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -253.6523895263672:  89%|████████▉ | 445/500 [10:35<01:19,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -256.991943359375:  89%|████████▉ | 446/500 [10:37<01:17,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -245.5530548095703:  89%|████████▉ | 447/500 [10:38<01:15,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -248.26953125:  90%|████████▉ | 448/500 [10:39<01:14,  1.42s/it]     \u001b[A\n",
      "episode_reward_mean_agents = -209.93594360351562:  90%|████████▉ | 449/500 [10:41<01:12,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -255.92161560058594:  90%|█████████ | 450/500 [10:42<01:11,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -220.377197265625:  90%|█████████ | 451/500 [10:44<01:10,  1.44s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -217.8987274169922:  90%|█████████ | 452/500 [10:45<01:09,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -253.16734313964844:  91%|█████████ | 453/500 [10:47<01:07,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -230.12290954589844:  91%|█████████ | 454/500 [10:48<01:05,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -225.3962860107422:  91%|█████████ | 455/500 [10:49<01:04,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -208.0519561767578:  91%|█████████ | 456/500 [10:51<01:02,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -258.7132873535156:  91%|█████████▏| 457/500 [10:52<01:00,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -244.1970672607422:  92%|█████████▏| 458/500 [10:54<00:59,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -270.2665710449219:  92%|█████████▏| 459/500 [10:55<00:57,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -263.45709228515625:  92%|█████████▏| 460/500 [10:56<00:56,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -249.01089477539062:  92%|█████████▏| 461/500 [10:58<00:55,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -255.57046508789062:  92%|█████████▏| 462/500 [10:59<00:54,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -274.5517578125:  93%|█████████▎| 463/500 [11:01<00:52,  1.43s/it]    \u001b[A\n",
      "episode_reward_mean_agents = -338.1521911621094:  93%|█████████▎| 464/500 [11:02<00:51,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -269.1197509765625:  93%|█████████▎| 465/500 [11:04<00:50,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -245.62120056152344:  93%|█████████▎| 466/500 [11:05<00:48,  1.43s/it]\u001b[A\n",
      "episode_reward_mean_agents = -238.5587921142578:  93%|█████████▎| 467/500 [11:06<00:46,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -276.5547180175781:  94%|█████████▎| 468/500 [11:08<00:45,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -226.6189727783203:  94%|█████████▍| 469/500 [11:09<00:43,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -292.95440673828125:  94%|█████████▍| 470/500 [11:11<00:42,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -246.5989532470703:  94%|█████████▍| 471/500 [11:12<00:40,  1.41s/it] \u001b[A\n",
      "episode_reward_mean_agents = -246.30419921875:  94%|█████████▍| 472/500 [11:14<00:39,  1.41s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -298.3018493652344:  95%|█████████▍| 473/500 [11:15<00:38,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -226.5472412109375:  95%|█████████▍| 474/500 [11:16<00:36,  1.41s/it]\u001b[A\n",
      "episode_reward_mean_agents = -281.6713562011719:  95%|█████████▌| 475/500 [11:18<00:35,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -211.2392578125:  95%|█████████▌| 476/500 [11:19<00:34,  1.44s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -231.0619354248047:  95%|█████████▌| 477/500 [11:21<00:33,  1.44s/it]\u001b[A\n",
      "episode_reward_mean_agents = -283.1983642578125:  96%|█████████▌| 478/500 [11:22<00:31,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -217.42434692382812:  96%|█████████▌| 479/500 [11:24<00:30,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -254.45166015625:  96%|█████████▌| 480/500 [11:25<00:29,  1.46s/it]   \u001b[A\n",
      "episode_reward_mean_agents = -279.2816467285156:  96%|█████████▌| 481/500 [11:27<00:27,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -232.0574188232422:  96%|█████████▋| 482/500 [11:28<00:26,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -217.67417907714844:  97%|█████████▋| 483/500 [11:30<00:24,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -254.528076171875:  97%|█████████▋| 484/500 [11:31<00:23,  1.46s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -205.56443786621094:  97%|█████████▋| 485/500 [11:32<00:21,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -236.85911560058594:  97%|█████████▋| 486/500 [11:34<00:20,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -296.9671325683594:  97%|█████████▋| 487/500 [11:35<00:18,  1.46s/it] \u001b[A\n",
      "episode_reward_mean_agents = -278.91943359375:  98%|█████████▊| 488/500 [11:37<00:17,  1.46s/it]  \u001b[A\n",
      "episode_reward_mean_agents = -291.82598876953125:  98%|█████████▊| 489/500 [11:38<00:16,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -292.6347961425781:  98%|█████████▊| 490/500 [11:40<00:14,  1.47s/it] \u001b[A\n",
      "episode_reward_mean_agents = -243.0751495361328:  98%|█████████▊| 491/500 [11:41<00:13,  1.46s/it]\u001b[A\n",
      "episode_reward_mean_agents = -275.5007019042969:  98%|█████████▊| 492/500 [11:43<00:11,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -262.6622314453125:  99%|█████████▊| 493/500 [11:44<00:10,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -319.14288330078125:  99%|█████████▉| 494/500 [11:46<00:08,  1.45s/it]\u001b[A\n",
      "episode_reward_mean_agents = -231.6223602294922:  99%|█████████▉| 495/500 [11:47<00:07,  1.44s/it] \u001b[A\n",
      "episode_reward_mean_agents = -244.849853515625:  99%|█████████▉| 496/500 [11:48<00:05,  1.43s/it] \u001b[A\n",
      "episode_reward_mean_agents = -223.3806610107422:  99%|█████████▉| 497/500 [11:50<00:04,  1.42s/it]\u001b[A\n",
      "episode_reward_mean_agents = -218.127197265625: 100%|█████████▉| 498/500 [11:51<00:02,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -275.26513671875: 100%|█████████▉| 499/500 [11:53<00:01,  1.42s/it] \u001b[A\n",
      "episode_reward_mean_agents = -267.14892578125: 100%|██████████| 500/500 [11:54<00:00,  1.43s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(\n",
    "    total=n_iters,\n",
    "    desc=\", \".join(\n",
    "        [f\"episode_reward_mean_{group} = 0\" for group in env.group_map.keys()]\n",
    "    ),\n",
    ")\n",
    "episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
    "train_group_map = copy.deepcopy(env.group_map)\n",
    "\n",
    "grad_norms = []\n",
    "# Training/collection iterations\n",
    "for iteration, batch in enumerate(collector):\n",
    "    current_frames = batch.numel()\n",
    "    batch = process_batch(batch)  # Util to expand done keys if needed\n",
    "    # Loop over groups\n",
    "    for group in train_group_map.keys():\n",
    "        group_batch = batch.exclude(\n",
    "            *[\n",
    "                key\n",
    "                for _group in env.group_map.keys()\n",
    "                if _group != group\n",
    "                for key in [_group, (\"next\", _group)]\n",
    "            ]\n",
    "        )  # Exclude data from other groups\n",
    "        group_batch = group_batch.reshape(\n",
    "            -1\n",
    "        )  # This just affects the leading dimensions in batch_size of the tensordict\n",
    "        replay_buffers[group].extend(group_batch)\n",
    "\n",
    "        for _ in range(n_optimiser_steps):\n",
    "            subdata = replay_buffers[group].sample()\n",
    "            loss_vals = losses[group](subdata)\n",
    "\n",
    "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
    "                loss = loss_vals[loss_name]\n",
    "                optimiser = optimisers[group][loss_name]\n",
    "                # print(type(phi_factor), phi_factor, isinstance(phi_factor, torch.Tensor))\n",
    "                loss.backward()\n",
    "\n",
    "                # Optional\n",
    "                params = optimiser.param_groups[0][\"params\"]\n",
    "                temp = torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
    "                total_norm = 0.0\n",
    "                # for p in params:\n",
    "                #     if p.grad is not None:\n",
    "                #         total_norm += p.grad.data.norm(2).item() ** 2\n",
    "                # total_norm = total_norm ** 0.5\n",
    "                # grad_norms.append(total_norm)\n",
    "\n",
    "                optimiser.step()\n",
    "                optimiser.zero_grad()\n",
    "\n",
    "            # Soft-update the target network\n",
    "            target_updaters[group].step()\n",
    "\n",
    "        # Exploration sigma anneal update\n",
    "        exploration_policies[group][-1].step(current_frames)\n",
    "\n",
    "    # Logging\n",
    "    for group in env.group_map.keys():\n",
    "        episode_reward_mean = (\n",
    "            batch.get((\"next\", group, \"episode_reward\"))[\n",
    "                batch.get((\"next\", group, \"done\"))\n",
    "            ]\n",
    "            .mean()\n",
    "            .item()\n",
    "        )\n",
    "        episode_reward_mean_map[group].append(episode_reward_mean)\n",
    "\n",
    "    pbar.set_description(\n",
    "        \", \".join(\n",
    "            [\n",
    "                f\"episode_reward_mean_{group} = {episode_reward_mean_map[group][-1]}\"\n",
    "                for group in env.group_map.keys()\n",
    "            ]\n",
    "        ),\n",
    "        refresh=False,\n",
    "    )\n",
    "    pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['agents'])\n"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 1)\n",
    "for i, group in enumerate(env.group_map.keys()):\n",
    "    axs.plot(episode_reward_mean_map[group], label=f\"Episode reward mean {group}\")\n",
    "    axs.set_ylabel(\"Reward\")\n",
    "    axs.legend()\n",
    "axs.set_xlabel(\"Training iterations\")\n",
    "plt.show()\n",
    "print(env.group_map.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(grad_norms)\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Gradient Norm\")\n",
    "plt.title(\"Gradient Norm over Training\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
